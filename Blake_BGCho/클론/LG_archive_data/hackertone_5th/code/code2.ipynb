{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "884eab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install category_encoders\n",
    "# !pip install tqdm\n",
    "# !pip install lightgbm\n",
    "# !pip install xgboost\n",
    "# !pip install catboost\n",
    "# !pip install torch\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e9265",
   "metadata": {},
   "source": [
    "# 제품 이상여부 판별 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab431",
   "metadata": {},
   "source": [
    "## 0. Environment & Data Loading & Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8341e8",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a315cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score)\n",
    "\n",
    "import matplotlib as plt\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import random\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d054e30",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5a17682",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\n",
    "\n",
    "df_test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b67a759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\n",
       "OK       11293\n",
       "549       7271\n",
       "162.4     3580\n",
       "550       2398\n",
       "550.3     1909\n",
       "549.5     1263\n",
       "548.5       26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12efb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_후처리용 = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e721823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Equipment_Dam</th>\n",
       "      <th>Model.Suffix_Dam</th>\n",
       "      <th>Workorder_Dam</th>\n",
       "      <th>Collect Date_Dam</th>\n",
       "      <th>CURE END POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Z Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Θ Collect Result_Dam</th>\n",
       "      <th>CURE SPEED Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION Θ Collect Result_Dam</th>\n",
       "      <th>...</th>\n",
       "      <th>Head Clean Position Z Collect Result_Fill2</th>\n",
       "      <th>Head Purge Position X Collect Result_Fill2</th>\n",
       "      <th>Head Purge Position Y Collect Result_Fill2</th>\n",
       "      <th>Head Purge Position Z Collect Result_Fill2</th>\n",
       "      <th>Machine Tact time Collect Result_Fill2</th>\n",
       "      <th>PalletID Collect Result_Fill2</th>\n",
       "      <th>Production Qty Collect Result_Fill2</th>\n",
       "      <th>Receip No Collect Result_Fill2</th>\n",
       "      <th>WorkMode Collect Result_Fill2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334505</td>\n",
       "      <td>4F1XA938-1</td>\n",
       "      <td>2024-04-25 11:10:00</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>100</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>91.8</td>\n",
       "      <td>270</td>\n",
       "      <td>50</td>\n",
       "      <td>114.612</td>\n",
       "      <td>19.9</td>\n",
       "      <td>7</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334505</td>\n",
       "      <td>3KPM0016-2</td>\n",
       "      <td>2023-09-19 14:30:00</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>...</td>\n",
       "      <td>91.8</td>\n",
       "      <td>270.0</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>19.600</td>\n",
       "      <td>7.0</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>4E1X9167-1</td>\n",
       "      <td>2024-03-05 09:30:00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>91.8</td>\n",
       "      <td>270</td>\n",
       "      <td>50</td>\n",
       "      <td>114.612</td>\n",
       "      <td>19.8</td>\n",
       "      <td>10</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3K1X0057-1</td>\n",
       "      <td>2023-09-25 15:40:00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>91.8</td>\n",
       "      <td>270.0</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>19.900</td>\n",
       "      <td>12.0</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3HPM0007-1</td>\n",
       "      <td>2023-06-27 13:20:00</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>...</td>\n",
       "      <td>91.8</td>\n",
       "      <td>270.0</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>19.700</td>\n",
       "      <td>8.0</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Equipment_Dam Model.Suffix_Dam Workorder_Dam     Collect Date_Dam  \\\n",
       "0  Dam dispenser #1      AJX75334505    4F1XA938-1  2024-04-25 11:10:00   \n",
       "1  Dam dispenser #1      AJX75334505    3KPM0016-2  2023-09-19 14:30:00   \n",
       "2  Dam dispenser #2      AJX75334501    4E1X9167-1  2024-03-05 09:30:00   \n",
       "3  Dam dispenser #2      AJX75334501    3K1X0057-1  2023-09-25 15:40:00   \n",
       "4  Dam dispenser #1      AJX75334501    3HPM0007-1  2023-06-27 13:20:00   \n",
       "\n",
       "   CURE END POSITION X Collect Result_Dam  \\\n",
       "0                                   240.0   \n",
       "1                                   240.0   \n",
       "2                                  1000.0   \n",
       "3                                  1000.0   \n",
       "4                                   240.0   \n",
       "\n",
       "   CURE END POSITION Z Collect Result_Dam  \\\n",
       "0                                     2.5   \n",
       "1                                     2.5   \n",
       "2                                    12.5   \n",
       "3                                    12.5   \n",
       "4                                     2.5   \n",
       "\n",
       "   CURE END POSITION Θ Collect Result_Dam  CURE SPEED Collect Result_Dam  \\\n",
       "0                                     -90                            100   \n",
       "1                                     -90                             70   \n",
       "2                                      90                             85   \n",
       "3                                      90                             70   \n",
       "4                                     -90                             70   \n",
       "\n",
       "   CURE START POSITION X Collect Result_Dam  \\\n",
       "0                                      1030   \n",
       "1                                      1030   \n",
       "2                                       280   \n",
       "3                                       280   \n",
       "4                                      1030   \n",
       "\n",
       "   CURE START POSITION Θ Collect Result_Dam  ...  \\\n",
       "0                                       -90  ...   \n",
       "1                                       -90  ...   \n",
       "2                                        90  ...   \n",
       "3                                        90  ...   \n",
       "4                                       -90  ...   \n",
       "\n",
       "   Head Clean Position Z Collect Result_Fill2  \\\n",
       "0                                        50.0   \n",
       "1                                        91.8   \n",
       "2                                        50.0   \n",
       "3                                        91.8   \n",
       "4                                        91.8   \n",
       "\n",
       "   Head Purge Position X Collect Result_Fill2  \\\n",
       "0                                        91.8   \n",
       "1                                       270.0   \n",
       "2                                        91.8   \n",
       "3                                       270.0   \n",
       "4                                       270.0   \n",
       "\n",
       "   Head Purge Position Y Collect Result_Fill2  \\\n",
       "0                                         270   \n",
       "1                                          50   \n",
       "2                                         270   \n",
       "3                                          50   \n",
       "4                                          50   \n",
       "\n",
       "   Head Purge Position Z Collect Result_Fill2  \\\n",
       "0                                          50   \n",
       "1                                          85   \n",
       "2                                          50   \n",
       "3                                          85   \n",
       "4                                          85   \n",
       "\n",
       "   Machine Tact time Collect Result_Fill2  PalletID Collect Result_Fill2  \\\n",
       "0                                 114.612                           19.9   \n",
       "1                                  19.600                            7.0   \n",
       "2                                 114.612                           19.8   \n",
       "3                                  19.900                           12.0   \n",
       "4                                  19.700                            8.0   \n",
       "\n",
       "   Production Qty Collect Result_Fill2 Receip No Collect Result_Fill2  \\\n",
       "0                                    7                            127   \n",
       "1                                  185                              1   \n",
       "2                                   10                             73   \n",
       "3                                  268                              1   \n",
       "4                                  121                              1   \n",
       "\n",
       "   WorkMode Collect Result_Fill2  target  \n",
       "0                              1  Normal  \n",
       "1                              0  Normal  \n",
       "2                              1  Normal  \n",
       "3                              0  Normal  \n",
       "4                              0  Normal  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop columns with percentage of missing value and drop only one unique value\n",
    "drop_cols = []\n",
    "for column in df_train.columns:\n",
    "    if ((df_train[column].isnull().sum() / df_train.shape[0]) > 0.4) or (df_train[column].nunique() <=1):\n",
    "        drop_cols.append(column)\n",
    "    \n",
    "df_train.drop(columns = drop_cols, inplace = True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e2fc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features_tr = df_train[['Collect Date_Dam','Collect Date_Fill1','Collect Date_Fill2','Collect Date_AutoClave']]\n",
    "time_features_te = df_test[['Collect Date_Dam','Collect Date_Fill1','Collect Date_Fill2','Collect Date_AutoClave']]\n",
    "\n",
    "df_train.drop(columns = ['Collect Date_Dam','Collect Date_Fill1','Collect Date_Fill2','Collect Date_AutoClave'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f23e5e",
   "metadata": {},
   "source": [
    "### DATA SHIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53edf6b3",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6f9e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "column  = list(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b7a5db69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WorkMode Collect Result_Fill1 기준(Dam)\n",
    "df_train_1 = df_train[df_train['WorkMode Collect Result_Fill1']!=7]\n",
    "df_train_2 = df_train[df_train['WorkMode Collect Result_Fill1']==7]\n",
    "replace_data  = df_train_1.iloc[:,15:69].shift(-1, axis=1).drop(columns=['Dispense Volume(Stage3) Collect Result_Dam','WorkMode Collect Result_Dam'])\n",
    "df_train.loc[df_train_1.index,replace_data.columns] = replace_data\n",
    "df_train = df_train.drop(columns  = ['WorkMode Collect Result_Dam'])\n",
    "\n",
    "#WorkMode Collect Result_Fill1 기준(Fill1)\n",
    "df_train_1 = df_train[df_train['WorkMode Collect Result_Fill1']!=7]\n",
    "df_train_2 = df_train[df_train['WorkMode Collect Result_Fill1']==7]\n",
    "replace_data  = df_train_1.iloc[:,89:112].shift(-1, axis=1).drop(columns=['WorkMode Collect Result_Fill1'])\n",
    "df_train.loc[df_train_1.index,replace_data.columns] = replace_data\n",
    "df_train = df_train.drop(columns  = ['WorkMode Collect Result_Fill1'])\n",
    "\n",
    "#WorkMode Collect Result_Fill1 기준(Fill2)\n",
    "idx = replace_data.index\n",
    "df_train_1 = df_train.loc[idx]\n",
    "replace_data  = df_train_1.iloc[:,120:143].shift(-1, axis=1).drop(columns=['WorkMode Collect Result_Fill2'])\n",
    "df_train.loc[df_train_1.index,replace_data.columns] = replace_data\n",
    "df_train = df_train.drop(columns  = ['WorkMode Collect Result_Fill2'])\n",
    "\n",
    "df_train.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a775c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\n",
       "549      7271\n",
       "162.4    3580\n",
       "162.4    3487\n",
       "162.7    2454\n",
       "550      2398\n",
       "164.2    2358\n",
       "550.4    2350\n",
       "550.3    1909\n",
       "550.3    1868\n",
       "550.6    1763\n",
       "551.5    1440\n",
       "549.5    1263\n",
       "551.7     970\n",
       "548.9     843\n",
       "161.2     826\n",
       "550.5     747\n",
       "163.5     566\n",
       "164.1     515\n",
       "551.8     495\n",
       "161.7     489\n",
       "552.1     480\n",
       "552.0     446\n",
       "549.4     446\n",
       "164.5     376\n",
       "163.3     244\n",
       "164.0     217\n",
       "163.8     208\n",
       "551.3     127\n",
       "551.2     101\n",
       "163.4      67\n",
       "550.7      66\n",
       "163.7      38\n",
       "163.0      29\n",
       "548.5      26\n",
       "551.6      20\n",
       "163.6      13\n",
       "163.1       9\n",
       "551.0       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b4d76",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8c9ebacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "column.remove('target')\n",
    "df_test = df_test[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "acd63bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_1 = df_test[df_test['WorkMode Collect Result_Fill1']!=7]\n",
    "df_test_2 = df_test[df_test['WorkMode Collect Result_Fill1']==7]\n",
    "replace_data = df_test_1.iloc[:,15:69].shift(-1, axis=1).drop(columns=['Dispense Volume(Stage3) Collect Result_Dam','WorkMode Collect Result_Dam'])\n",
    "df_test.loc[df_test_1.index,replace_data.columns] = replace_data\n",
    "df_test = df_test.drop(columns  = ['WorkMode Collect Result_Dam'])\n",
    "\n",
    "\n",
    "df_test_1 = df_test[df_test['WorkMode Collect Result_Fill1']!=7]\n",
    "df_test_2 = df_test[df_test['WorkMode Collect Result_Fill1']==7]\n",
    "replace_data  = df_test_1.iloc[:,89:112].shift(-1, axis=1).drop(columns=['WorkMode Collect Result_Fill1'])\n",
    "df_test.loc[df_test_1.index,replace_data.columns] = replace_data\n",
    "df_test = df_test.drop(columns  = ['WorkMode Collect Result_Fill1'])\n",
    "\n",
    "\n",
    "idx = replace_data.index\n",
    "df_test_1 = df_test.loc[idx]\n",
    "replace_data  = df_test_1.iloc[:,120:143].shift(-1, axis=1).drop(columns=['WorkMode Collect Result_Fill2'])\n",
    "df_test.loc[df_test_1.index,replace_data.columns] = replace_data\n",
    "df_test = df_test.drop(columns  = ['WorkMode Collect Result_Fill2'])\n",
    "\n",
    "df_test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d94ea",
   "metadata": {},
   "source": [
    "## Float Transfrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "806b3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2bd5805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float)\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float)\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f303b33",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac257df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Equipment_Dam</th>\n",
       "      <th>Model.Suffix_Dam</th>\n",
       "      <th>Workorder_Dam</th>\n",
       "      <th>CURE END POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Z Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Θ Collect Result_Dam</th>\n",
       "      <th>CURE SPEED Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION Θ Collect Result_Dam</th>\n",
       "      <th>DISCHARGED SPEED OF RESIN Collect Result_Dam</th>\n",
       "      <th>...</th>\n",
       "      <th>HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2</th>\n",
       "      <th>HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2</th>\n",
       "      <th>HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2</th>\n",
       "      <th>HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2</th>\n",
       "      <th>Head Purge Position Z Collect Result_Fill2</th>\n",
       "      <th>Machine Tact time Collect Result_Fill2</th>\n",
       "      <th>PalletID Collect Result_Fill2</th>\n",
       "      <th>Production Qty Collect Result_Fill2</th>\n",
       "      <th>Receip No Collect Result_Fill2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334505</td>\n",
       "      <td>4F1XA938-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>100</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>428.0</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>114.612</td>\n",
       "      <td>19.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334505</td>\n",
       "      <td>3KPM0016-2</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>428.0</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>85.000</td>\n",
       "      <td>19.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>4E1X9167-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>1324.2</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>114.612</td>\n",
       "      <td>19.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3K1X0057-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1324.2</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>85.000</td>\n",
       "      <td>19.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3HPM0007-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>428.0</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>85.000</td>\n",
       "      <td>19.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Equipment_Dam Model.Suffix_Dam Workorder_Dam  \\\n",
       "0  Dam dispenser #1      AJX75334505    4F1XA938-1   \n",
       "1  Dam dispenser #1      AJX75334505    3KPM0016-2   \n",
       "2  Dam dispenser #2      AJX75334501    4E1X9167-1   \n",
       "3  Dam dispenser #2      AJX75334501    3K1X0057-1   \n",
       "4  Dam dispenser #1      AJX75334501    3HPM0007-1   \n",
       "\n",
       "   CURE END POSITION X Collect Result_Dam  \\\n",
       "0                                   240.0   \n",
       "1                                   240.0   \n",
       "2                                  1000.0   \n",
       "3                                  1000.0   \n",
       "4                                   240.0   \n",
       "\n",
       "   CURE END POSITION Z Collect Result_Dam  \\\n",
       "0                                     2.5   \n",
       "1                                     2.5   \n",
       "2                                    12.5   \n",
       "3                                    12.5   \n",
       "4                                     2.5   \n",
       "\n",
       "   CURE END POSITION Θ Collect Result_Dam  CURE SPEED Collect Result_Dam  \\\n",
       "0                                     -90                            100   \n",
       "1                                     -90                             70   \n",
       "2                                      90                             85   \n",
       "3                                      90                             70   \n",
       "4                                     -90                             70   \n",
       "\n",
       "   CURE START POSITION X Collect Result_Dam  \\\n",
       "0                                      1030   \n",
       "1                                      1030   \n",
       "2                                       280   \n",
       "3                                       280   \n",
       "4                                      1030   \n",
       "\n",
       "   CURE START POSITION Θ Collect Result_Dam  \\\n",
       "0                                       -90   \n",
       "1                                       -90   \n",
       "2                                        90   \n",
       "3                                        90   \n",
       "4                                       -90   \n",
       "\n",
       "   DISCHARGED SPEED OF RESIN Collect Result_Dam  ...  \\\n",
       "0                                            16  ...   \n",
       "1                                            10  ...   \n",
       "2                                            16  ...   \n",
       "3                                            10  ...   \n",
       "4                                            10  ...   \n",
       "\n",
       "   HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2  \\\n",
       "0                                              428.0            \n",
       "1                                              428.0            \n",
       "2                                             1324.2            \n",
       "3                                             1324.2            \n",
       "4                                              428.0            \n",
       "\n",
       "   HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2  \\\n",
       "0                                              243.7            \n",
       "1                                              243.7            \n",
       "2                                              243.5            \n",
       "3                                              243.5            \n",
       "4                                              243.7            \n",
       "\n",
       "   HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2  \\\n",
       "0                                              243.7            \n",
       "1                                              243.7            \n",
       "2                                              243.5            \n",
       "3                                              243.5            \n",
       "4                                              243.7            \n",
       "\n",
       "   HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2  \\\n",
       "0                                              243.7            \n",
       "1                                              243.7            \n",
       "2                                              243.5            \n",
       "3                                              243.5            \n",
       "4                                              243.7            \n",
       "\n",
       "   Head Purge Position Z Collect Result_Fill2  \\\n",
       "0                                     114.612   \n",
       "1                                      85.000   \n",
       "2                                     114.612   \n",
       "3                                      85.000   \n",
       "4                                      85.000   \n",
       "\n",
       "   Machine Tact time Collect Result_Fill2  PalletID Collect Result_Fill2  \\\n",
       "0                                    19.9                            7.0   \n",
       "1                                    19.6                            7.0   \n",
       "2                                    19.8                           10.0   \n",
       "3                                    19.9                           12.0   \n",
       "4                                    19.7                            8.0   \n",
       "\n",
       "   Production Qty Collect Result_Fill2  Receip No Collect Result_Fill2  target  \n",
       "0                                  127                               1  Normal  \n",
       "1                                  185                               1  Normal  \n",
       "2                                   73                               1  Normal  \n",
       "3                                  268                               1  Normal  \n",
       "4                                  121                               1  Normal  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Equipment_Dam</th>\n",
       "      <th>Model.Suffix_Dam</th>\n",
       "      <th>Workorder_Dam</th>\n",
       "      <th>CURE END POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Z Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Θ Collect Result_Dam</th>\n",
       "      <th>CURE SPEED Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION Θ Collect Result_Dam</th>\n",
       "      <th>DISCHARGED SPEED OF RESIN Collect Result_Dam</th>\n",
       "      <th>...</th>\n",
       "      <th>HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2</th>\n",
       "      <th>HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2</th>\n",
       "      <th>HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2</th>\n",
       "      <th>HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2</th>\n",
       "      <th>HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2</th>\n",
       "      <th>Head Purge Position Z Collect Result_Fill2</th>\n",
       "      <th>Machine Tact time Collect Result_Fill2</th>\n",
       "      <th>PalletID Collect Result_Fill2</th>\n",
       "      <th>Production Qty Collect Result_Fill2</th>\n",
       "      <th>Receip No Collect Result_Fill2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3J1XF767-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1324.2</td>\n",
       "      <td>1324.2</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>4B1XD472-2</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>1324.2</td>\n",
       "      <td>1324.2</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3H1XE355-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>427.9</td>\n",
       "      <td>428.0</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3L1XA128-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1324.2</td>\n",
       "      <td>1324.2</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>243.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>4A1XA639-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>427.9</td>\n",
       "      <td>428.0</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>243.7</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Equipment_Dam Model.Suffix_Dam Workorder_Dam  \\\n",
       "0  Dam dispenser #2      AJX75334501    3J1XF767-1   \n",
       "1  Dam dispenser #2      AJX75334501    4B1XD472-2   \n",
       "2  Dam dispenser #1      AJX75334501    3H1XE355-1   \n",
       "3  Dam dispenser #2      AJX75334501    3L1XA128-1   \n",
       "4  Dam dispenser #1      AJX75334501    4A1XA639-1   \n",
       "\n",
       "   CURE END POSITION X Collect Result_Dam  \\\n",
       "0                                  1000.0   \n",
       "1                                  1000.0   \n",
       "2                                   240.0   \n",
       "3                                  1000.0   \n",
       "4                                   240.0   \n",
       "\n",
       "   CURE END POSITION Z Collect Result_Dam  \\\n",
       "0                                    12.5   \n",
       "1                                    12.5   \n",
       "2                                     2.5   \n",
       "3                                    12.5   \n",
       "4                                     2.5   \n",
       "\n",
       "   CURE END POSITION Θ Collect Result_Dam  CURE SPEED Collect Result_Dam  \\\n",
       "0                                      90                             70   \n",
       "1                                      90                             70   \n",
       "2                                     -90                             70   \n",
       "3                                      90                             70   \n",
       "4                                     -90                             70   \n",
       "\n",
       "   CURE START POSITION X Collect Result_Dam  \\\n",
       "0                                       280   \n",
       "1                                       280   \n",
       "2                                      1030   \n",
       "3                                       280   \n",
       "4                                      1030   \n",
       "\n",
       "   CURE START POSITION Θ Collect Result_Dam  \\\n",
       "0                                        90   \n",
       "1                                        90   \n",
       "2                                       -90   \n",
       "3                                        90   \n",
       "4                                       -90   \n",
       "\n",
       "   DISCHARGED SPEED OF RESIN Collect Result_Dam  ...  \\\n",
       "0                                            10  ...   \n",
       "1                                            16  ...   \n",
       "2                                            10  ...   \n",
       "3                                            10  ...   \n",
       "4                                            16  ...   \n",
       "\n",
       "   HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2  \\\n",
       "0                                             1324.2            \n",
       "1                                             1324.2            \n",
       "2                                              427.9            \n",
       "3                                             1324.2            \n",
       "4                                              427.9            \n",
       "\n",
       "   HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2  \\\n",
       "0                                             1324.2            \n",
       "1                                             1324.2            \n",
       "2                                              428.0            \n",
       "3                                             1324.2            \n",
       "4                                              428.0            \n",
       "\n",
       "   HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2  \\\n",
       "0                                              243.5            \n",
       "1                                              243.5            \n",
       "2                                              243.7            \n",
       "3                                              243.5            \n",
       "4                                              243.7            \n",
       "\n",
       "   HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2  \\\n",
       "0                                              243.5            \n",
       "1                                              243.5            \n",
       "2                                              243.7            \n",
       "3                                              243.5            \n",
       "4                                              243.7            \n",
       "\n",
       "   HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2  \\\n",
       "0                                              243.5            \n",
       "1                                              243.5            \n",
       "2                                              243.7            \n",
       "3                                              243.5            \n",
       "4                                              243.7            \n",
       "\n",
       "   Head Purge Position Z Collect Result_Fill2  \\\n",
       "0                                        85.0   \n",
       "1                                        85.0   \n",
       "2                                        85.0   \n",
       "3                                        85.0   \n",
       "4                                        85.0   \n",
       "\n",
       "   Machine Tact time Collect Result_Fill2  PalletID Collect Result_Fill2  \\\n",
       "0                                    19.8                           13.0   \n",
       "1                                    19.8                           14.0   \n",
       "2                                    19.7                            1.0   \n",
       "3                                    20.0                           14.0   \n",
       "4                                    19.8                            1.0   \n",
       "\n",
       "   Production Qty Collect Result_Fill2  Receip No Collect Result_Fill2  \n",
       "0                                  195                               1  \n",
       "1                                  256                               1  \n",
       "2                                   98                               1  \n",
       "3                                    0                               1  \n",
       "4                                  215                               1  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop columns only one unique value\n",
    "drop_cols = []\n",
    "for column in df_train.columns:\n",
    "    if df_train[column].nunique() == 1:\n",
    "        drop_cols.append(column)\n",
    "        \n",
    "df_train.drop(columns = drop_cols, inplace = True)\n",
    "df_test.drop(columns = drop_cols, inplace = True)\n",
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a21bf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Equipment_Dam</th>\n",
       "      <th>Model.Suffix_Dam</th>\n",
       "      <th>Workorder_Dam</th>\n",
       "      <th>CURE END POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Z Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Θ Collect Result_Dam</th>\n",
       "      <th>CURE SPEED Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION Θ Collect Result_Dam</th>\n",
       "      <th>DISCHARGED SPEED OF RESIN Collect Result_Dam</th>\n",
       "      <th>...</th>\n",
       "      <th>Head Purge Position Z Collect Result_Fill2</th>\n",
       "      <th>Machine Tact time Collect Result_Fill2</th>\n",
       "      <th>PalletID Collect Result_Fill2</th>\n",
       "      <th>Production Qty Collect Result_Fill2</th>\n",
       "      <th>Receip No Collect Result_Fill2</th>\n",
       "      <th>target</th>\n",
       "      <th>Collect Date_Dam</th>\n",
       "      <th>Collect Date_Fill1</th>\n",
       "      <th>Collect Date_Fill2</th>\n",
       "      <th>Collect Date_AutoClave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334505</td>\n",
       "      <td>4F1XA938-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>100</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>114.612</td>\n",
       "      <td>19.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2024-04-25 11:10:00</td>\n",
       "      <td>2024-04-25 11:20:00</td>\n",
       "      <td>2024-04-25 11:20:00</td>\n",
       "      <td>2024-04-25 11:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334505</td>\n",
       "      <td>3KPM0016-2</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>85.000</td>\n",
       "      <td>19.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2023-09-19 14:30:00</td>\n",
       "      <td>2023-09-19 14:30:00</td>\n",
       "      <td>2023-09-19 14:30:00</td>\n",
       "      <td>2023-09-19 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>4E1X9167-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>114.612</td>\n",
       "      <td>19.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2024-03-05 09:30:00</td>\n",
       "      <td>2024-03-05 09:30:00</td>\n",
       "      <td>2024-03-05 09:30:00</td>\n",
       "      <td>2024-03-05 10:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3K1X0057-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>85.000</td>\n",
       "      <td>19.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2023-09-25 15:40:00</td>\n",
       "      <td>2023-09-25 15:40:00</td>\n",
       "      <td>2023-09-25 15:40:00</td>\n",
       "      <td>2023-09-25 16:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3HPM0007-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>85.000</td>\n",
       "      <td>19.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2023-06-27 13:20:00</td>\n",
       "      <td>2023-06-27 13:20:00</td>\n",
       "      <td>2023-06-27 13:20:00</td>\n",
       "      <td>2023-06-27 14:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Equipment_Dam Model.Suffix_Dam Workorder_Dam  \\\n",
       "0  Dam dispenser #1      AJX75334505    4F1XA938-1   \n",
       "1  Dam dispenser #1      AJX75334505    3KPM0016-2   \n",
       "2  Dam dispenser #2      AJX75334501    4E1X9167-1   \n",
       "3  Dam dispenser #2      AJX75334501    3K1X0057-1   \n",
       "4  Dam dispenser #1      AJX75334501    3HPM0007-1   \n",
       "\n",
       "   CURE END POSITION X Collect Result_Dam  \\\n",
       "0                                   240.0   \n",
       "1                                   240.0   \n",
       "2                                  1000.0   \n",
       "3                                  1000.0   \n",
       "4                                   240.0   \n",
       "\n",
       "   CURE END POSITION Z Collect Result_Dam  \\\n",
       "0                                     2.5   \n",
       "1                                     2.5   \n",
       "2                                    12.5   \n",
       "3                                    12.5   \n",
       "4                                     2.5   \n",
       "\n",
       "   CURE END POSITION Θ Collect Result_Dam  CURE SPEED Collect Result_Dam  \\\n",
       "0                                     -90                            100   \n",
       "1                                     -90                             70   \n",
       "2                                      90                             85   \n",
       "3                                      90                             70   \n",
       "4                                     -90                             70   \n",
       "\n",
       "   CURE START POSITION X Collect Result_Dam  \\\n",
       "0                                      1030   \n",
       "1                                      1030   \n",
       "2                                       280   \n",
       "3                                       280   \n",
       "4                                      1030   \n",
       "\n",
       "   CURE START POSITION Θ Collect Result_Dam  \\\n",
       "0                                       -90   \n",
       "1                                       -90   \n",
       "2                                        90   \n",
       "3                                        90   \n",
       "4                                       -90   \n",
       "\n",
       "   DISCHARGED SPEED OF RESIN Collect Result_Dam  ...  \\\n",
       "0                                            16  ...   \n",
       "1                                            10  ...   \n",
       "2                                            16  ...   \n",
       "3                                            10  ...   \n",
       "4                                            10  ...   \n",
       "\n",
       "   Head Purge Position Z Collect Result_Fill2  \\\n",
       "0                                     114.612   \n",
       "1                                      85.000   \n",
       "2                                     114.612   \n",
       "3                                      85.000   \n",
       "4                                      85.000   \n",
       "\n",
       "   Machine Tact time Collect Result_Fill2  PalletID Collect Result_Fill2  \\\n",
       "0                                    19.9                            7.0   \n",
       "1                                    19.6                            7.0   \n",
       "2                                    19.8                           10.0   \n",
       "3                                    19.9                           12.0   \n",
       "4                                    19.7                            8.0   \n",
       "\n",
       "   Production Qty Collect Result_Fill2  Receip No Collect Result_Fill2  \\\n",
       "0                                  127                               1   \n",
       "1                                  185                               1   \n",
       "2                                   73                               1   \n",
       "3                                  268                               1   \n",
       "4                                  121                               1   \n",
       "\n",
       "   target     Collect Date_Dam   Collect Date_Fill1   Collect Date_Fill2  \\\n",
       "0  Normal  2024-04-25 11:10:00  2024-04-25 11:20:00  2024-04-25 11:20:00   \n",
       "1  Normal  2023-09-19 14:30:00  2023-09-19 14:30:00  2023-09-19 14:30:00   \n",
       "2  Normal  2024-03-05 09:30:00  2024-03-05 09:30:00  2024-03-05 09:30:00   \n",
       "3  Normal  2023-09-25 15:40:00  2023-09-25 15:40:00  2023-09-25 15:40:00   \n",
       "4  Normal  2023-06-27 13:20:00  2023-06-27 13:20:00  2023-06-27 13:20:00   \n",
       "\n",
       "   Collect Date_AutoClave  \n",
       "0     2024-04-25 11:50:00  \n",
       "1     2023-09-19 15:00:00  \n",
       "2     2024-03-05 10:10:00  \n",
       "3     2023-09-25 16:20:00  \n",
       "4     2023-06-27 14:00:00  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Equipment_Dam</th>\n",
       "      <th>Model.Suffix_Dam</th>\n",
       "      <th>Workorder_Dam</th>\n",
       "      <th>CURE END POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Z Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Θ Collect Result_Dam</th>\n",
       "      <th>CURE SPEED Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION Θ Collect Result_Dam</th>\n",
       "      <th>DISCHARGED SPEED OF RESIN Collect Result_Dam</th>\n",
       "      <th>...</th>\n",
       "      <th>HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2</th>\n",
       "      <th>Head Purge Position Z Collect Result_Fill2</th>\n",
       "      <th>Machine Tact time Collect Result_Fill2</th>\n",
       "      <th>PalletID Collect Result_Fill2</th>\n",
       "      <th>Production Qty Collect Result_Fill2</th>\n",
       "      <th>Receip No Collect Result_Fill2</th>\n",
       "      <th>Collect Date_Dam</th>\n",
       "      <th>Collect Date_Fill1</th>\n",
       "      <th>Collect Date_Fill2</th>\n",
       "      <th>Collect Date_AutoClave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3J1XF767-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>243.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>195</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-09-15 13:20:00</td>\n",
       "      <td>2023-09-15 13:30:00</td>\n",
       "      <td>2023-09-15 13:30:00</td>\n",
       "      <td>2023-09-15 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>4B1XD472-2</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>243.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-02-06 16:50:00</td>\n",
       "      <td>2024-02-06 16:50:00</td>\n",
       "      <td>2024-02-06 16:50:00</td>\n",
       "      <td>2024-02-06 17:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3H1XE355-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>243.7</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-14 11:30:00</td>\n",
       "      <td>2023-07-14 11:40:00</td>\n",
       "      <td>2023-07-14 11:40:00</td>\n",
       "      <td>2023-07-14 12:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dam dispenser #2</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>3L1XA128-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>243.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-03 08:00:00</td>\n",
       "      <td>2023-11-03 08:00:00</td>\n",
       "      <td>2023-11-03 08:00:00</td>\n",
       "      <td>2023-11-03 08:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dam dispenser #1</td>\n",
       "      <td>AJX75334501</td>\n",
       "      <td>4A1XA639-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>-90</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>243.7</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-12-23 14:00:00</td>\n",
       "      <td>2023-12-23 14:00:00</td>\n",
       "      <td>2023-12-23 14:10:00</td>\n",
       "      <td>2023-12-23 14:40:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Equipment_Dam Model.Suffix_Dam Workorder_Dam  \\\n",
       "0  Dam dispenser #2      AJX75334501    3J1XF767-1   \n",
       "1  Dam dispenser #2      AJX75334501    4B1XD472-2   \n",
       "2  Dam dispenser #1      AJX75334501    3H1XE355-1   \n",
       "3  Dam dispenser #2      AJX75334501    3L1XA128-1   \n",
       "4  Dam dispenser #1      AJX75334501    4A1XA639-1   \n",
       "\n",
       "   CURE END POSITION X Collect Result_Dam  \\\n",
       "0                                  1000.0   \n",
       "1                                  1000.0   \n",
       "2                                   240.0   \n",
       "3                                  1000.0   \n",
       "4                                   240.0   \n",
       "\n",
       "   CURE END POSITION Z Collect Result_Dam  \\\n",
       "0                                    12.5   \n",
       "1                                    12.5   \n",
       "2                                     2.5   \n",
       "3                                    12.5   \n",
       "4                                     2.5   \n",
       "\n",
       "   CURE END POSITION Θ Collect Result_Dam  CURE SPEED Collect Result_Dam  \\\n",
       "0                                      90                             70   \n",
       "1                                      90                             70   \n",
       "2                                     -90                             70   \n",
       "3                                      90                             70   \n",
       "4                                     -90                             70   \n",
       "\n",
       "   CURE START POSITION X Collect Result_Dam  \\\n",
       "0                                       280   \n",
       "1                                       280   \n",
       "2                                      1030   \n",
       "3                                       280   \n",
       "4                                      1030   \n",
       "\n",
       "   CURE START POSITION Θ Collect Result_Dam  \\\n",
       "0                                        90   \n",
       "1                                        90   \n",
       "2                                       -90   \n",
       "3                                        90   \n",
       "4                                       -90   \n",
       "\n",
       "   DISCHARGED SPEED OF RESIN Collect Result_Dam  ...  \\\n",
       "0                                            10  ...   \n",
       "1                                            16  ...   \n",
       "2                                            10  ...   \n",
       "3                                            10  ...   \n",
       "4                                            16  ...   \n",
       "\n",
       "   HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2  \\\n",
       "0                                              243.5            \n",
       "1                                              243.5            \n",
       "2                                              243.7            \n",
       "3                                              243.5            \n",
       "4                                              243.7            \n",
       "\n",
       "   Head Purge Position Z Collect Result_Fill2  \\\n",
       "0                                        85.0   \n",
       "1                                        85.0   \n",
       "2                                        85.0   \n",
       "3                                        85.0   \n",
       "4                                        85.0   \n",
       "\n",
       "   Machine Tact time Collect Result_Fill2  PalletID Collect Result_Fill2  \\\n",
       "0                                    19.8                           13.0   \n",
       "1                                    19.8                           14.0   \n",
       "2                                    19.7                            1.0   \n",
       "3                                    20.0                           14.0   \n",
       "4                                    19.8                            1.0   \n",
       "\n",
       "   Production Qty Collect Result_Fill2  Receip No Collect Result_Fill2  \\\n",
       "0                                  195                               1   \n",
       "1                                  256                               1   \n",
       "2                                   98                               1   \n",
       "3                                    0                               1   \n",
       "4                                  215                               1   \n",
       "\n",
       "      Collect Date_Dam   Collect Date_Fill1   Collect Date_Fill2  \\\n",
       "0  2023-09-15 13:20:00  2023-09-15 13:30:00  2023-09-15 13:30:00   \n",
       "1  2024-02-06 16:50:00  2024-02-06 16:50:00  2024-02-06 16:50:00   \n",
       "2  2023-07-14 11:30:00  2023-07-14 11:40:00  2023-07-14 11:40:00   \n",
       "3  2023-11-03 08:00:00  2023-11-03 08:00:00  2023-11-03 08:00:00   \n",
       "4  2023-12-23 14:00:00  2023-12-23 14:00:00  2023-12-23 14:10:00   \n",
       "\n",
       "   Collect Date_AutoClave  \n",
       "0     2023-09-15 14:00:00  \n",
       "1     2024-02-06 17:30:00  \n",
       "2     2023-07-14 12:10:00  \n",
       "3     2023-11-03 08:30:00  \n",
       "4     2023-12-23 14:40:00  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Time rematching\n",
    "df_train = pd.concat([df_train, time_features_tr], axis = 1)\n",
    "df_test = pd.concat([df_test, time_features_te], axis = 1)\n",
    "\n",
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "355e52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Int to Object Transformation\n",
    "transform_col = [\n",
    "                'Production Qty Collect Result_Dam','Production Qty Collect Result_Fill1','Production Qty Collect Result_Fill2',\n",
    "                'PalletID Collect Result_Dam','PalletID Collect Result_Fill1','PalletID Collect Result_Fill2',\n",
    "                'Receip No Collect Result_Dam','Receip No Collect Result_Fill1','Receip No Collect Result_Fill2']\n",
    "\n",
    "df_train[transform_col] = df_train[transform_col].astype(int).astype(object)\n",
    "df_test[transform_col] = df_test[transform_col].astype(int).astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d3f098ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#중복 columns 처리\n",
    "##Equipment, Model.Suffix, Workorder 통일화\n",
    "df_train.rename(columns = {'Equipment_Dam':'Equipment', 'Model.Suffix_Dam':'Model.Suffix', 'Workorder_Dam':'Workorder'}, inplace = True)\n",
    "df_train.drop(columns = ['Equipment_Fill1', 'Equipment_Fill2',\n",
    "                        'Model.Suffix_AutoClave', 'Model.Suffix_Fill1', 'Model.Suffix_Fill2',\n",
    "                        'Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2'], inplace = True)\n",
    "df_test.rename(columns = {'Equipment_Dam':'Equipment', 'Model.Suffix_Dam':'Model.Suffix', 'Workorder_Dam':'Workorder'}, inplace = True)\n",
    "df_test.drop(columns = ['Equipment_Fill1', 'Equipment_Fill2',\n",
    "                        'Model.Suffix_AutoClave', 'Model.Suffix_Fill1', 'Model.Suffix_Fill2',\n",
    "                        'Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2'], inplace = True)\n",
    "\n",
    "df_train['Equipment'] = df_train['Equipment'].map({'Dam dispenser #1':'Dispenser_1', 'Dam dispenser #2':'Dispneser_2'})\n",
    "df_test['Equipment'] = df_test['Equipment'].map({'Dam dispenser #1':'Dispenser_1', 'Dam dispenser #2':'Dispneser_2'})\n",
    "\n",
    "##Production Qty, PalletID, Receip No 통일화\n",
    "df_train.rename(columns = {'Production Qty Collect Result_Dam':'Production Qty Collect Result', \n",
    "                           'PalletID Collect Result_Dam':'PalletID Collect Result',\n",
    "                          'Receip No Collect Result_Dam':'Receip No Collect Result'}, inplace = True)\n",
    "df_train.drop(columns = ['Production Qty Collect Result_Fill1', 'Production Qty Collect Result_Fill2',\n",
    "                        'PalletID Collect Result_Fill1', 'PalletID Collect Result_Fill2',\n",
    "                        'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'], inplace = True)\n",
    "df_test.rename(columns = {'Production Qty Collect Result_Dam':'Production Qty Collect Result', \n",
    "                           'PalletID Collect Result_Dam':'PalletID Collect Result',\n",
    "                          'Receip No Collect Result_Dam':'Receip No Collect Result'}, inplace = True)\n",
    "df_test.drop(columns = ['Production Qty Collect Result_Fill1', 'Production Qty Collect Result_Fill2',\n",
    "                        'PalletID Collect Result_Fill1', 'PalletID Collect Result_Fill2',\n",
    "                        'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3a8ddce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Work_Equip'] = df_train['Equipment'] + df_train['Workorder']\n",
    "df_test['Work_Equip'] = df_test['Equipment'] + df_test['Workorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ba6d1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equipment/PalletID, Model.Suffix/Receip No 통일화\n",
    "df_train.rename(columns = {'Equipment':'Equipment_PalletID', \n",
    "                           'Model.Suffix':'Model_Receip'}, inplace = True)\n",
    "df_train['Equipment_PalletID'] = df_train['Equipment_PalletID'] + '_' + df_train['PalletID Collect Result'].astype(str)\n",
    "df_train['Model_Receip'] = df_train['Model_Receip'] + '_' + df_train['Receip No Collect Result'].astype(str)\n",
    "df_test.rename(columns = {'Equipment':'Equipment_PalletID', \n",
    "                           'Model.Suffix':'Model_Receip'}, inplace = True)\n",
    "df_test['Equipment_PalletID'] = df_test['Equipment_PalletID'] + '_' + df_test['PalletID Collect Result'].astype(str)\n",
    "df_test['Model_Receip'] = df_test['Model_Receip'] + '_' + df_test['Receip No Collect Result'].astype(str)\n",
    "\n",
    "df_train.drop(columns = ['PalletID Collect Result', 'Receip No Collect Result'], inplace = True)\n",
    "df_test.drop(columns = ['PalletID Collect Result', 'Receip No Collect Result'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a532492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 663/663 [00:06<00:00, 109.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#Production Qty Transformation\n",
    "for lst in tqdm(df_train['Workorder'].unique()):\n",
    "    \n",
    "    tr = df_train[df_train['Workorder'] == lst]\n",
    "    te = df_test[df_test['Workorder'] == lst]\n",
    "\n",
    "    tr['Production_Sequence_Ratio'] = tr['Production Qty Collect Result'] / tr['Production Qty Collect Result'].max()\n",
    "    te['Production_Sequence_Ratio'] = te['Production Qty Collect Result'] / tr['Production Qty Collect Result'].max()\n",
    "    \n",
    "    tr['sin_sequence'] = np.sin(2 * np.pi * tr['Production Qty Collect Result'].astype(float) / 59.0)\n",
    "    te['sin_sequence'] = np.sin(2 * np.pi * te['Production Qty Collect Result'].astype(float) / 59.0)\n",
    "    \n",
    "    tr['cos_sequence'] = np.cos(2 * np.pi * tr['Production Qty Collect Result'].astype(float) / 59.0)\n",
    "    te['cos_sequence'] = np.cos(2 * np.pi * te['Production Qty Collect Result'].astype(float) / 59.0)\n",
    "    \n",
    "    df_train.loc[tr.index,'Production_Sequence_ratio'] = tr['Production_Sequence_Ratio']\n",
    "    df_test.loc[te.index,'Production_Sequence_ratio'] = te['Production_Sequence_Ratio']\n",
    "\n",
    "    df_train.loc[tr.index,'sin_sequence'] = tr['sin_sequence']\n",
    "    df_test.loc[te.index,'sin_sequence'] = te['sin_sequence']\n",
    "    \n",
    "    df_train.loc[tr.index,'cos_sequence'] = tr['cos_sequence']\n",
    "    df_test.loc[te.index,'cos_sequence'] = te['cos_sequence'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510e02e",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24486a5",
   "metadata": {},
   "source": [
    "### Model detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bfc851ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#패턴 통일\n",
    "def remove_zeros(input_string):\n",
    "    # 패턴 정의: - 문자 뒤에 000이 있는 경우\n",
    "    pattern = r'-(000)'\n",
    "\n",
    "    # re.sub를 사용하여 패턴을 대체\n",
    "    modified_string = re.sub(pattern, '-', input_string)\n",
    "    return modified_string\n",
    "\n",
    "df_train['Workorder'] = df_train['Workorder'].apply(lambda x : remove_zeros(x))\n",
    "df_test['Workorder'] = df_test['Workorder'].apply(lambda x : remove_zeros(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa0e0b",
   "metadata": {},
   "source": [
    "### Dam Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492eaab7",
   "metadata": {},
   "source": [
    "#### Workorder & Production Qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dc77609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문자열 정보 추출\n",
    "df_train['Wo_Number'] = df_train['Workorder'].apply(lambda x : x[:1]).astype(str)\n",
    "df_train['number_alpha'] = df_train['Workorder'].apply(lambda x : x[:2]).astype(str)\n",
    "df_train['alpha'] = df_train['Workorder'].apply(lambda x : x[1]).astype(str)\n",
    "df_train['Wo_Main'] = df_train['Workorder'].apply(lambda x : x[:3]).astype(str)\n",
    "df_train['Wo_Sub'] = df_train['Workorder'].apply(lambda x : x[3]).astype(str)\n",
    "df_train['Wo_Detail_1'] = df_train['Workorder'].apply(lambda x : x[4]).astype(str)\n",
    "df_train['Wo_Detail_2'] = df_train['Workorder'].apply(lambda x : x[5:]).astype(str)\n",
    "df_train['last'] = df_train['Workorder'].apply(lambda x : x[-1]).astype(str)\n",
    "\n",
    "df_train['sj_1'] = df_train['Workorder'].apply(lambda x : x[:4]).astype(str)\n",
    "df_train['sj_5'] = df_train['Workorder'].apply(lambda x : x[:8]).astype(str)\n",
    "df_train['sj_6'] = df_train['Workorder'].apply(lambda x : x[2]).astype(str)\n",
    "df_train['sj_7'] = df_train['Workorder'].apply(lambda x : x[5]).astype(str)\n",
    "df_train['sj_8'] = df_train['Workorder'].apply(lambda x : x[4:8]).astype(str)\n",
    "df_train['sj_9'] = df_train['Workorder'].apply(lambda x : x[-5:-2]).astype(str)\n",
    "\n",
    "df_test['Wo_Number'] = df_test['Workorder'].apply(lambda x : x[:1]).astype(str)\n",
    "df_test['number_alpha'] = df_test['Workorder'].apply(lambda x : x[:2]).astype(str)\n",
    "df_test['alpha'] = df_test['Workorder'].apply(lambda x : x[1]).astype(str)\n",
    "df_test['Wo_Main'] = df_test['Workorder'].apply(lambda x : x[:3]).astype(str)\n",
    "df_test['Wo_Sub'] = df_test['Workorder'].apply(lambda x : x[3]).astype(str)\n",
    "df_test['Wo_Detail_1'] = df_test['Workorder'].apply(lambda x : x[4]).astype(str)\n",
    "df_test['Wo_Detail_2'] = df_test['Workorder'].apply(lambda x : x[5:]).astype(str)\n",
    "df_test['last'] = df_test['Workorder'].apply(lambda x : x[-1]).astype(str)\n",
    "\n",
    "df_test['sj_1'] = df_test['Workorder'].apply(lambda x : x[:4]).astype(str)\n",
    "df_test['sj_5'] = df_test['Workorder'].apply(lambda x : x[:8]).astype(str)\n",
    "df_test['sj_6'] = df_test['Workorder'].apply(lambda x : x[2]).astype(str)\n",
    "df_test['sj_7'] = df_test['Workorder'].apply(lambda x : x[5]).astype(str)\n",
    "df_test['sj_8'] = df_test['Workorder'].apply(lambda x : x[4:8]).astype(str)\n",
    "df_test['sj_9'] = df_test['Workorder'].apply(lambda x : x[-5:-2]).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b6f08",
   "metadata": {},
   "source": [
    "#### Signal combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "32976cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CURE(Dam)\n",
    "##CURE 가동 방향 및 가동높이\n",
    "df_train['Cure_HD_Dam'] = df_train['CURE END POSITION X Collect Result_Dam'] - df_train['CURE START POSITION X Collect Result_Dam']\n",
    "df_test['Cure_HD_Dam'] = df_test['CURE END POSITION X Collect Result_Dam'] - df_test['CURE START POSITION X Collect Result_Dam']\n",
    "\n",
    "##CURE 가동 시간\n",
    "df_train['Cure_time_Dam'] = df_train['Cure_HD_Dam'] / df_train['CURE SPEED Collect Result_Dam']\n",
    "df_test['Cure_time_Dam'] = df_test['Cure_HD_Dam'] / df_test['CURE SPEED Collect Result_Dam']\n",
    "\n",
    "##CURE position category\n",
    "df_train['Cure_position_category_Dam'] = df_train['CURE END POSITION X Collect Result_Dam'].map({240.0:'Down',1000.0:'Up'}).astype(str)\n",
    "df_test['Cure_position_category_Dam'] = df_test['CURE END POSITION X Collect Result_Dam'].map({240.0:'Down',1000.0:'Up'}).astype(str)\n",
    "\n",
    "##CURE Hysteresis\n",
    "def f_H(x):\n",
    "    return np.arccos(1 - 2 * np.tanh(5.16 * (x / (1 + 1.31 * x**0.99))**0.706))\n",
    "\n",
    "u_c = df_train['CURE SPEED Collect Result_Dam']\n",
    "theta_e_1 = df_train['CURE START POSITION Θ Collect Result_Dam']\n",
    "\n",
    "mu = 1\n",
    "gamma = 1\n",
    "df_train['Theta_d_1_Dam'] = f_H((mu * u_c / gamma) + f_H(theta_e_1))\n",
    "\n",
    "u_c = df_test['CURE SPEED Collect Result_Dam']\n",
    "theta_e_1 = df_test['CURE START POSITION Θ Collect Result_Dam']  # equilibrium contact angle 데이터 배열\n",
    "\n",
    "df_test['Theta_d_1_Dam'] = f_H((mu * u_c / gamma) + f_H(theta_e_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "32ab7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCHARGED RESIN(Dam)\n",
    "##Resin 토출기 이동거리\n",
    "df_train['Discharged_Resin_distance_1_Dam'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "df_train['Discharged_Resin_distance_2_Dam'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "df_train['Discharged_Resin_distance_3_Dam'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "df_test['Discharged_Resin_distance_1_Dam'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "df_test['Discharged_Resin_distance_2_Dam'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "df_test['Discharged_Resin_distance_3_Dam'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "\n",
    "##Resin 토출 총 소요 시간\n",
    "df_train['TotalTime_Discharged_Resin_Dam'] = df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam'] + df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam'] + df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "df_test['TotalTime_Discharged_Resin_Dam'] = df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam'] + df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam'] + df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c565a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPENSE(Dam)\n",
    "##Dispensor 크기 변화\n",
    "df_train['Dispense_volume_change1_Dam'] = df_train['Dispense Volume(Stage2) Collect Result_Dam'] - df_train['Dispense Volume(Stage1) Collect Result_Dam']\n",
    "df_train['Dispense_volume_change2_Dam'] = df_train['Dispense Volume(Stage3) Collect Result_Dam'] - df_train['Dispense Volume(Stage2) Collect Result_Dam']\n",
    "df_test['Dispense_volume_change1_Dam'] = df_test['Dispense Volume(Stage2) Collect Result_Dam'] - df_test['Dispense Volume(Stage1) Collect Result_Dam']\n",
    "df_test['Dispense_volume_change2_Dam'] = df_test['Dispense Volume(Stage3) Collect Result_Dam'] - df_test['Dispense Volume(Stage2) Collect Result_Dam']\n",
    "\n",
    "##Dispensor 크기 변화량\n",
    "df_train['Dispense_volume_change1_abs_Dam'] = df_train['Dispense_volume_change1_Dam'].abs()\n",
    "df_train['Dispense_volume_change2_abs_Dam'] = df_train['Dispense_volume_change2_Dam'].abs()\n",
    "df_test['Dispense_volume_change1_abs_Dam'] = df_test['Dispense_volume_change1_Dam'].abs()\n",
    "df_test['Dispense_volume_change2_abs_Dam'] = df_test['Dispense_volume_change2_Dam'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19681c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCHARGED RESIN & DISPENSE (Dam)\n",
    "##도포된 Resin 양\n",
    "df_train['Total_Resin_volume1_Dam'] = df_train['Discharged_Resin_distance_1_Dam'] * df_train['Dispense Volume(Stage1) Collect Result_Dam']\n",
    "df_train['Total_Resin_volume2_Dam'] = df_train['Discharged_Resin_distance_2_Dam'] * df_train['Dispense Volume(Stage2) Collect Result_Dam']\n",
    "df_train['Total_Resin_volume3_Dam'] = df_train['Discharged_Resin_distance_3_Dam'] * df_train['Dispense Volume(Stage3) Collect Result_Dam']\n",
    "df_test['Total_Resin_volume1_Dam'] = df_test['Discharged_Resin_distance_1_Dam'] * df_test['Dispense Volume(Stage1) Collect Result_Dam']\n",
    "df_test['Total_Resin_volume2_Dam'] = df_test['Discharged_Resin_distance_2_Dam'] * df_test['Dispense Volume(Stage2) Collect Result_Dam']\n",
    "df_test['Total_Resin_volume3_Dam'] = df_test['Discharged_Resin_distance_3_Dam'] * df_test['Dispense Volume(Stage3) Collect Result_Dam']\n",
    "\n",
    "##시간당 토출량\n",
    "df_train['Stage1_Dam_Volume_Speed'] = df_train['Dispense Volume(Stage1) Collect Result_Dam'] / df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "df_train['Stage2_Dam_Volume_Speed'] = df_train['Dispense Volume(Stage2) Collect Result_Dam'] / df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "df_train['Stage3_Dam_Volume_Speed'] = df_train['Dispense Volume(Stage3) Collect Result_Dam'] / df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "df_test['Stage1_Dam_Volume_Speed'] = df_test['Dispense Volume(Stage1) Collect Result_Dam'] / df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "df_test['Stage2_Dam_Volume_Speed'] = df_test['Dispense Volume(Stage2) Collect Result_Dam'] / df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "df_test['Stage3_Dam_Volume_Speed'] = df_test['Dispense Volume(Stage3) Collect Result_Dam'] / df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef92c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEAD(Dam)\n",
    "##Dam X,Y,Z\n",
    "df_train['Dam_X_1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam']\n",
    "df_train['Dam_X_2'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "df_train['Dam_X_3'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "df_test['Dam_X_1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam']\n",
    "df_test['Dam_X_2'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "df_test['Dam_X_3'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_train['Dam_Y_1'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "df_train['Dam_Y_2'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "df_train['Dam_Y_3'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "df_test['Dam_Y_1'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "df_test['Dam_Y_2'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "df_test['Dam_Y_3'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_train['Dam_dist_1'] = np.sqrt(df_train['Dam_X_1']**2 +df_train['Dam_Y_1']**2)\n",
    "df_train['Dam_dist_2'] = np.sqrt(df_train['Dam_X_2']**2 +df_train['Dam_Y_2']**2)\n",
    "df_train['Dam_dist_3'] = np.sqrt(df_train['Dam_X_3']**2 +df_train['Dam_Y_3']**2)\n",
    "df_test['Dam_dist_1'] = np.sqrt(df_test['Dam_X_1']**2 +df_test['Dam_Y_1']**2)\n",
    "df_test['Dam_dist_2'] = np.sqrt(df_test['Dam_X_2']**2 +df_test['Dam_Y_2']**2)\n",
    "df_test['Dam_dist_3'] = np.sqrt(df_test['Dam_X_3']**2 +df_test['Dam_Y_3']**2)\n",
    "\n",
    "##노즐 분사 벡터 크기 & 각도\n",
    "def calculate_magnitude(a,b,c):\n",
    "    # 벡터 크기 계산\n",
    "    return np.sqrt(a**2 + b**2 +c**2)\n",
    "\n",
    "def calculate_angles(a,b,c):\n",
    "    # 벡터 크기\n",
    "    magnitude = calculate_magnitude(a,b,c)\n",
    "    \n",
    "    # 각 좌표축과의 각도 (cosine)\n",
    "    angle_x = a / magnitude\n",
    "    angle_y = b / magnitude\n",
    "    angle_z = c / magnitude\n",
    "    \n",
    "    return angle_x, angle_y, angle_z\n",
    "\n",
    "df_train['Vector_Dam_Stage1']= calculate_magnitude(df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'])\n",
    "df_train['Vector_Dam_Stage2']= calculate_magnitude(df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'])\n",
    "df_train['Vector_Dam_Stage3']= calculate_magnitude(df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'])\n",
    "df_test['Vector_Dam_Stage1']= calculate_magnitude(df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'])\n",
    "df_test['Vector_Dam_Stage2']= calculate_magnitude(df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'])\n",
    "df_test['Vector_Dam_Stage3']= calculate_magnitude(df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'])\n",
    "\n",
    "df_train['Angle_x_Dam_Stage1'],df_train['Angle_y_Dam_Stage1'],df_train['Angle_z_Dam_Stage1'] = calculate_angles(df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'])\n",
    "df_train['Angle_x_Dam_Stage2'] ,df_train['Angle_y_Dam_Stage2'],df_train['Angle_z_Dam_Stage2'] = calculate_angles(df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'])\n",
    "df_train['Angle_x_Dam_Stage3'] ,df_train['Angle_y_Dam_Stage3'],df_train['Angle_z_Dam_Stage3'] = calculate_angles(df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'])\n",
    "df_test['Angle_x_Dam_Stage1'],df_test['Angle_y_Dam_Stage1'],df_test['Angle_z_Dam_Stage1'] = calculate_angles(df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'])\n",
    "df_test['Angle_x_Dam_Stage2'] ,df_test['Angle_y_Dam_Stage2'],df_test['Angle_z_Dam_Stage2'] = calculate_angles(df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'])\n",
    "df_test['Angle_x_Dam_Stage3'] ,df_test['Angle_y_Dam_Stage3'],df_test['Angle_z_Dam_Stage3'] = calculate_angles(df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ee2ea3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Circle & Line Distance Speed(Dam)\n",
    "##Circle speed sum\n",
    "df_train['Stage1_Circle_sum_Speed_Dam'] = df_train['Stage1 Circle1 Distance Speed Collect Result_Dam'] + df_train['Stage1 Circle2 Distance Speed Collect Result_Dam'] + df_train['Stage1 Circle3 Distance Speed Collect Result_Dam'] + df_train['Stage1 Circle4 Distance Speed Collect Result_Dam']\n",
    "df_train['Stage2_Circle_sum_Speed_Dam'] = df_train['Stage2 Circle1 Distance Speed Collect Result_Dam'] + df_train['Stage2 Circle2 Distance Speed Collect Result_Dam'] + df_train['Stage2 Circle3 Distance Speed Collect Result_Dam'] + df_train['Stage2 Circle4 Distance Speed Collect Result_Dam']\n",
    "df_train['Stage3_Circle_sum_Speed_Dam'] = df_train['Stage3 Circle1 Distance Speed Collect Result_Dam'] + df_train['Stage3 Circle2 Distance Speed Collect Result_Dam'] + df_train['Stage3 Circle3 Distance Speed Collect Result_Dam'] + df_train['Stage3 Circle4 Distance Speed Collect Result_Dam']\n",
    "df_test['Stage1_Circle_sum_Speed_Dam'] = df_test['Stage1 Circle1 Distance Speed Collect Result_Dam'] + df_test['Stage1 Circle2 Distance Speed Collect Result_Dam'] + df_test['Stage1 Circle3 Distance Speed Collect Result_Dam'] + df_test['Stage1 Circle4 Distance Speed Collect Result_Dam']\n",
    "df_test['Stage2_Circle_sum_Speed_Dam'] = df_test['Stage2 Circle1 Distance Speed Collect Result_Dam'] + df_test['Stage2 Circle2 Distance Speed Collect Result_Dam'] + df_test['Stage2 Circle3 Distance Speed Collect Result_Dam'] + df_test['Stage2 Circle4 Distance Speed Collect Result_Dam']\n",
    "df_test['Stage3_Circle_sum_Speed_Dam'] = df_test['Stage3 Circle1 Distance Speed Collect Result_Dam'] + df_test['Stage3 Circle2 Distance Speed Collect Result_Dam'] + df_test['Stage3 Circle3 Distance Speed Collect Result_Dam'] + df_test['Stage3 Circle4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "##Line speed sum\n",
    "df_train['Stage1_Line_sum_Speed_Dam'] = df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line4 Distance Speed Collect Result_Dam']\n",
    "df_train['Stage2_Line_sum_Speed_Dam'] = df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line4 Distance Speed Collect Result_Dam']\n",
    "df_train['Stage3_Line_sum_Speed_Dam'] = df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line4 Distance Speed Collect Result_Dam']\n",
    "df_test['Stage1_Line_sum_Speed_Dam'] = df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line4 Distance Speed Collect Result_Dam']\n",
    "df_test['Stage2_Line_sum_Speed_Dam'] = df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line4 Distance Speed Collect Result_Dam']\n",
    "df_test['Stage3_Line_sum_Speed_Dam'] = df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "##Diff from Circle to Line\n",
    "df_train['Abs_speed_Stage1_Dam'] = df_train['Stage1_Circle_sum_Speed_Dam'] - df_train['Stage1_Line_sum_Speed_Dam']\n",
    "df_train['Abs_speed_Stage2_Dam'] = df_train['Stage2_Circle_sum_Speed_Dam'] - df_train['Stage2_Line_sum_Speed_Dam']\n",
    "df_train['Abs_speed_Stage3_Dam'] = df_train['Stage3_Circle_sum_Speed_Dam'] - df_train['Stage3_Line_sum_Speed_Dam']\n",
    "df_test['Abs_speed_Stage1_Dam'] = df_test['Stage1_Circle_sum_Speed_Dam'] - df_test['Stage1_Line_sum_Speed_Dam']\n",
    "df_test['Abs_speed_Stage2_Dam'] = df_test['Stage2_Circle_sum_Speed_Dam'] - df_test['Stage2_Line_sum_Speed_Dam']\n",
    "df_test['Abs_speed_Stage3_Dam'] = df_test['Stage3_Circle_sum_Speed_Dam'] - df_test['Stage3_Line_sum_Speed_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aaa87e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tact Time & Discharged Resin\n",
    "##Diff from Tact Time to Discharged Time\n",
    "df_train['Abs_Tact_discharged_time_Dam'] = df_train['Machine Tact time Collect Result_Dam'] - df_train['TotalTime_Discharged_Resin_Dam']\n",
    "df_test['Abs_Tact_discharged_time_Dam'] = df_test['Machine Tact time Collect Result_Dam'] - df_test['TotalTime_Discharged_Resin_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2225ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thickness(Dam)\n",
    "##Diff Thickness\n",
    "df_train['THICKNESS_abs_Dam_1'] = df_train['THICKNESS 1 Collect Result_Dam'] - df_train['THICKNESS 2 Collect Result_Dam']\n",
    "df_train['THICKNESS_abs_Dam_2'] = df_train['THICKNESS 1 Collect Result_Dam'] - df_train['THICKNESS 3 Collect Result_Dam']\n",
    "df_train['THICKNESS_abs_Dam_3'] = df_train['THICKNESS 2 Collect Result_Dam'] - df_train['THICKNESS 3 Collect Result_Dam']\n",
    "df_test['THICKNESS_abs_Dam_1'] = df_test['THICKNESS 1 Collect Result_Dam'] - df_test['THICKNESS 2 Collect Result_Dam']\n",
    "df_test['THICKNESS_abs_Dam_2'] = df_test['THICKNESS 1 Collect Result_Dam'] - df_test['THICKNESS 3 Collect Result_Dam']\n",
    "df_test['THICKNESS_abs_Dam_3'] = df_test['THICKNESS 2 Collect Result_Dam'] - df_test['THICKNESS 3 Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145958b8",
   "metadata": {},
   "source": [
    "### AutoClave Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6db60474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#탈포 압력 값 정제\n",
    "df_train['AutoClave_1st_Pressure_str'] = df_train['1st Pressure 1st Pressure Unit Time_AutoClave'].astype(str).str[-1].astype('float')\n",
    "df_train['1st Pressure Unit Time_AutoClave_new'] = df_train['1st Pressure 1st Pressure Unit Time_AutoClave'].round(-1)\n",
    "df_train['AutoClave_2nd_Pressure_str'] = df_train['2nd Pressure Unit Time_AutoClave'].astype(str).str[-1].astype('float')\n",
    "df_train['2nd Pressure Unit Time_AutoClave_new'] = df_train['2nd Pressure Unit Time_AutoClave'].round(-1)\n",
    "df_train['AutoClave_3rd_Pressure_str'] = df_train['3rd Pressure Unit Time_AutoClave'].astype(str).str[-1].astype('float')\n",
    "df_train['3rd Pressure Unit Time_AutoClave_new'] = df_train['3rd Pressure Unit Time_AutoClave'].round(-1)\n",
    "df_test['AutoClave_1st_Pressure_str'] = df_test['1st Pressure 1st Pressure Unit Time_AutoClave'].astype(str).str[-1].astype('float')\n",
    "df_test['1st Pressure Unit Time_AutoClave_new'] = df_test['1st Pressure 1st Pressure Unit Time_AutoClave'].round(-1)\n",
    "df_test['AutoClave_2nd_Pressure_str'] = df_test['2nd Pressure Unit Time_AutoClave'].astype(str).str[-1].astype('float')\n",
    "df_test['2nd Pressure Unit Time_AutoClave_new'] = df_test['2nd Pressure Unit Time_AutoClave'].round(-1)\n",
    "df_test['AutoClave_3rd_Pressure_str'] = df_test['3rd Pressure Unit Time_AutoClave'].astype(str).str[-1].astype('float')\n",
    "df_test['3rd Pressure Unit Time_AutoClave_new'] = df_test['3rd Pressure Unit Time_AutoClave'].round(-1)\n",
    "\n",
    "##Pressure str sum\n",
    "df_train['AutoClave_Pressure_str_Sum']= df_train['AutoClave_1st_Pressure_str'] + df_train['AutoClave_2nd_Pressure_str'] + df_train['AutoClave_3rd_Pressure_str']\n",
    "df_test['AutoClave_Pressure_str_Sum']= df_test['AutoClave_1st_Pressure_str'] + df_test['AutoClave_2nd_Pressure_str'] + df_test['AutoClave_3rd_Pressure_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0cca24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#탈포 압력 & 시간\n",
    "##Slope of Pressure-Time Curve\n",
    "df_train['Pressure_Time_slope1_AutoClave'] = df_train['1st Pressure Collect Result_AutoClave'] / (df_train['1st Pressure 1st Pressure Unit Time_AutoClave'] + 1)\n",
    "df_train['Pressure_Time_slope2_AutoClave'] = (df_train['2nd Pressure Collect Result_AutoClave'] - df_train['1st Pressure Collect Result_AutoClave']) / (df_train['2nd Pressure Unit Time_AutoClave'] + 1)\n",
    "df_train['Pressure_Time_slope3_AutoClave'] = (df_train['3rd Pressure Collect Result_AutoClave'] - df_train['2nd Pressure Collect Result_AutoClave']) / (df_train['3rd Pressure Unit Time_AutoClave'] + 1)\n",
    "df_test['Pressure_Time_slope1_AutoClave'] = df_test['1st Pressure Collect Result_AutoClave'] / (df_test['1st Pressure 1st Pressure Unit Time_AutoClave'] + 1)\n",
    "df_test['Pressure_Time_slope2_AutoClave'] = (df_test['2nd Pressure Collect Result_AutoClave'] - df_test['1st Pressure Collect Result_AutoClave']) / (df_test['2nd Pressure Unit Time_AutoClave'] + 1)\n",
    "df_test['Pressure_Time_slope3_AutoClave'] = (df_test['3rd Pressure Collect Result_AutoClave'] - df_test['2nd Pressure Collect Result_AutoClave']) / (df_test['3rd Pressure Unit Time_AutoClave'] + 1)\n",
    "\n",
    "##Total Pressure Change\n",
    "df_train['Total_Pressure_change_AutoClave'] = df_train['3rd Pressure Collect Result_AutoClave'] - df_train['1st Pressure Collect Result_AutoClave']\n",
    "df_test['Total_Pressure_change_AutoClave'] = df_test['3rd Pressure Collect Result_AutoClave'] - df_test['1st Pressure Collect Result_AutoClave']\n",
    "\n",
    "##Total Pressure\n",
    "df_train['Total_Pressure_AutoClave'] = df_train['1st Pressure Collect Result_AutoClave'] + df_train['2nd Pressure Collect Result_AutoClave'] + df_train['3rd Pressure Collect Result_AutoClave'] \n",
    "df_test['Total_Pressure_AutoClave'] = df_test['1st Pressure Collect Result_AutoClave'] + df_test['2nd Pressure Collect Result_AutoClave'] + df_test['3rd Pressure Collect Result_AutoClave'] \n",
    "\n",
    "##Average Pressure\n",
    "df_train['Average_Pressure_AutoClave'] = (df_train['1st Pressure Collect Result_AutoClave'] + df_train['2nd Pressure Collect Result_AutoClave'] + df_train['3rd Pressure Collect Result_AutoClave']) / 3\n",
    "df_test['Average_Pressure_AutoClave'] = (df_test['1st Pressure Collect Result_AutoClave'] + df_test['2nd Pressure Collect Result_AutoClave'] + df_test['3rd Pressure Collect Result_AutoClave']) / 3\n",
    "\n",
    "##Totel Pressure Time\n",
    "df_train['Total_pressure_time_AutoClave'] = df_train['1st Pressure 1st Pressure Unit Time_AutoClave'] + df_train['2nd Pressure Unit Time_AutoClave'] + df_train['3rd Pressure Unit Time_AutoClave']\n",
    "df_test['Total_pressure_time_AutoClave'] = df_test['1st Pressure 1st Pressure Unit Time_AutoClave'] + df_test['2nd Pressure Unit Time_AutoClave'] + df_test['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "##Area Under Pressure Time Curve\n",
    "pressure_values = df_train[['1st Pressure Collect Result_AutoClave','2nd Pressure Collect Result_AutoClave','3rd Pressure Collect Result_AutoClave']]\n",
    "df_train['Area_under_pressure_time_AutoClave'] = np.trapz(pressure_values, axis = 1)\n",
    "pressure_values = df_test[['1st Pressure Collect Result_AutoClave','2nd Pressure Collect Result_AutoClave','3rd Pressure Collect Result_AutoClave']]\n",
    "df_test['Area_under_pressure_time_AutoClave'] = np.trapz(pressure_values, axis = 1)\n",
    "\n",
    "##Diff Time from Chamber Temp to Total Pressure\n",
    "df_train['Time_AutoClave_else'] = df_train['Chamber Temp. Unit Time_AutoClave'] - df_train['Total_pressure_time_AutoClave']\n",
    "df_test['Time_AutoClave_else'] = df_test['Chamber Temp. Unit Time_AutoClave'] - df_test['Total_pressure_time_AutoClave']\n",
    "\n",
    "##Multiply Pressure and Time\n",
    "df_train['1st Pressure_multi_time'] = df_train['1st Pressure Collect Result_AutoClave']  *  df_train['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_train['2nd Pressure_multi_time'] = df_train['2nd Pressure Collect Result_AutoClave'] * df_train['2nd Pressure Unit Time_AutoClave']\n",
    "df_train['3rd Pressure_multi_time'] = df_train['3rd Pressure Collect Result_AutoClave'] * df_train['3rd Pressure Unit Time_AutoClave']\n",
    "df_train['Total_Pressure_bytime'] = df_train['1st Pressure_multi_time'] + df_train['2nd Pressure_multi_time'] + df_train['3rd Pressure_multi_time']\n",
    "df_test['1st Pressure_multi_time'] = df_test['1st Pressure Collect Result_AutoClave'] * df_test['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_test['2nd Pressure_multi_time'] = df_test['2nd Pressure Collect Result_AutoClave'] * df_test['2nd Pressure Unit Time_AutoClave']\n",
    "df_test['3rd Pressure_multi_time'] = df_test['3rd Pressure Collect Result_AutoClave'] * df_test['3rd Pressure Unit Time_AutoClave']\n",
    "df_test['Total_Pressure_bytime'] = df_test['1st Pressure_multi_time'] + df_test['2nd Pressure_multi_time'] + df_test['3rd Pressure_multi_time']\n",
    "\n",
    "##PV = NRT\n",
    "df_train['Auto_vol_1_AutoClave'] = (df_train['Chamber Temp. Collect Result_AutoClave'] + 273) / df_train['1st Pressure Collect Result_AutoClave']\n",
    "df_train['Auto_vol_2_AutoClave'] = (df_train['Chamber Temp. Collect Result_AutoClave'] + 273) / df_train['2nd Pressure Collect Result_AutoClave']\n",
    "df_train['Auto_vol_3_AutoClave'] = (df_train['Chamber Temp. Collect Result_AutoClave'] + 273) / df_train['3rd Pressure Collect Result_AutoClave']\n",
    "df_test['Auto_vol_1_AutoClave'] = (df_test['Chamber Temp. Collect Result_AutoClave'] + 273) / df_test['1st Pressure Collect Result_AutoClave']\n",
    "df_test['Auto_vol_2_AutoClave'] = (df_test['Chamber Temp. Collect Result_AutoClave'] + 273) / df_test['2nd Pressure Collect Result_AutoClave']\n",
    "df_test['Auto_vol_3_AutoClave'] = (df_test['Chamber Temp. Collect Result_AutoClave'] + 273) / df_test['3rd Pressure Collect Result_AutoClave']\n",
    "\n",
    "##Absolute Chamber Temp\n",
    "df_train['K_TEMP_AutoClave'] = df_train['Chamber Temp. Collect Result_AutoClave'] + 273\n",
    "df_test['K_TEMP_AutoClave'] = df_test['Chamber Temp. Collect Result_AutoClave'] + 273\n",
    "\n",
    "##Arrhenius\n",
    "df_train['Arrhenius_AutoClave'] = np.exp(1 / (df_train['Chamber Temp. Collect Result_AutoClave'] + 273) - 1 / 373.15)\n",
    "df_test['Arrhenius_AutoClave'] = np.exp(1 / (df_test['Chamber Temp. Collect Result_AutoClave'] + 273) - 1 / 373.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c35a54",
   "metadata": {},
   "source": [
    "### Fill1 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c6a92",
   "metadata": {},
   "source": [
    "#### Signal combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c0633dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCHARGED RESIN(Fill1)\n",
    "##Resin 토출기 이동거리\n",
    "df_train['Discharged_Resin_distance_1_Fill1'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "df_train['Discharged_Resin_distance_2_Fill1'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "df_train['Discharged_Resin_distance_3_Fill1'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "df_test['Discharged_Resin_distance_1_Fill1'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "df_test['Discharged_Resin_distance_2_Fill1'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "df_test['Discharged_Resin_distance_3_Fill1'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "\n",
    "##Resin 토출 총 소요 시간\n",
    "df_train['TotalTime_Discharged_Resin_Fill1'] = df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1'] + df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1'] + df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "df_test['TotalTime_Discharged_Resin_Fill1'] = df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1'] + df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1'] + df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "28879709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPENSE(Fill1)\n",
    "##Dispensor 크기 변화\n",
    "df_train['Dispense_volume_change1_Fill1'] = df_train['Dispense Volume(Stage2) Collect Result_Fill1'] - df_train['Dispense Volume(Stage1) Collect Result_Fill1']\n",
    "df_train['Dispense_volume_change2_Fill1'] = df_train['Dispense Volume(Stage3) Collect Result_Fill1'] - df_train['Dispense Volume(Stage2) Collect Result_Fill1']\n",
    "df_test['Dispense_volume_change1_Fill1'] = df_test['Dispense Volume(Stage2) Collect Result_Fill1'] - df_test['Dispense Volume(Stage1) Collect Result_Fill1']\n",
    "df_test['Dispense_volume_change2_Fill1'] = df_test['Dispense Volume(Stage3) Collect Result_Fill1'] - df_test['Dispense Volume(Stage2) Collect Result_Fill1']\n",
    "\n",
    "##Dispensor 크기 변화량\n",
    "df_train['Dispense_volume_change1_abs_Fill1'] = df_train['Dispense_volume_change1_Fill1'].abs()\n",
    "df_train['Dispense_volume_change2_abs_Fill1'] = df_train['Dispense_volume_change2_Fill1'].abs()\n",
    "df_test['Dispense_volume_change1_abs_Fill1'] = df_test['Dispense_volume_change1_Fill1'].abs()\n",
    "df_test['Dispense_volume_change2_abs_Fill1'] = df_test['Dispense_volume_change2_Fill1'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2afbdc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCHARGED RESIN & DISPENSE (Fill1)\n",
    "##도포된 Resin 양\n",
    "df_train['Total_Resin_volume1_Fill1'] = df_train['Discharged_Resin_distance_1_Fill1'] * df_train['Dispense Volume(Stage1) Collect Result_Fill1']\n",
    "df_train['Total_Resin_volume2_Fill1'] = df_train['Discharged_Resin_distance_2_Fill1'] * df_train['Dispense Volume(Stage2) Collect Result_Fill1']\n",
    "df_train['Total_Resin_volume3_Fill1'] = df_train['Discharged_Resin_distance_3_Fill1'] * df_train['Dispense Volume(Stage3) Collect Result_Fill1']\n",
    "df_test['Total_Resin_volume1_Fill1'] = df_test['Discharged_Resin_distance_1_Fill1'] * df_test['Dispense Volume(Stage1) Collect Result_Fill1']\n",
    "df_test['Total_Resin_volume2_Fill1'] = df_test['Discharged_Resin_distance_2_Fill1'] * df_test['Dispense Volume(Stage2) Collect Result_Fill1']\n",
    "df_test['Total_Resin_volume3_Fill1'] = df_test['Discharged_Resin_distance_3_Fill1'] * df_test['Dispense Volume(Stage3) Collect Result_Fill1']\n",
    "\n",
    "# 시간당 토출량\n",
    "df_train['Stage1_Fill1_Volume_Speed'] = df_train['Dispense Volume(Stage1) Collect Result_Fill1'] / df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "df_train['Stage2_Fill1_Volume_Speed'] = df_train['Dispense Volume(Stage2) Collect Result_Fill1'] / df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "df_train['Stage3_Fill1_Volume_Speed'] = df_train['Dispense Volume(Stage3) Collect Result_Fill1'] / df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "df_test['Stage1_Fill1_Volume_Speed'] = df_test['Dispense Volume(Stage1) Collect Result_Fill1'] / df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "df_test['Stage2_Fill1_Volume_Speed'] = df_test['Dispense Volume(Stage2) Collect Result_Fill1'] / df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "df_test['Stage3_Fill1_Volume_Speed'] = df_test['Dispense Volume(Stage3) Collect Result_Fill1'] / df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1ce632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEAD(Fill1)\n",
    "##Fill1 X,Y,Z\n",
    "df_train['Fill1_X_1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1']\n",
    "df_train['Fill1_X_2'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1']\n",
    "df_train['Fill1_X_3'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1']\n",
    "df_test['Fill1_X_1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1']\n",
    "df_test['Fill1_X_2'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1']\n",
    "df_test['Fill1_X_3'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_train['Fill1_Y_1'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "df_train['Fill1_Y_2'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "df_train['Fill1_Y_3'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "df_test['Fill1_Y_1'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "df_test['Fill1_Y_2'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "df_test['Fill1_Y_3'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_train['Fill1_dist_1'] = np.sqrt(df_train['Fill1_X_1']**2 +df_train['Fill1_Y_1']**2)\n",
    "df_train['Fill1_dist_2'] = np.sqrt(df_train['Fill1_X_2']**2 +df_train['Fill1_Y_2']**2)\n",
    "df_train['Fill1_dist_3'] = np.sqrt(df_train['Fill1_X_3']**2 +df_train['Fill1_Y_3']**2)\n",
    "df_test['Fill1_dist_1'] = np.sqrt(df_test['Fill1_X_1']**2 +df_test['Fill1_Y_1']**2)\n",
    "df_test['Fill1_dist_2'] = np.sqrt(df_test['Fill1_X_2']**2 +df_test['Fill1_Y_2']**2)\n",
    "df_test['Fill1_dist_3'] = np.sqrt(df_test['Fill1_X_3']**2 +df_test['Fill1_Y_3']**2)\n",
    "\n",
    "df_train['Vector_Fill1_Stage1']= calculate_magnitude(df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'])\n",
    "df_train['Vector_Fill1_Stage2']= calculate_magnitude(df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'])\n",
    "df_train['Vector_Fill1_Stage3']= calculate_magnitude(df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'])\n",
    "\n",
    "df_train['Angle_x_Fill1_Stage1'],df_train['Angle_y_Fill1_Stage1'],df_train['Angle_z_Fill1_Stage1'] = calculate_angles(df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'])\n",
    "df_train['Angle_x_Fill1_Stage2'] ,df_train['Angle_y_Fill1_Stage2'],df_train['Angle_z_Fill1_Stage2'] = calculate_angles(df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'])\n",
    "df_train['Angle_x_Fill1_Stage3'] ,df_train['Angle_y_Fill1_Stage3'],df_train['Angle_z_Fill1_Stage3'] = calculate_angles(df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'])\n",
    "df_test['Vector_Fill1_Stage1']= calculate_magnitude(df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'])\n",
    "df_test['Vector_Fill1_Stage2']= calculate_magnitude(df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'])\n",
    "df_test['Vector_Fill1_Stage3']= calculate_magnitude(df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'])\n",
    "\n",
    "df_test['Angle_x_Fill1_Stage1'],df_test['Angle_y_Fill1_Stage1'],df_test['Angle_z_Fill1_Stage1'] = calculate_angles(df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'])\n",
    "df_test['Angle_x_Fill1_Stage2'] ,df_test['Angle_y_Fill1_Stage2'],df_test['Angle_z_Fill1_Stage2'] = calculate_angles(df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'])\n",
    "df_test['Angle_x_Fill1_Stage3'] ,df_test['Angle_y_Fill1_Stage3'],df_test['Angle_z_Fill1_Stage3'] = calculate_angles(df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "70e11059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tact Time & Discharged Resin\n",
    "##Diff from Tact Time to Discharged Time\n",
    "df_train['Abs_Tact_discharged_time_Fill1'] = df_train['Machine Tact time Collect Result_Fill1'] - df_train['TotalTime_Discharged_Resin_Fill1']\n",
    "df_test['Abs_Tact_discharged_time_Fill1'] = df_test['Machine Tact time Collect Result_Fill1'] - df_test['TotalTime_Discharged_Resin_Fill1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6372a",
   "metadata": {},
   "source": [
    "### Fill2 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8560111",
   "metadata": {},
   "source": [
    "#### Signal combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "37806b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CURE(Fill2)\n",
    "##CURE 가동 방향 및 가동높이\n",
    "df_train['Cure_HD_Fill2'] = df_train['CURE END POSITION X Collect Result_Fill2'] - df_train['CURE START POSITION X Collect Result_Fill2']\n",
    "df_test['Cure_HD_Fill2'] = df_test['CURE END POSITION X Collect Result_Fill2'] - df_test['CURE START POSITION X Collect Result_Fill2']\n",
    "\n",
    "##CURE 가동 시간\n",
    "df_train['Cure_time_Fill2'] = df_train['Cure_HD_Fill2'] / df_train['CURE SPEED Collect Result_Fill2']\n",
    "df_test['Cure_time_Fill2'] = df_test['Cure_HD_Fill2'] / df_test['CURE SPEED Collect Result_Fill2']\n",
    "\n",
    "##CURE position category\n",
    "df_train['Cure_position_category_Fill2'] = df_train['CURE END POSITION X Collect Result_Fill2'].map({240:'Down',1020:'Up'}).astype(str)\n",
    "df_test['Cure_position_category_Fill2'] = df_test['CURE END POSITION X Collect Result_Fill2'].map({240:'Down',1020:'Up'}).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c047b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEAD(Fill2)\n",
    "# Fill2 X,Y,Z\n",
    "df_train['Fill2_X_1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2']\n",
    "df_train['Fill2_X_2'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2']\n",
    "df_train['Fill2_X_3'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2']\n",
    "df_test['Fill2_X_1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2']\n",
    "df_test['Fill2_X_2'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2']\n",
    "df_test['Fill2_X_3'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2']\n",
    "\n",
    "df_train['Fill2_Y_1'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2']\n",
    "df_train['Fill2_Y_2'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2']\n",
    "df_train['Fill2_Y_3'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2']\n",
    "df_test['Fill2_Y_1'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2']\n",
    "df_test['Fill2_Y_2'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2']\n",
    "df_test['Fill2_Y_3'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2']\n",
    "\n",
    "df_train['Fill2_dist_1'] = np.sqrt(df_train['Fill2_X_1']**2 +df_train['Fill2_Y_1']**2)\n",
    "df_train['Fill2_dist_2'] = np.sqrt(df_train['Fill2_X_2']**2 +df_train['Fill2_Y_2']**2)\n",
    "df_train['Fill2_dist_3'] = np.sqrt(df_train['Fill2_X_3']**2 +df_train['Fill2_Y_3']**2)\n",
    "df_test['Fill2_dist_1'] = np.sqrt(df_test['Fill2_X_1']**2 +df_test['Fill2_Y_1']**2)\n",
    "df_test['Fill2_dist_2'] = np.sqrt(df_test['Fill2_X_2']**2 +df_test['Fill2_Y_2']**2)\n",
    "df_test['Fill2_dist_3'] = np.sqrt(df_test['Fill2_X_3']**2 +df_test['Fill2_Y_3']**2)\n",
    "\n",
    "df_train['Vector_Fill2_Stage1']= calculate_magnitude(df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2'])\n",
    "df_train['Vector_Fill2_Stage2']= calculate_magnitude(df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2'])\n",
    "df_train['Vector_Fill2_Stage3']= calculate_magnitude(df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2'])\n",
    "df_test['Vector_Fill2_Stage1']= calculate_magnitude(df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2'])\n",
    "df_test['Vector_Fill2_Stage2']= calculate_magnitude(df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2'])\n",
    "df_test['Vector_Fill2_Stage3']= calculate_magnitude(df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2'])\n",
    "\n",
    "df_train['Angle_x_Fill2_Stage1'],df_train['Angle_y_Fill2_Stage1'],df_train['Angle_z_Fill2_Stage1'] = calculate_angles(df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2'])\n",
    "df_train['Angle_x_Fill2_Stage2'] ,df_train['Angle_y_Fill2_Stage2'],df_train['Angle_z_Fill2_Stage2'] = calculate_angles(df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2'])\n",
    "df_train['Angle_x_Fill2_Stage3'] ,df_train['Angle_y_Fill2_Stage3'],df_train['Angle_z_Fill2_Stage3'] = calculate_angles(df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2'],\n",
    "                                                   df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2'],\n",
    "                                                    df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2'])\n",
    "df_test['Angle_x_Fill2_Stage1'],df_test['Angle_y_Fill2_Stage1'],df_test['Angle_z_Fill2_Stage1'] = calculate_angles(df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2'])\n",
    "df_test['Angle_x_Fill2_Stage2'] ,df_test['Angle_y_Fill2_Stage2'],df_test['Angle_z_Fill2_Stage2'] = calculate_angles(df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2'])\n",
    "df_test['Angle_x_Fill2_Stage3'] ,df_test['Angle_y_Fill2_Stage3'],df_test['Angle_z_Fill2_Stage3'] = calculate_angles(df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2'],\n",
    "                                                   df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2'],\n",
    "                                                    df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb6a41",
   "metadata": {},
   "source": [
    "### Total Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3581f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum Time\n",
    "##Tact Time \n",
    "df_train['Machine Tact time Collect Result_Total'] = df_train['Machine Tact time Collect Result_Dam'] + df_train['Machine Tact time Collect Result_Fill1'] + df_train['Machine Tact time Collect Result_Fill2']\n",
    "df_test['Machine Tact time Collect Result_Total'] = df_test['Machine Tact time Collect Result_Dam'] + df_test['Machine Tact time Collect Result_Fill1'] + df_test['Machine Tact time Collect Result_Fill2']\n",
    "\n",
    "##Sum Tact and Chamber \n",
    "df_train['Total_time'] = df_train['Chamber Temp. Unit Time_AutoClave'] + df_train['Machine Tact time Collect Result_Dam'] + df_train['Machine Tact time Collect Result_Fill1'] + df_train['Machine Tact time Collect Result_Fill2']\n",
    "df_test['Total_time'] = df_test['Chamber Temp. Unit Time_AutoClave'] + df_test['Machine Tact time Collect Result_Dam'] + df_test['Machine Tact time Collect Result_Fill1'] + df_test['Machine Tact time Collect Result_Fill2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "89e4fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Q_1'] = df_train['Dispense Volume(Stage1) Collect Result_Dam'] / df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "df_train['Q_2'] = df_train['Dispense Volume(Stage2) Collect Result_Dam'] / df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "df_train['Q_3'] = df_train['Dispense Volume(Stage3) Collect Result_Dam'] / df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_train['Q_4'] = df_train['Dispense Volume(Stage1) Collect Result_Fill1'] / df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "df_train['Q_5'] = df_train['Dispense Volume(Stage2) Collect Result_Fill1'] / df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "df_train['Q_6'] = df_train['Dispense Volume(Stage3) Collect Result_Fill1'] / df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_test['Q_1'] = df_test['Dispense Volume(Stage1) Collect Result_Dam'] / df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "df_test['Q_2'] = df_test['Dispense Volume(Stage2) Collect Result_Dam'] / df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "df_test['Q_3'] = df_test['Dispense Volume(Stage3) Collect Result_Dam'] / df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_test['Q_4'] = df_test['Dispense Volume(Stage1) Collect Result_Fill1'] / df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "df_test['Q_5'] = df_test['Dispense Volume(Stage2) Collect Result_Fill1'] / df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "df_test['Q_6'] = df_test['Dispense Volume(Stage3) Collect Result_Fill1'] / df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "289c2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Pressure/Time_Stage1'] = df_train['1st Pressure Collect Result_AutoClave'] / df_train['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_train['Pressure/Time_Stage2'] = df_train['2nd Pressure Collect Result_AutoClave'] / df_train['2nd Pressure Unit Time_AutoClave']\n",
    "df_train['Pressure/Time_Stage3'] = df_train['3rd Pressure Collect Result_AutoClave'] / df_train['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "df_train['np_1'] = df_train['Q_1'] * df_train['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_train['np_2'] = df_train['Q_2'] * df_train['2nd Pressure Unit Time_AutoClave']\n",
    "df_train['np_3'] = df_train['Q_3'] * df_train['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "df_train['두께_유량_1'] = df_train['THICKNESS 1 Collect Result_Dam'] / df_train['Q_1']\n",
    "df_train['두께_유량_2'] = df_train['THICKNESS 2 Collect Result_Dam'] / df_train['Q_2']\n",
    "df_train['두께_유량_3'] = df_train['THICKNESS 3 Collect Result_Dam'] / df_train['Q_3']\n",
    "\n",
    "df_train['부피_유량_1'] = df_train['Dispense Volume(Stage1) Collect Result_Dam'] / df_train['THICKNESS 1 Collect Result_Dam']\n",
    "df_train['부피_유량_2'] = df_train['Dispense Volume(Stage2) Collect Result_Dam'] / df_train['THICKNESS 2 Collect Result_Dam']\n",
    "df_train['부피_유량_3'] = df_train['Dispense Volume(Stage3) Collect Result_Dam'] / df_train['THICKNESS 3 Collect Result_Dam']\n",
    "\n",
    "df_test['Pressure/Time_Stage1'] = df_test['1st Pressure Collect Result_AutoClave'] / df_test['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_test['Pressure/Time_Stage2'] = df_test['2nd Pressure Collect Result_AutoClave'] / df_test['2nd Pressure Unit Time_AutoClave']\n",
    "df_test['Pressure/Time_Stage3'] = df_test['3rd Pressure Collect Result_AutoClave'] / df_test['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "df_test['np_1'] = df_test['Q_1'] * df_test['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_test['np_2'] = df_test['Q_2'] * df_test['2nd Pressure Unit Time_AutoClave']\n",
    "df_test['np_3'] = df_test['Q_3'] * df_test['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "df_test['두께_유량_1'] = df_test['THICKNESS 1 Collect Result_Dam'] / df_test['Q_1']\n",
    "df_test['두께_유량_2'] = df_test['THICKNESS 2 Collect Result_Dam'] / df_test['Q_2']\n",
    "df_test['두께_유량_3'] = df_test['THICKNESS 3 Collect Result_Dam'] / df_test['Q_3']\n",
    "\n",
    "df_test['부피_유량_1'] = df_test['Dispense Volume(Stage1) Collect Result_Dam'] / df_test['THICKNESS 1 Collect Result_Dam']\n",
    "df_test['부피_유량_2'] = df_test['Dispense Volume(Stage2) Collect Result_Dam'] / df_test['THICKNESS 2 Collect Result_Dam']\n",
    "df_test['부피_유량_3'] = df_test['Dispense Volume(Stage3) Collect Result_Dam'] / df_test['THICKNESS 3 Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f270d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['volume_sum_stage_1']=df_train['Dispense Volume(Stage1) Collect Result_Dam'] + df_train['Dispense Volume(Stage1) Collect Result_Fill1']\n",
    "df_train['volume_sum_stage_2']=df_train['Dispense Volume(Stage2) Collect Result_Dam'] + df_train['Dispense Volume(Stage2) Collect Result_Fill1']\n",
    "df_train['volume_sum_stage_3']=df_train['Dispense Volume(Stage3) Collect Result_Dam'] + df_train['Dispense Volume(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_train['pv_1'] = df_train['volume_sum_stage_1'] * df_train['1st Pressure Collect Result_AutoClave']\n",
    "df_train['pv_2'] = df_train['volume_sum_stage_2'] * df_train['2nd Pressure Collect Result_AutoClave']\n",
    "df_train['pv_3'] = df_train['volume_sum_stage_3'] * df_train['3rd Pressure Collect Result_AutoClave']\n",
    "\n",
    "df_train['pvt_1'] = df_train['volume_sum_stage_1'] * df_train['1st Pressure Collect Result_AutoClave'] * df_train['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_train['pvt_2'] = df_train['volume_sum_stage_2'] * df_train['2nd Pressure Collect Result_AutoClave'] * df_train['2nd Pressure Unit Time_AutoClave']\n",
    "df_train['pvt_3'] = df_train['volume_sum_stage_3'] * df_train['3rd Pressure Collect Result_AutoClave'] * df_train['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "df_train['speed_1'] = df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] / df_train['Stage1 Circle1 Distance Speed Collect Result_Dam']\n",
    "df_train['speed_2'] = df_train['Stage1 Line2 Distance Speed Collect Result_Dam'] / df_train['Stage1 Circle2 Distance Speed Collect Result_Dam']\n",
    "df_train['speed_3'] = df_train['Stage1 Line3 Distance Speed Collect Result_Dam'] / df_train['Stage1 Circle3 Distance Speed Collect Result_Dam']\n",
    "df_train['speed_4'] = df_train['Stage1 Line4 Distance Speed Collect Result_Dam'] / df_train['Stage1 Circle4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_train['speed_5'] = df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] / df_train['Stage2 Circle1 Distance Speed Collect Result_Dam']\n",
    "df_train['speed_6'] = df_train['Stage2 Line2 Distance Speed Collect Result_Dam'] / df_train['Stage2 Circle2 Distance Speed Collect Result_Dam']\n",
    "df_train['speed_7'] = df_train['Stage2 Line3 Distance Speed Collect Result_Dam'] / df_train['Stage2 Circle3 Distance Speed Collect Result_Dam']\n",
    "df_train['speed_8'] = df_train['Stage2 Line4 Distance Speed Collect Result_Dam'] / df_train['Stage2 Circle4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_train['speed_9'] = df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] / df_train['Stage3 Circle1 Distance Speed Collect Result_Dam']\n",
    "df_train['speed_10'] = df_train['Stage3 Line2 Distance Speed Collect Result_Dam'] / df_train['Stage3 Circle2 Distance Speed Collect Result_Dam']\n",
    "df_train['speed_11'] = df_train['Stage3 Line3 Distance Speed Collect Result_Dam'] / df_train['Stage3 Circle3 Distance Speed Collect Result_Dam']\n",
    "df_train['speed_12'] = df_train['Stage3 Line4 Distance Speed Collect Result_Dam'] / df_train['Stage3 Circle4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_test['volume_sum_stage_1']=df_test['Dispense Volume(Stage1) Collect Result_Dam'] + df_test['Dispense Volume(Stage1) Collect Result_Fill1']\n",
    "df_test['volume_sum_stage_2']=df_test['Dispense Volume(Stage2) Collect Result_Dam'] + df_test['Dispense Volume(Stage2) Collect Result_Fill1']\n",
    "df_test['volume_sum_stage_3']=df_test['Dispense Volume(Stage3) Collect Result_Dam'] + df_test['Dispense Volume(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_test['pv_1'] = df_test['volume_sum_stage_1'] * df_test['1st Pressure Collect Result_AutoClave']\n",
    "df_test['pv_2'] = df_test['volume_sum_stage_2'] * df_test['2nd Pressure Collect Result_AutoClave']\n",
    "df_test['pv_3'] = df_test['volume_sum_stage_3'] * df_test['3rd Pressure Collect Result_AutoClave']\n",
    "\n",
    "df_test['pvt_1'] = df_test['volume_sum_stage_1'] * df_test['1st Pressure Collect Result_AutoClave'] * df_test['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_test['pvt_2'] = df_test['volume_sum_stage_2'] * df_test['2nd Pressure Collect Result_AutoClave'] * df_test['2nd Pressure Unit Time_AutoClave']\n",
    "df_test['pvt_3'] = df_test['volume_sum_stage_3'] * df_test['3rd Pressure Collect Result_AutoClave'] * df_test['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "df_test['speed_1'] = df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] / df_test['Stage1 Circle1 Distance Speed Collect Result_Dam']\n",
    "df_test['speed_2'] = df_test['Stage1 Line2 Distance Speed Collect Result_Dam'] / df_test['Stage1 Circle2 Distance Speed Collect Result_Dam']\n",
    "df_test['speed_3'] = df_test['Stage1 Line3 Distance Speed Collect Result_Dam'] / df_test['Stage1 Circle3 Distance Speed Collect Result_Dam']\n",
    "df_test['speed_4'] = df_test['Stage1 Line4 Distance Speed Collect Result_Dam'] / df_test['Stage1 Circle4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_test['speed_5'] = df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] / df_test['Stage2 Circle1 Distance Speed Collect Result_Dam']\n",
    "df_test['speed_6'] = df_test['Stage2 Line2 Distance Speed Collect Result_Dam'] / df_test['Stage2 Circle2 Distance Speed Collect Result_Dam']\n",
    "df_test['speed_7'] = df_test['Stage2 Line3 Distance Speed Collect Result_Dam'] / df_test['Stage2 Circle3 Distance Speed Collect Result_Dam']\n",
    "df_test['speed_8'] = df_test['Stage2 Line4 Distance Speed Collect Result_Dam'] / df_test['Stage2 Circle4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_test['speed_9'] = df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] / df_test['Stage3 Circle1 Distance Speed Collect Result_Dam']\n",
    "df_test['speed_10'] = df_test['Stage3 Line2 Distance Speed Collect Result_Dam'] / df_test['Stage3 Circle2 Distance Speed Collect Result_Dam']\n",
    "df_test['speed_11'] = df_test['Stage3 Line3 Distance Speed Collect Result_Dam'] / df_test['Stage3 Circle3 Distance Speed Collect Result_Dam']\n",
    "df_test['speed_12'] = df_test['Stage3 Line4 Distance Speed Collect Result_Dam'] / df_test['Stage3 Circle4 Distance Speed Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "94b662bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['CURE_START_END_DISTANCE_Fill2_X'] = df_train['CURE START POSITION X Collect Result_Fill2'] - df_train['CURE END POSITION X Collect Result_Fill2']\n",
    "df_train['CURE_STANDBY_END_DISTANCE_Fill2_Z'] = df_train['CURE STANDBY POSITION Z Collect Result_Fill2'] - df_train['CURE END POSITION Z Collect Result_Fill2']\n",
    "df_train['CURE_TIME_X_Fill2'] = df_train['CURE_START_END_DISTANCE_Fill2_X'] / df_train['CURE SPEED Collect Result_Fill2']\n",
    "df_train['CURE_TIME_Z_Fill2'] = df_train['CURE_STANDBY_END_DISTANCE_Fill2_Z'] / df_train['CURE SPEED Collect Result_Fill2']\n",
    "df_train['CURE_START_END_DISTANCE_Dam_X'] = df_train['CURE START POSITION X Collect Result_Dam'] - df_train['CURE END POSITION X Collect Result_Dam']\n",
    "df_train['CURE_TIME_X_Dam'] = df_train['CURE_START_END_DISTANCE_Dam_X'] / df_train['CURE SPEED Collect Result_Dam']\n",
    "\n",
    "df_test['CURE_START_END_DISTANCE_Fill2_X'] = df_test['CURE START POSITION X Collect Result_Fill2'] - df_test['CURE END POSITION X Collect Result_Fill2']\n",
    "df_test['CURE_STANDBY_END_DISTANCE_Fill2_Z'] = df_test['CURE STANDBY POSITION Z Collect Result_Fill2'] - df_test['CURE END POSITION Z Collect Result_Fill2']\n",
    "df_test['CURE_TIME_X_Fill2'] = df_test['CURE_START_END_DISTANCE_Fill2_X'] / df_test['CURE SPEED Collect Result_Fill2']\n",
    "df_test['CURE_TIME_Z_Fill2'] = df_test['CURE_STANDBY_END_DISTANCE_Fill2_Z'] / df_test['CURE SPEED Collect Result_Fill2']\n",
    "df_test['CURE_START_END_DISTANCE_Dam_X'] = df_test['CURE START POSITION X Collect Result_Dam'] - df_test['CURE END POSITION X Collect Result_Dam']\n",
    "df_test['CURE_TIME_X_Dam'] = df_test['CURE_START_END_DISTANCE_Dam_X'] / df_test['CURE SPEED Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "70f47733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tt = df_train.copy()\n",
    "df_test_tt = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a3f71fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Features\n",
    "df_train['Collect Date_Dam'] = pd.to_datetime(df_train['Collect Date_Dam'])\n",
    "df_train['Collect Date_Fill1'] = pd.to_datetime(df_train['Collect Date_Fill1'])\n",
    "df_train['Collect Date_Fill2'] = pd.to_datetime(df_train['Collect Date_Fill2'])\n",
    "df_train['Collect Date_AutoClave'] = pd.to_datetime(df_train['Collect Date_AutoClave'])\n",
    "\n",
    "##Time diff\n",
    "df_train['Dam_Fill1_Time_Diff'] = (df_train['Collect Date_Fill1'] - df_train['Collect Date_Dam']).dt.total_seconds()\n",
    "df_train['Fill1_Fill2_Time_Diff'] = (df_train['Collect Date_Fill2'] - df_train['Collect Date_Fill1']).dt.total_seconds()\n",
    "df_train['Fill2_Auto_Time_Diff'] = (df_train['Collect Date_AutoClave'] - df_train['Collect Date_Fill2']).dt.total_seconds()\n",
    "df_train['Total_Time_Diff'] = (df_train['Collect Date_AutoClave'] - df_train['Collect Date_Dam']).dt.total_seconds()\n",
    "\n",
    "##Fill1&Fill2 Match\n",
    "df_train['Fill1_Fill2_match'] = (df_train['Collect Date_Fill1'] == df_train['Collect Date_Fill2']).astype(int)\n",
    "\n",
    "##Time Transform\n",
    "df_train['year_Dam'] = df_train['Collect Date_Dam'].dt.year\n",
    "df_train['month_Dam'] = df_train['Collect Date_Dam'].dt.month\n",
    "df_train['day_Dam'] = df_train['Collect Date_Dam'].dt.day\n",
    "df_train['weekday_Dam'] = df_train['Collect Date_Dam'].dt.weekday\n",
    "df_train['hour_Dam'] = df_train['Collect Date_Dam'].dt.hour\n",
    "df_train['minute_Dam'] = df_train['Collect Date_Dam'].dt.minute\n",
    "df_train['second_Dam'] = df_train['Collect Date_Dam'].dt.second\n",
    "\n",
    "df_train['year_Fill1'] = df_train['Collect Date_Fill1'].dt.year\n",
    "df_train['month_Fill1'] = df_train['Collect Date_Fill1'].dt.month\n",
    "df_train['day_Fill1'] = df_train['Collect Date_Fill1'].dt.day\n",
    "df_train['weekday_Fill1'] = df_train['Collect Date_Fill1'].dt.weekday\n",
    "df_train['hour_Fill1'] = df_train['Collect Date_Fill1'].dt.hour\n",
    "df_train['minute_Fill1'] = df_train['Collect Date_Fill1'].dt.minute\n",
    "df_train['second_Fill1'] = df_train['Collect Date_Fill1'].dt.second\n",
    "\n",
    "df_train['year_Fill2'] = df_train['Collect Date_Fill2'].dt.year\n",
    "df_train['month_Fill2'] = df_train['Collect Date_Fill2'].dt.month\n",
    "df_train['day_Fill2'] = df_train['Collect Date_Fill2'].dt.day\n",
    "df_train['weekday_Fill2'] = df_train['Collect Date_Fill2'].dt.weekday\n",
    "df_train['hour_Fill2'] = df_train['Collect Date_Fill2'].dt.hour\n",
    "df_train['minute_Fill2'] = df_train['Collect Date_Fill2'].dt.minute\n",
    "df_train['second_Fill2'] = df_train['Collect Date_Fill2'].dt.second\n",
    "\n",
    "df_train['sin_hour_Dam'] = np.sin(2 * np.pi * df_train['hour_Dam']/23.0)\n",
    "df_train['cos_hour_Dam'] = np.cos(2 * np.pi * df_train['hour_Dam']/23.0)\n",
    "df_train['sin_minute_Dam'] = np.sin(2 * np.pi * df_train['minute_Dam']/59.0)\n",
    "df_train['cos_minute_Dam'] = np.cos(2 * np.pi * df_train['minute_Dam']/59.0)\n",
    "df_train['sin_date_Dam'] = -np.sin(2 * np.pi * (df_train['month_Dam']+df_train['day_Dam']/31)/12)\n",
    "df_train['cos_date_Dam'] = -np.sin(2 * np.pi * (df_train['month_Dam']+df_train['day_Dam']/31)/12)\n",
    "df_train['sin_month_Dam'] = -np.sin(2 * np.pi * df_train['month_Dam']/12.0)\n",
    "df_train['cos_month_Dam'] = -np.cos(2 * np.pi * df_train['month_Dam']/12.0)\n",
    "\n",
    "##Cycling Transform\n",
    "df_train['sin_hour_Fill1'] = np.sin(2 * np.pi * df_train['hour_Fill1']/23.0)\n",
    "df_train['cos_hour_Fill1'] = np.cos(2 * np.pi * df_train['hour_Fill1']/23.0)\n",
    "df_train['sin_minute_Fill1'] = np.sin(2 * np.pi * df_train['minute_Fill1']/59.0)\n",
    "df_train['cos_minute_Fill1'] = np.cos(2 * np.pi * df_train['minute_Fill1']/59.0)\n",
    "df_train['sin_date_Fill1'] = -np.sin(2 * np.pi * (df_train['month_Fill1']+df_train['day_Fill1']/31)/12)\n",
    "df_train['cos_date_Fill1'] = -np.sin(2 * np.pi * (df_train['month_Fill1']+df_train['day_Fill1']/31)/12)\n",
    "df_train['sin_month_Fill1'] = -np.sin(2 * np.pi * df_train['month_Fill1']/12.0)\n",
    "df_train['cos_month_Fill1'] = -np.cos(2 * np.pi * df_train['month_Fill1']/12.0)\n",
    "\n",
    "df_train['sin_hour_Fill2'] = np.sin(2 * np.pi * df_train['hour_Fill2']/23.0)\n",
    "df_train['cos_hour_Fill2'] = np.cos(2 * np.pi * df_train['hour_Fill2']/23.0)\n",
    "df_train['sin_minute_Fill2'] = np.sin(2 * np.pi * df_train['minute_Fill2']/59.0)\n",
    "df_train['cos_minute_Fill2'] = np.cos(2 * np.pi * df_train['minute_Fill2']/59.0)\n",
    "df_train['sin_date_Fill2'] = -np.sin(2 * np.pi * (df_train['month_Fill2']+df_train['day_Fill2']/31)/12)\n",
    "df_train['cos_date_Fill2'] = -np.sin(2 * np.pi * (df_train['month_Fill2']+df_train['day_Fill2']/31)/12)\n",
    "df_train['sin_month_Fill2'] = -np.sin(2 * np.pi * df_train['month_Fill2']/12.0)\n",
    "df_train['cos_month_Fill2'] = -np.cos(2 * np.pi * df_train['month_Fill2']/12.0)\n",
    "\n",
    "#Time Features\n",
    "df_test['Collect Date_Dam'] = pd.to_datetime(df_test['Collect Date_Dam'])\n",
    "df_test['Collect Date_Fill1'] = pd.to_datetime(df_test['Collect Date_Fill1'])\n",
    "df_test['Collect Date_Fill2'] = pd.to_datetime(df_test['Collect Date_Fill2'])\n",
    "df_test['Collect Date_AutoClave'] = pd.to_datetime(df_test['Collect Date_AutoClave'])\n",
    "\n",
    "##Time diff\n",
    "df_test['Dam_Fill1_Time_Diff'] = (df_test['Collect Date_Fill1'] - df_test['Collect Date_Dam']).dt.total_seconds()\n",
    "df_test['Fill1_Fill2_Time_Diff'] = (df_test['Collect Date_Fill2'] - df_test['Collect Date_Fill1']).dt.total_seconds()\n",
    "df_test['Fill2_Auto_Time_Diff'] = (df_test['Collect Date_AutoClave'] - df_test['Collect Date_Fill2']).dt.total_seconds()\n",
    "df_test['Total_Time_Diff'] = (df_test['Collect Date_AutoClave'] - df_test['Collect Date_Dam']).dt.total_seconds()\n",
    "\n",
    "##Fill1&Fill2 Match\n",
    "df_test['Fill1_Fill2_match'] = (df_test['Collect Date_Fill1'] == df_test['Collect Date_Fill2']).astype(int)\n",
    "\n",
    "##Time Transform\n",
    "df_test['year_Dam'] = df_test['Collect Date_Dam'].dt.year\n",
    "df_test['month_Dam'] = df_test['Collect Date_Dam'].dt.month\n",
    "df_test['day_Dam'] = df_test['Collect Date_Dam'].dt.day\n",
    "df_test['weekday_Dam'] = df_test['Collect Date_Dam'].dt.weekday\n",
    "df_test['hour_Dam'] = df_test['Collect Date_Dam'].dt.hour\n",
    "df_test['minute_Dam'] = df_test['Collect Date_Dam'].dt.minute\n",
    "df_test['second_Dam'] = df_test['Collect Date_Dam'].dt.second\n",
    "\n",
    "df_test['year_Fill1'] = df_test['Collect Date_Fill1'].dt.year\n",
    "df_test['month_Fill1'] = df_test['Collect Date_Fill1'].dt.month\n",
    "df_test['day_Fill1'] = df_test['Collect Date_Fill1'].dt.day\n",
    "df_test['weekday_Fill1'] = df_test['Collect Date_Fill1'].dt.weekday\n",
    "df_test['hour_Fill1'] = df_test['Collect Date_Fill1'].dt.hour\n",
    "df_test['minute_Fill1'] = df_test['Collect Date_Fill1'].dt.minute\n",
    "df_test['second_Fill1'] = df_test['Collect Date_Fill1'].dt.second\n",
    "\n",
    "df_test['year_Fill2'] = df_test['Collect Date_Fill2'].dt.year\n",
    "df_test['month_Fill2'] = df_test['Collect Date_Fill2'].dt.month\n",
    "df_test['day_Fill2'] = df_test['Collect Date_Fill2'].dt.day\n",
    "df_test['weekday_Fill2'] = df_test['Collect Date_Fill2'].dt.weekday\n",
    "df_test['hour_Fill2'] = df_test['Collect Date_Fill2'].dt.hour\n",
    "df_test['minute_Fill2'] = df_test['Collect Date_Fill2'].dt.minute\n",
    "df_test['second_Fill2'] = df_test['Collect Date_Fill2'].dt.second\n",
    "\n",
    "df_test['sin_hour_Dam'] = np.sin(2 * np.pi * df_test['hour_Dam']/23.0)\n",
    "df_test['cos_hour_Dam'] = np.cos(2 * np.pi * df_test['hour_Dam']/23.0)\n",
    "df_test['sin_minute_Dam'] = np.sin(2 * np.pi * df_test['minute_Dam']/59.0)\n",
    "df_test['cos_minute_Dam'] = np.cos(2 * np.pi * df_test['minute_Dam']/59.0)\n",
    "df_test['sin_date_Dam'] = -np.sin(2 * np.pi * (df_test['month_Dam']+df_test['day_Dam']/31)/12)\n",
    "df_test['cos_date_Dam'] = -np.sin(2 * np.pi * (df_test['month_Dam']+df_test['day_Dam']/31)/12)\n",
    "df_test['sin_month_Dam'] = -np.sin(2 * np.pi * df_test['month_Dam']/12.0)\n",
    "df_test['cos_month_Dam'] = -np.cos(2 * np.pi * df_test['month_Dam']/12.0)\n",
    "\n",
    "##Cycling Transform\n",
    "df_test['sin_hour_Fill1'] = np.sin(2 * np.pi * df_test['hour_Fill1']/23.0)\n",
    "df_test['cos_hour_Fill1'] = np.cos(2 * np.pi * df_test['hour_Fill1']/23.0)\n",
    "df_test['sin_minute_Fill1'] = np.sin(2 * np.pi * df_test['minute_Fill1']/59.0)\n",
    "df_test['cos_minute_Fill1'] = np.cos(2 * np.pi * df_test['minute_Fill1']/59.0)\n",
    "df_test['sin_date_Fill1'] = -np.sin(2 * np.pi * (df_test['month_Fill1']+df_test['day_Fill1']/31)/12)\n",
    "df_test['cos_date_Fill1'] = -np.sin(2 * np.pi * (df_test['month_Fill1']+df_test['day_Fill1']/31)/12)\n",
    "df_test['sin_month_Fill1'] = -np.sin(2 * np.pi * df_test['month_Fill1']/12.0)\n",
    "df_test['cos_month_Fill1'] = -np.cos(2 * np.pi * df_test['month_Fill1']/12.0)\n",
    "\n",
    "df_test['sin_hour_Fill2'] = np.sin(2 * np.pi * df_test['hour_Fill2']/23.0)\n",
    "df_test['cos_hour_Fill2'] = np.cos(2 * np.pi * df_test['hour_Fill2']/23.0)\n",
    "df_test['sin_minute_Fill2'] = np.sin(2 * np.pi * df_test['minute_Fill2']/59.0)\n",
    "df_test['cos_minute_Fill2'] = np.cos(2 * np.pi * df_test['minute_Fill2']/59.0)\n",
    "df_test['sin_date_Fill2'] = -np.sin(2 * np.pi * (df_test['month_Fill2']+df_test['day_Fill2']/31)/12)\n",
    "df_test['cos_date_Fill2'] = -np.sin(2 * np.pi * (df_test['month_Fill2']+df_test['day_Fill2']/31)/12)\n",
    "df_test['sin_month_Fill2'] = -np.sin(2 * np.pi * df_test['month_Fill2']/12.0)\n",
    "df_test['cos_month_Fill2'] = -np.cos(2 * np.pi * df_test['month_Fill2']/12.0)\n",
    "\n",
    "df_train.drop(columns = ['Collect Date_Dam','Collect Date_Fill1','Collect Date_Fill2','Collect Date_AutoClave'], inplace=True)\n",
    "df_test.drop(columns = ['Collect Date_Dam','Collect Date_Fill1','Collect Date_Fill2','Collect Date_AutoClave'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27331e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "edd898d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전 컬럼 개수 :  355\n",
      "중복 제거 후 컬럼 개수 :  307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Equipment_PalletID</th>\n",
       "      <th>Model_Receip</th>\n",
       "      <th>Workorder</th>\n",
       "      <th>CURE END POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Z Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Θ Collect Result_Dam</th>\n",
       "      <th>CURE SPEED Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION X Collect Result_Dam</th>\n",
       "      <th>DISCHARGED SPEED OF RESIN Collect Result_Dam</th>\n",
       "      <th>DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam</th>\n",
       "      <th>...</th>\n",
       "      <th>sin_hour_Fill1</th>\n",
       "      <th>cos_hour_Fill1</th>\n",
       "      <th>sin_minute_Fill1</th>\n",
       "      <th>cos_minute_Fill1</th>\n",
       "      <th>sin_date_Fill1</th>\n",
       "      <th>sin_hour_Fill2</th>\n",
       "      <th>cos_hour_Fill2</th>\n",
       "      <th>sin_minute_Fill2</th>\n",
       "      <th>cos_minute_Fill2</th>\n",
       "      <th>sin_date_Fill2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dispenser_1_7</td>\n",
       "      <td>AJX75334505_1</td>\n",
       "      <td>4F1XA938-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>100</td>\n",
       "      <td>1030</td>\n",
       "      <td>16</td>\n",
       "      <td>14.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136167</td>\n",
       "      <td>-0.990686</td>\n",
       "      <td>0.847734</td>\n",
       "      <td>-0.530421</td>\n",
       "      <td>-0.585049</td>\n",
       "      <td>0.136167</td>\n",
       "      <td>-0.990686</td>\n",
       "      <td>0.847734</td>\n",
       "      <td>-0.530421</td>\n",
       "      <td>-0.585049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dispenser_1_7</td>\n",
       "      <td>AJX75334505_1</td>\n",
       "      <td>3KPM0016-2</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>10</td>\n",
       "      <td>21.3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.631088</td>\n",
       "      <td>-0.775711</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.998583</td>\n",
       "      <td>0.948947</td>\n",
       "      <td>-0.631088</td>\n",
       "      <td>-0.775711</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.998583</td>\n",
       "      <td>0.948947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dispneser_2_10</td>\n",
       "      <td>AJX75334501_1</td>\n",
       "      <td>4E1X9167-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>280</td>\n",
       "      <td>16</td>\n",
       "      <td>14.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631088</td>\n",
       "      <td>-0.775711</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.998583</td>\n",
       "      <td>-0.996436</td>\n",
       "      <td>0.631088</td>\n",
       "      <td>-0.775711</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.998583</td>\n",
       "      <td>-0.996436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dispneser_2_12</td>\n",
       "      <td>AJX75334501_1</td>\n",
       "      <td>3K1X0057-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>10</td>\n",
       "      <td>21.3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.816970</td>\n",
       "      <td>-0.576680</td>\n",
       "      <td>-0.899312</td>\n",
       "      <td>-0.437307</td>\n",
       "      <td>0.912166</td>\n",
       "      <td>-0.816970</td>\n",
       "      <td>-0.576680</td>\n",
       "      <td>-0.899312</td>\n",
       "      <td>-0.437307</td>\n",
       "      <td>0.912166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dispenser_1_8</td>\n",
       "      <td>AJX75334501_1</td>\n",
       "      <td>3HPM0007-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>10</td>\n",
       "      <td>9.7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.398401</td>\n",
       "      <td>-0.917211</td>\n",
       "      <td>0.847734</td>\n",
       "      <td>-0.530421</td>\n",
       "      <td>0.440394</td>\n",
       "      <td>-0.398401</td>\n",
       "      <td>-0.917211</td>\n",
       "      <td>0.847734</td>\n",
       "      <td>-0.530421</td>\n",
       "      <td>0.440394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Equipment_PalletID   Model_Receip   Workorder  \\\n",
       "0      Dispenser_1_7  AJX75334505_1  4F1XA938-1   \n",
       "1      Dispenser_1_7  AJX75334505_1  3KPM0016-2   \n",
       "2     Dispneser_2_10  AJX75334501_1  4E1X9167-1   \n",
       "3     Dispneser_2_12  AJX75334501_1  3K1X0057-1   \n",
       "4      Dispenser_1_8  AJX75334501_1  3HPM0007-1   \n",
       "\n",
       "   CURE END POSITION X Collect Result_Dam  \\\n",
       "0                                   240.0   \n",
       "1                                   240.0   \n",
       "2                                  1000.0   \n",
       "3                                  1000.0   \n",
       "4                                   240.0   \n",
       "\n",
       "   CURE END POSITION Z Collect Result_Dam  \\\n",
       "0                                     2.5   \n",
       "1                                     2.5   \n",
       "2                                    12.5   \n",
       "3                                    12.5   \n",
       "4                                     2.5   \n",
       "\n",
       "   CURE END POSITION Θ Collect Result_Dam  CURE SPEED Collect Result_Dam  \\\n",
       "0                                     -90                            100   \n",
       "1                                     -90                             70   \n",
       "2                                      90                             85   \n",
       "3                                      90                             70   \n",
       "4                                     -90                             70   \n",
       "\n",
       "   CURE START POSITION X Collect Result_Dam  \\\n",
       "0                                      1030   \n",
       "1                                      1030   \n",
       "2                                       280   \n",
       "3                                       280   \n",
       "4                                      1030   \n",
       "\n",
       "   DISCHARGED SPEED OF RESIN Collect Result_Dam  \\\n",
       "0                                            16   \n",
       "1                                            10   \n",
       "2                                            16   \n",
       "3                                            10   \n",
       "4                                            10   \n",
       "\n",
       "   DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam  ...  sin_hour_Fill1  \\\n",
       "0                                               14.9    ...        0.136167   \n",
       "1                                               21.3    ...       -0.631088   \n",
       "2                                               14.7    ...        0.631088   \n",
       "3                                               21.3    ...       -0.816970   \n",
       "4                                                9.7    ...       -0.398401   \n",
       "\n",
       "   cos_hour_Fill1  sin_minute_Fill1  cos_minute_Fill1  sin_date_Fill1  \\\n",
       "0       -0.990686          0.847734         -0.530421       -0.585049   \n",
       "1       -0.775711         -0.053222         -0.998583        0.948947   \n",
       "2       -0.775711         -0.053222         -0.998583       -0.996436   \n",
       "3       -0.576680         -0.899312         -0.437307        0.912166   \n",
       "4       -0.917211          0.847734         -0.530421        0.440394   \n",
       "\n",
       "   sin_hour_Fill2  cos_hour_Fill2  sin_minute_Fill2  cos_minute_Fill2  \\\n",
       "0        0.136167       -0.990686          0.847734         -0.530421   \n",
       "1       -0.631088       -0.775711         -0.053222         -0.998583   \n",
       "2        0.631088       -0.775711         -0.053222         -0.998583   \n",
       "3       -0.816970       -0.576680         -0.899312         -0.437307   \n",
       "4       -0.398401       -0.917211          0.847734         -0.530421   \n",
       "\n",
       "   sin_date_Fill2  \n",
       "0       -0.585049  \n",
       "1        0.948947  \n",
       "2       -0.996436  \n",
       "3        0.912166  \n",
       "4        0.440394  \n",
       "\n",
       "[5 rows x 307 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Equipment_PalletID</th>\n",
       "      <th>Model_Receip</th>\n",
       "      <th>Workorder</th>\n",
       "      <th>CURE END POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Z Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Θ Collect Result_Dam</th>\n",
       "      <th>CURE SPEED Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION X Collect Result_Dam</th>\n",
       "      <th>DISCHARGED SPEED OF RESIN Collect Result_Dam</th>\n",
       "      <th>DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam</th>\n",
       "      <th>...</th>\n",
       "      <th>sin_hour_Fill1</th>\n",
       "      <th>cos_hour_Fill1</th>\n",
       "      <th>sin_minute_Fill1</th>\n",
       "      <th>cos_minute_Fill1</th>\n",
       "      <th>sin_date_Fill1</th>\n",
       "      <th>sin_hour_Fill2</th>\n",
       "      <th>cos_hour_Fill2</th>\n",
       "      <th>sin_minute_Fill2</th>\n",
       "      <th>cos_minute_Fill2</th>\n",
       "      <th>sin_date_Fill2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dispneser_2_13</td>\n",
       "      <td>AJX75334501_1</td>\n",
       "      <td>3J1XF767-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>10</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.398401</td>\n",
       "      <td>-0.917211</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.998583</td>\n",
       "      <td>0.968077</td>\n",
       "      <td>-0.398401</td>\n",
       "      <td>-0.917211</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.998583</td>\n",
       "      <td>0.968077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dispneser_2_14</td>\n",
       "      <td>AJX75334501_1</td>\n",
       "      <td>4B1XD472-2</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>16</td>\n",
       "      <td>14.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.942261</td>\n",
       "      <td>-0.334880</td>\n",
       "      <td>-0.818303</td>\n",
       "      <td>0.574787</td>\n",
       "      <td>-0.912166</td>\n",
       "      <td>-0.942261</td>\n",
       "      <td>-0.334880</td>\n",
       "      <td>-0.818303</td>\n",
       "      <td>0.574787</td>\n",
       "      <td>-0.912166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dispenser_1_1</td>\n",
       "      <td>AJX75334501_1</td>\n",
       "      <td>3H1XE355-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>10</td>\n",
       "      <td>9.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136167</td>\n",
       "      <td>-0.990686</td>\n",
       "      <td>-0.899312</td>\n",
       "      <td>-0.437307</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>0.136167</td>\n",
       "      <td>-0.990686</td>\n",
       "      <td>-0.899312</td>\n",
       "      <td>-0.437307</td>\n",
       "      <td>0.688967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dispneser_2_14</td>\n",
       "      <td>AJX75334501_1</td>\n",
       "      <td>3L1XA128-1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>280</td>\n",
       "      <td>10</td>\n",
       "      <td>21.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816970</td>\n",
       "      <td>-0.576680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455495</td>\n",
       "      <td>0.816970</td>\n",
       "      <td>-0.576680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dispenser_1_1</td>\n",
       "      <td>AJX75334501_1</td>\n",
       "      <td>4A1XA639-1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-90</td>\n",
       "      <td>70</td>\n",
       "      <td>1030</td>\n",
       "      <td>16</td>\n",
       "      <td>13.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.631088</td>\n",
       "      <td>-0.775711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.378779</td>\n",
       "      <td>-0.631088</td>\n",
       "      <td>-0.775711</td>\n",
       "      <td>0.874763</td>\n",
       "      <td>0.484551</td>\n",
       "      <td>-0.378779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Equipment_PalletID   Model_Receip   Workorder  \\\n",
       "0     Dispneser_2_13  AJX75334501_1  3J1XF767-1   \n",
       "1     Dispneser_2_14  AJX75334501_1  4B1XD472-2   \n",
       "2      Dispenser_1_1  AJX75334501_1  3H1XE355-1   \n",
       "3     Dispneser_2_14  AJX75334501_1  3L1XA128-1   \n",
       "4      Dispenser_1_1  AJX75334501_1  4A1XA639-1   \n",
       "\n",
       "   CURE END POSITION X Collect Result_Dam  \\\n",
       "0                                  1000.0   \n",
       "1                                  1000.0   \n",
       "2                                   240.0   \n",
       "3                                  1000.0   \n",
       "4                                   240.0   \n",
       "\n",
       "   CURE END POSITION Z Collect Result_Dam  \\\n",
       "0                                    12.5   \n",
       "1                                    12.5   \n",
       "2                                     2.5   \n",
       "3                                    12.5   \n",
       "4                                     2.5   \n",
       "\n",
       "   CURE END POSITION Θ Collect Result_Dam  CURE SPEED Collect Result_Dam  \\\n",
       "0                                      90                             70   \n",
       "1                                      90                             70   \n",
       "2                                     -90                             70   \n",
       "3                                      90                             70   \n",
       "4                                     -90                             70   \n",
       "\n",
       "   CURE START POSITION X Collect Result_Dam  \\\n",
       "0                                       280   \n",
       "1                                       280   \n",
       "2                                      1030   \n",
       "3                                       280   \n",
       "4                                      1030   \n",
       "\n",
       "   DISCHARGED SPEED OF RESIN Collect Result_Dam  \\\n",
       "0                                            10   \n",
       "1                                            16   \n",
       "2                                            10   \n",
       "3                                            10   \n",
       "4                                            16   \n",
       "\n",
       "   DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam  ...  sin_hour_Fill1  \\\n",
       "0                                               17.0    ...       -0.398401   \n",
       "1                                               14.2    ...       -0.942261   \n",
       "2                                                9.7    ...        0.136167   \n",
       "3                                               21.3    ...        0.816970   \n",
       "4                                               13.2    ...       -0.631088   \n",
       "\n",
       "   cos_hour_Fill1  sin_minute_Fill1  cos_minute_Fill1  sin_date_Fill1  \\\n",
       "0       -0.917211         -0.053222         -0.998583        0.968077   \n",
       "1       -0.334880         -0.818303          0.574787       -0.912166   \n",
       "2       -0.990686         -0.899312         -0.437307        0.688967   \n",
       "3       -0.576680          0.000000          1.000000        0.455495   \n",
       "4       -0.775711          0.000000          1.000000       -0.378779   \n",
       "\n",
       "   sin_hour_Fill2  cos_hour_Fill2  sin_minute_Fill2  cos_minute_Fill2  \\\n",
       "0       -0.398401       -0.917211         -0.053222         -0.998583   \n",
       "1       -0.942261       -0.334880         -0.818303          0.574787   \n",
       "2        0.136167       -0.990686         -0.899312         -0.437307   \n",
       "3        0.816970       -0.576680          0.000000          1.000000   \n",
       "4       -0.631088       -0.775711          0.874763          0.484551   \n",
       "\n",
       "   sin_date_Fill2  \n",
       "0        0.968077  \n",
       "1       -0.912166  \n",
       "2        0.688967  \n",
       "3        0.455495  \n",
       "4       -0.378779  \n",
       "\n",
       "[5 rows x 306 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#중복열 제거\n",
    "duplicated_col = set(df_train.columns) - set(df_train.loc[:,~df_train.T.duplicated()].columns)\n",
    "duplicated_col = list(duplicated_col)\n",
    "\n",
    "print(\"중복 제거 전 컬럼 개수 : \", df_train.shape[1])\n",
    "df_train.drop(columns = duplicated_col, inplace = True)\n",
    "df_test.drop(columns = duplicated_col, inplace = True)\n",
    "print(\"중복 제거 후 컬럼 개수 : \", df_train.shape[1])\n",
    "\n",
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d0cfd42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org = df_train.iloc[:,:271]\n",
    "df_train_time = df_train.iloc[:,271:]\n",
    "\n",
    "df_test_org = df_test.iloc[:,:270]\n",
    "df_test_time = df_test.iloc[:,270:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9293b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna(0.0, inplace = True)\n",
    "df_test.fillna(0.0, inplace = True)\n",
    "\n",
    "df_train = df_train.replace([np.inf, -np.inf], 0.0)\n",
    "df_test = df_test.replace([np.inf, -np.inf], 0.0)\n",
    "\n",
    "df_train['Production Qty Collect Result'] = df_train['Production Qty Collect Result'].astype(int)\n",
    "df_train['Production_Sequence_ratio'] = df_train['Production_Sequence_ratio'].astype(float)\n",
    "\n",
    "df_test['Production Qty Collect Result'] = df_test['Production Qty Collect Result'].astype(int)\n",
    "df_test['Production_Sequence_ratio'] = df_test['Production_Sequence_ratio'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "45f5137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org.fillna(0.0, inplace = True)\n",
    "df_test_org.fillna(0.0, inplace = True)\n",
    "\n",
    "df_train_org = df_train_org.replace([np.inf, -np.inf], 0.0)\n",
    "df_test_org = df_test_org.replace([np.inf, -np.inf], 0.0)\n",
    "\n",
    "df_train_org['Production Qty Collect Result'] = df_train_org['Production Qty Collect Result'].astype(int)\n",
    "df_train_org['Production_Sequence_ratio'] = df_train_org['Production_Sequence_ratio'].astype(float)\n",
    "\n",
    "df_test_org['Production Qty Collect Result'] = df_test_org['Production Qty Collect Result'].astype(int)\n",
    "df_test_org['Production_Sequence_ratio'] = df_test_org['Production_Sequence_ratio'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f1194a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clof  = df_train.copy()\n",
    "df_test_clof = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaedfb1",
   "metadata": {},
   "source": [
    "### Scaling(Normalizing) & Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e5a0a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target mapping\n",
    "mapping = {'Normal': 0, 'AbNormal': 1}\n",
    "df_train['target'] = df_train['target'].map(mapping)\n",
    "\n",
    "#Identify numerical columns\n",
    "numerical_cols = df_train.select_dtypes(include=['float','int']).columns.to_list()\n",
    "numerical_cols.remove(\"target\")\n",
    "\n",
    "#Initialize the Normalizer\n",
    "scaler = Normalizer()\n",
    "\n",
    "#Fit and transform the numerical columns\n",
    "df_train[numerical_cols] = scaler.fit_transform(df_train[numerical_cols])\n",
    "df_test[numerical_cols] = scaler.transform(df_test[numerical_cols])\n",
    "\n",
    "#Target Encoding\n",
    "str_col = []\n",
    "for col in df_train.columns:\n",
    "    if df_train[col].dtype == \"object\":\n",
    "        str_col.append(col)\n",
    "\n",
    "for col in str_col:\n",
    "    te = ce.TargetEncoder()\n",
    "    df_train[col] = te.fit_transform(df_train[col], df_train['target'])\n",
    "    df_test[col] = te.transform(df_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "90730a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target mapping\n",
    "mapping = {'Normal': 0, 'AbNormal': 1}\n",
    "df_train_org['target'] = df_train_org['target'].map(mapping)\n",
    "\n",
    "#Identify numerical columns\n",
    "numerical_cols = df_train_org.select_dtypes(include=['float','int']).columns.to_list()\n",
    "numerical_cols.remove(\"target\")\n",
    "\n",
    "#Initialize the Normalizer\n",
    "scaler = Normalizer()\n",
    "\n",
    "#Fit and transform the numerical columns\n",
    "df_train_org[numerical_cols] = scaler.fit_transform(df_train_org[numerical_cols])\n",
    "df_test_org[numerical_cols] = scaler.transform(df_test_org[numerical_cols])\n",
    "\n",
    "#Target Encoding\n",
    "str_col = []\n",
    "for col in df_train_org.columns:\n",
    "    if df_train_org[col].dtype == \"object\":\n",
    "        str_col.append(col)\n",
    "\n",
    "for col in str_col:\n",
    "    te = ce.TargetEncoder()\n",
    "    df_train_org[col] = te.fit_transform(df_train_org[col], df_train_org['target'])\n",
    "    df_test_org[col] = te.transform(df_test_org[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9b6ab",
   "metadata": {},
   "source": [
    "### Validation Strategy based on Hard and Easy Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e795806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard samples: 16038, Easy samples: 24468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Equipment_PalletID</th>\n",
       "      <th>Model_Receip</th>\n",
       "      <th>Workorder</th>\n",
       "      <th>CURE END POSITION X Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Z Collect Result_Dam</th>\n",
       "      <th>CURE END POSITION Θ Collect Result_Dam</th>\n",
       "      <th>CURE SPEED Collect Result_Dam</th>\n",
       "      <th>CURE START POSITION X Collect Result_Dam</th>\n",
       "      <th>DISCHARGED SPEED OF RESIN Collect Result_Dam</th>\n",
       "      <th>DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam</th>\n",
       "      <th>...</th>\n",
       "      <th>speed_7</th>\n",
       "      <th>speed_9</th>\n",
       "      <th>speed_11</th>\n",
       "      <th>CURE_START_END_DISTANCE_Fill2_X</th>\n",
       "      <th>CURE_STANDBY_END_DISTANCE_Fill2_Z</th>\n",
       "      <th>CURE_TIME_X_Fill2</th>\n",
       "      <th>CURE_TIME_Z_Fill2</th>\n",
       "      <th>CURE_START_END_DISTANCE_Dam_X</th>\n",
       "      <th>CURE_TIME_X_Dam</th>\n",
       "      <th>easy_hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054457</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.158385</td>\n",
       "      <td>0.004127</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.001548</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.017714</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.013414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.013586</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>EASY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.054457</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.015314</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.016154</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.012233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>EASY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052058</td>\n",
       "      <td>0.056695</td>\n",
       "      <td>0.009534</td>\n",
       "      <td>0.017099</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.013337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.012311</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>EASY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042871</td>\n",
       "      <td>0.056695</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.021956</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.017126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.015809</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>EASY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060155</td>\n",
       "      <td>0.056695</td>\n",
       "      <td>0.109495</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000970</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.011097</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>2.154775e-07</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>HARD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Equipment_PalletID  Model_Receip  Workorder  \\\n",
       "0            0.054457      0.048600   0.158385   \n",
       "1            0.054457      0.048600   0.015314   \n",
       "2            0.052058      0.056695   0.009534   \n",
       "3            0.042871      0.056695   0.057143   \n",
       "4            0.060155      0.056695   0.109495   \n",
       "\n",
       "   CURE END POSITION X Collect Result_Dam  \\\n",
       "0                                0.004127   \n",
       "1                                0.003764   \n",
       "2                                0.017099   \n",
       "3                                0.021956   \n",
       "4                                0.002586   \n",
       "\n",
       "   CURE END POSITION Z Collect Result_Dam  \\\n",
       "0                                0.000043   \n",
       "1                                0.000039   \n",
       "2                                0.000214   \n",
       "3                                0.000274   \n",
       "4                                0.000027   \n",
       "\n",
       "   CURE END POSITION Θ Collect Result_Dam  CURE SPEED Collect Result_Dam  \\\n",
       "0                               -0.001548                       0.001720   \n",
       "1                               -0.001412                       0.001098   \n",
       "2                                0.001539                       0.001453   \n",
       "3                                0.001976                       0.001537   \n",
       "4                               -0.000970                       0.000754   \n",
       "\n",
       "   CURE START POSITION X Collect Result_Dam  \\\n",
       "0                                  0.017714   \n",
       "1                                  0.016154   \n",
       "2                                  0.004788   \n",
       "3                                  0.006148   \n",
       "4                                  0.011097   \n",
       "\n",
       "   DISCHARGED SPEED OF RESIN Collect Result_Dam  \\\n",
       "0                                      0.000275   \n",
       "1                                      0.000157   \n",
       "2                                      0.000274   \n",
       "3                                      0.000220   \n",
       "4                                      0.000108   \n",
       "\n",
       "   DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam  ...   speed_7  \\\n",
       "0                                           0.000256    ...  0.000017   \n",
       "1                                           0.000334    ...  0.000016   \n",
       "2                                           0.000251    ...  0.000017   \n",
       "3                                           0.000468    ...  0.000022   \n",
       "4                                           0.000105    ...  0.000011   \n",
       "\n",
       "    speed_9  speed_11  CURE_START_END_DISTANCE_Fill2_X  \\\n",
       "0  0.000017  0.000017                         0.013414   \n",
       "1  0.000016  0.000016                         0.012233   \n",
       "2  0.000017  0.000017                         0.013337   \n",
       "3  0.000022  0.000022                         0.017126   \n",
       "4  0.000011  0.000011                         0.008404   \n",
       "\n",
       "   CURE_STANDBY_END_DISTANCE_Fill2_Z  CURE_TIME_X_Fill2  CURE_TIME_Z_Fill2  \\\n",
       "0                           0.000000           0.000279       0.000000e+00   \n",
       "1                           0.000000           0.000245       0.000000e+00   \n",
       "2                           0.000000           0.000267       0.000000e+00   \n",
       "3                           0.000000           0.000343       0.000000e+00   \n",
       "4                           0.000011           0.000168       2.154775e-07   \n",
       "\n",
       "   CURE_START_END_DISTANCE_Dam_X  CURE_TIME_X_Dam  easy_hard  \n",
       "0                       0.013586         0.000136       EASY  \n",
       "1                       0.012390         0.000177       EASY  \n",
       "2                      -0.012311        -0.000145       EASY  \n",
       "3                      -0.015809        -0.000226       EASY  \n",
       "4                       0.008511         0.000122       HARD  \n",
       "\n",
       "[5 rows x 272 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Assuming df_train_org is your DataFrame and 'target' is your column of labels\n",
    "# Label 'normal' is mapped to 0, and 'abnormal' is mapped to 1 for clarity\n",
    "\n",
    "# Separate the data by label\n",
    "normal_data = df_train_org[df_train_org['target'] == 0].drop(columns=['target'])\n",
    "abnormal_data = df_train_org[df_train_org['target'] == 1].drop(columns=['target'])\n",
    "\n",
    "# Calculate covariance matrices\n",
    "cov_normal = np.cov(normal_data.T)\n",
    "cov_abnormal = np.cov(abnormal_data.T)\n",
    "\n",
    "# Add a small value to the diagonal for regularization\n",
    "reg_value = 1e-6\n",
    "cov_normal += np.eye(cov_normal.shape[0]) * reg_value\n",
    "cov_abnormal += np.eye(cov_abnormal.shape[0]) * reg_value\n",
    "\n",
    "# Calculate inverse covariance matrices\n",
    "inv_cov_normal = inv(cov_normal)\n",
    "inv_cov_abnormal = inv(cov_abnormal)\n",
    "\n",
    "# Calculate means\n",
    "mean_normal = np.mean(normal_data, axis=0)\n",
    "mean_abnormal = np.mean(abnormal_data, axis=0)\n",
    "\n",
    "hard_samples = []\n",
    "easy_samples = []\n",
    "\n",
    "# Define a threshold for classifying a sample as hard\n",
    "threshold = 0.3 # You can adjust this threshold as needed\n",
    "\n",
    "# Classify samples based on Mahalanobis distance\n",
    "for index, row in df_train_org.drop(columns=['target']).iterrows():\n",
    "    row_values = row.values\n",
    "    dist_normal = mahalanobis(row_values, mean_normal, inv_cov_normal)\n",
    "    dist_abnormal = mahalanobis(row_values, mean_abnormal, inv_cov_abnormal)\n",
    "\n",
    "    # If the distances are similar, consider it a hard sample\n",
    "    if abs(dist_normal - dist_abnormal) < threshold:\n",
    "        hard_samples.append(df_train_org.iloc[index])\n",
    "    else:\n",
    "        easy_samples.append(df_train_org.iloc[index])\n",
    "\n",
    "print(f\"Hard samples: {len(hard_samples)}, Easy samples: {len(easy_samples)}\")\n",
    "\n",
    "# Convert hard_samples and easy_samples into DataFrames for easy viewing\n",
    "hard_samples_df = pd.DataFrame(hard_samples)\n",
    "easy_samples_df = pd.DataFrame(easy_samples)\n",
    "\n",
    "hard_samples_df['easy_hard'] = \"HARD\"\n",
    "easy_samples_df['easy_hard'] = \"EASY\"\n",
    "\n",
    "df_train_org = pd.concat([hard_samples_df, easy_samples_df], axis = 0).sort_index()\n",
    "df_train_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0b579ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['easy_hard'] = df_train_org['easy_hard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bd387a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DL 시드 고정\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # GPU 사용 시 추가\n",
    "    torch.backends.cudnn.deterministic = True  # Reproducibility를 위한 설정\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f95e64",
   "metadata": {},
   "source": [
    "# clof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70d03543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training fold 1/10...\n",
      "Epoch 1/100, Loss: 0.1537\n",
      "Epoch 2/100, Loss: 0.1288\n",
      "Epoch 3/100, Loss: 0.1961\n",
      "Epoch 4/100, Loss: 0.1898\n",
      "Epoch 5/100, Loss: 0.2101\n",
      "Epoch 6/100, Loss: 0.0689\n",
      "Epoch 7/100, Loss: 0.1770\n",
      "Epoch 8/100, Loss: 0.0902\n",
      "Epoch 9/100, Loss: 0.0867\n",
      "Epoch 10/100, Loss: 0.0894\n",
      "Epoch 11/100, Loss: 0.1449\n",
      "Epoch 12/100, Loss: 0.0608\n",
      "Epoch 13/100, Loss: 0.0804\n",
      "Epoch 14/100, Loss: 0.0757\n",
      "Epoch 15/100, Loss: 0.0665\n",
      "Epoch 16/100, Loss: 0.0504\n",
      "Epoch 17/100, Loss: 0.0946\n",
      "Epoch 18/100, Loss: 0.0660\n",
      "Epoch 19/100, Loss: 0.0838\n",
      "Epoch 20/100, Loss: 0.0582\n",
      "Epoch 21/100, Loss: 0.0473\n",
      "Epoch 22/100, Loss: 0.0680\n",
      "Epoch 23/100, Loss: 0.1426\n",
      "Epoch 24/100, Loss: 0.0714\n",
      "Epoch 25/100, Loss: 0.0529\n",
      "Epoch 26/100, Loss: 0.0356\n",
      "Epoch 27/100, Loss: 0.0706\n",
      "Epoch 28/100, Loss: 0.0744\n",
      "Epoch 29/100, Loss: 0.0599\n",
      "Epoch 30/100, Loss: 0.0257\n",
      "Epoch 31/100, Loss: 0.0564\n",
      "Epoch 32/100, Loss: 0.1180\n",
      "Epoch 33/100, Loss: 0.0690\n",
      "Epoch 34/100, Loss: 0.0439\n",
      "Epoch 35/100, Loss: 0.0695\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 36/100, Loss: 0.0311\n",
      "Epoch 37/100, Loss: 0.0636\n",
      "Epoch 38/100, Loss: 0.0696\n",
      "Epoch 39/100, Loss: 0.0472\n",
      "Epoch 40/100, Loss: 0.0225\n",
      "Epoch 41/100, Loss: 0.0635\n",
      "Epoch 42/100, Loss: 0.0460\n",
      "Epoch 43/100, Loss: 0.0549\n",
      "Epoch 44/100, Loss: 0.0399\n",
      "Epoch 45/100, Loss: 0.0818\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 46/100, Loss: 0.0437\n",
      "Epoch 47/100, Loss: 0.0848\n",
      "Epoch 48/100, Loss: 0.0258\n",
      "Epoch 49/100, Loss: 0.0930\n",
      "Epoch 50/100, Loss: 0.0382\n",
      "Epoch 51/100, Loss: 0.0484\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 52/100, Loss: 0.0257\n",
      "Epoch 53/100, Loss: 0.0260\n",
      "Epoch 54/100, Loss: 0.0272\n",
      "Epoch 55/100, Loss: 0.0463\n",
      "Epoch 56/100, Loss: 0.1289\n",
      "Epoch 57/100, Loss: 0.0294\n",
      "Epoch 58/100, Loss: 0.0833\n",
      "Epoch 59/100, Loss: 0.0234\n",
      "Epoch 60/100, Loss: 0.0595\n",
      "Epoch 61/100, Loss: 0.0692\n",
      "Epoch 62/100, Loss: 0.0545\n",
      "Epoch 63/100, Loss: 0.0285\n",
      "Epoch 64/100, Loss: 0.1163\n",
      "Epoch 65/100, Loss: 0.1080\n",
      "Epoch 66/100, Loss: 0.0908\n",
      "Epoch 67/100, Loss: 0.1525\n",
      "Epoch 68/100, Loss: 0.1180\n",
      "Epoch 69/100, Loss: 0.0460\n",
      "Epoch 70/100, Loss: 0.0378\n",
      "Epoch 71/100, Loss: 0.0444\n",
      "Epoch 72/100, Loss: 0.0976\n",
      "Epoch 73/100, Loss: 0.0869\n",
      "Epoch 74/100, Loss: 0.0274\n",
      "Epoch 75/100, Loss: 0.0525\n",
      "Epoch 76/100, Loss: 0.0379\n",
      "Epoch 77/100, Loss: 0.0340\n",
      "Epoch 78/100, Loss: 0.0227\n",
      "Epoch 79/100, Loss: 0.0317\n",
      "Epoch 80/100, Loss: 0.0374\n",
      "Epoch 81/100, Loss: 0.0412\n",
      "Epoch 82/100, Loss: 0.0299\n",
      "Epoch 83/100, Loss: 0.0224\n",
      "Epoch 84/100, Loss: 0.0337\n",
      "Epoch 85/100, Loss: 0.0963\n",
      "Epoch 86/100, Loss: 0.0615\n",
      "Epoch 87/100, Loss: 0.0726\n",
      "Epoch 88/100, Loss: 0.0959\n",
      "Epoch 89/100, Loss: 0.1606\n",
      "Epoch 90/100, Loss: 0.0513\n",
      "Epoch 91/100, Loss: 0.0338\n",
      "Epoch 92/100, Loss: 0.0349\n",
      "Epoch 93/100, Loss: 0.0353\n",
      "Epoch 94/100, Loss: 0.0371\n",
      "Epoch 95/100, Loss: 0.0522\n",
      "Epoch 96/100, Loss: 0.0837\n",
      "Epoch 97/100, Loss: 0.0606\n",
      "Epoch 98/100, Loss: 0.0342\n",
      "Epoch 99/100, Loss: 0.0940\n",
      "Epoch 100/100, Loss: 0.0320\n",
      "Training fold 2/10...\n",
      "Epoch 1/100, Loss: 0.1468\n",
      "Epoch 2/100, Loss: 0.1680\n",
      "Epoch 3/100, Loss: 0.1129\n",
      "Epoch 4/100, Loss: 0.1033\n",
      "Epoch 5/100, Loss: 0.1407\n",
      "Epoch 6/100, Loss: 0.0626\n",
      "Epoch 7/100, Loss: 0.1187\n",
      "Epoch 8/100, Loss: 0.1166\n",
      "Epoch 9/100, Loss: 0.1013\n",
      "Epoch 10/100, Loss: 0.0628\n",
      "Epoch 11/100, Loss: 0.0715\n",
      "Epoch 12/100, Loss: 0.0616\n",
      "Epoch 13/100, Loss: 0.1438\n",
      "Epoch 14/100, Loss: 0.0685\n",
      "Epoch 15/100, Loss: 0.0591\n",
      "Epoch 16/100, Loss: 0.0663\n",
      "Epoch 17/100, Loss: 0.0836\n",
      "Epoch 18/100, Loss: 0.0599\n",
      "Epoch 19/100, Loss: 0.0791\n",
      "Epoch 20/100, Loss: 0.0548\n",
      "Epoch 21/100, Loss: 0.0759\n",
      "Epoch 22/100, Loss: 0.0559\n",
      "Epoch 23/100, Loss: 0.1042\n",
      "Epoch 24/100, Loss: 0.0320\n",
      "Epoch 25/100, Loss: 0.0772\n",
      "Epoch 26/100, Loss: 0.0906\n",
      "Epoch 27/100, Loss: 0.0664\n",
      "Epoch 28/100, Loss: 0.2288\n",
      "Epoch 29/100, Loss: 0.0599\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 30/100, Loss: 0.0463\n",
      "Epoch 31/100, Loss: 0.1123\n",
      "Epoch 32/100, Loss: 0.0345\n",
      "Epoch 33/100, Loss: 0.0556\n",
      "Epoch 34/100, Loss: 0.0426\n",
      "Epoch 35/100, Loss: 0.0321\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 36/100, Loss: 0.0420\n",
      "Epoch 37/100, Loss: 0.0770\n",
      "Epoch 38/100, Loss: 0.0566\n",
      "Epoch 39/100, Loss: 0.0452\n",
      "Epoch 40/100, Loss: 0.0429\n",
      "Epoch 41/100, Loss: 0.0557\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 42/100, Loss: 0.0966\n",
      "Epoch 43/100, Loss: 0.0494\n",
      "Epoch 44/100, Loss: 0.0543\n",
      "Epoch 45/100, Loss: 0.0427\n",
      "Epoch 46/100, Loss: 0.0354\n",
      "Epoch 47/100, Loss: 0.1220\n",
      "Epoch 48/100, Loss: 0.0348\n",
      "Epoch 49/100, Loss: 0.0300\n",
      "Epoch 50/100, Loss: 0.0387\n",
      "Epoch 51/100, Loss: 0.0398\n",
      "Epoch 52/100, Loss: 0.0315\n",
      "Epoch 53/100, Loss: 0.0733\n",
      "Epoch 54/100, Loss: 0.0286\n",
      "Epoch 55/100, Loss: 0.0274\n",
      "Epoch 56/100, Loss: 0.0406\n",
      "Epoch 57/100, Loss: 0.0572\n",
      "Epoch 58/100, Loss: 0.0577\n",
      "Epoch 59/100, Loss: 0.0464\n",
      "Epoch 60/100, Loss: 0.0596\n",
      "Epoch 61/100, Loss: 0.0492\n",
      "Epoch 62/100, Loss: 0.0343\n",
      "Epoch 63/100, Loss: 0.0300\n",
      "Epoch 64/100, Loss: 0.0992\n",
      "Epoch 65/100, Loss: 0.0810\n",
      "Epoch 66/100, Loss: 0.0410\n",
      "Epoch 67/100, Loss: 0.0388\n",
      "Epoch 68/100, Loss: 0.0595\n",
      "Epoch 69/100, Loss: 0.0404\n",
      "Epoch 70/100, Loss: 0.0444\n",
      "Epoch 71/100, Loss: 0.0489\n",
      "Epoch 72/100, Loss: 0.0463\n",
      "Epoch 73/100, Loss: 0.0894\n",
      "Epoch 74/100, Loss: 0.0413\n",
      "Epoch 75/100, Loss: 0.0314\n",
      "Epoch 76/100, Loss: 0.0576\n",
      "Epoch 77/100, Loss: 0.1149\n",
      "Epoch 78/100, Loss: 0.0417\n",
      "Epoch 79/100, Loss: 0.0470\n",
      "Epoch 80/100, Loss: 0.1170\n",
      "Epoch 81/100, Loss: 0.0572\n",
      "Epoch 82/100, Loss: 0.0474\n",
      "Epoch 83/100, Loss: 0.0324\n",
      "Epoch 84/100, Loss: 0.0299\n",
      "Epoch 85/100, Loss: 0.1092\n",
      "Epoch 86/100, Loss: 0.0541\n",
      "Epoch 87/100, Loss: 0.1106\n",
      "Epoch 88/100, Loss: 0.0825\n",
      "Epoch 89/100, Loss: 0.0456\n",
      "Epoch 90/100, Loss: 0.0373\n",
      "Epoch 91/100, Loss: 0.0388\n",
      "Epoch 92/100, Loss: 0.0488\n",
      "Epoch 93/100, Loss: 0.0578\n",
      "Epoch 94/100, Loss: 0.0443\n",
      "Epoch 95/100, Loss: 0.0596\n",
      "Epoch 96/100, Loss: 0.0345\n",
      "Epoch 97/100, Loss: 0.0320\n",
      "Epoch 98/100, Loss: 0.0491\n",
      "Epoch 99/100, Loss: 0.0443\n",
      "Epoch 100/100, Loss: 0.0294\n",
      "Training fold 3/10...\n",
      "Epoch 1/100, Loss: 0.1407\n",
      "Epoch 2/100, Loss: 0.1194\n",
      "Epoch 3/100, Loss: 0.1251\n",
      "Epoch 4/100, Loss: 0.1899\n",
      "Epoch 5/100, Loss: 0.1014\n",
      "Epoch 6/100, Loss: 0.1021\n",
      "Epoch 7/100, Loss: 0.0564\n",
      "Epoch 8/100, Loss: 0.0597\n",
      "Epoch 9/100, Loss: 0.0790\n",
      "Epoch 10/100, Loss: 0.1114\n",
      "Epoch 11/100, Loss: 0.0733\n",
      "Epoch 12/100, Loss: 0.0756\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13/100, Loss: 0.0675\n",
      "Epoch 14/100, Loss: 0.0559\n",
      "Epoch 15/100, Loss: 0.0549\n",
      "Epoch 16/100, Loss: 0.0556\n",
      "Epoch 17/100, Loss: 0.0411\n",
      "Epoch 18/100, Loss: 0.0767\n",
      "Epoch 19/100, Loss: 0.1193\n",
      "Epoch 20/100, Loss: 0.0962\n",
      "Epoch 21/100, Loss: 0.0990\n",
      "Epoch 22/100, Loss: 0.1696\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 23/100, Loss: 0.1075\n",
      "Epoch 24/100, Loss: 0.1046\n",
      "Epoch 25/100, Loss: 0.1119\n",
      "Epoch 26/100, Loss: 0.1019\n",
      "Epoch 27/100, Loss: 0.0661\n",
      "Epoch 28/100, Loss: 0.0549\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 29/100, Loss: 0.0658\n",
      "Epoch 30/100, Loss: 0.0746\n",
      "Epoch 31/100, Loss: 0.0484\n",
      "Epoch 32/100, Loss: 0.0640\n",
      "Epoch 33/100, Loss: 0.0474\n",
      "Epoch 34/100, Loss: 0.0572\n",
      "Epoch 35/100, Loss: 0.2239\n",
      "Epoch 36/100, Loss: 0.0758\n",
      "Epoch 37/100, Loss: 0.0535\n",
      "Epoch 38/100, Loss: 0.1944\n",
      "Epoch 39/100, Loss: 0.0733\n",
      "Epoch 40/100, Loss: 0.0898\n",
      "Epoch 41/100, Loss: 0.1182\n",
      "Epoch 42/100, Loss: 0.1280\n",
      "Epoch 43/100, Loss: 0.0681\n",
      "Epoch 44/100, Loss: 0.0785\n",
      "Epoch 45/100, Loss: 0.0609\n",
      "Epoch 46/100, Loss: 0.0565\n",
      "Epoch 47/100, Loss: 0.0533\n",
      "Epoch 48/100, Loss: 0.0559\n",
      "Epoch 49/100, Loss: 0.0704\n",
      "Epoch 50/100, Loss: 0.0765\n",
      "Epoch 51/100, Loss: 0.1147\n",
      "Epoch 52/100, Loss: 0.0586\n",
      "Epoch 53/100, Loss: 0.0833\n",
      "Epoch 54/100, Loss: 0.1131\n",
      "Epoch 55/100, Loss: 0.0470\n",
      "Epoch 56/100, Loss: 0.0531\n",
      "Epoch 57/100, Loss: 0.0794\n",
      "Epoch 58/100, Loss: 0.0585\n",
      "Epoch 59/100, Loss: 0.0752\n",
      "Epoch 60/100, Loss: 0.1807\n",
      "Epoch 61/100, Loss: 0.0787\n",
      "Epoch 62/100, Loss: 0.0631\n",
      "Epoch 63/100, Loss: 0.0581\n",
      "Epoch 64/100, Loss: 0.0447\n",
      "Epoch 65/100, Loss: 0.0448\n",
      "Epoch 66/100, Loss: 0.0697\n",
      "Epoch 67/100, Loss: 0.0674\n",
      "Epoch 68/100, Loss: 0.0463\n",
      "Epoch 69/100, Loss: 0.0726\n",
      "Epoch 70/100, Loss: 0.0538\n",
      "Epoch 71/100, Loss: 0.0563\n",
      "Epoch 72/100, Loss: 0.0581\n",
      "Epoch 73/100, Loss: 0.0540\n",
      "Epoch 74/100, Loss: 0.0525\n",
      "Epoch 75/100, Loss: 0.0563\n",
      "Epoch 76/100, Loss: 0.0520\n",
      "Epoch 77/100, Loss: 0.0737\n",
      "Epoch 78/100, Loss: 0.0569\n",
      "Epoch 79/100, Loss: 0.0548\n",
      "Epoch 80/100, Loss: 0.0802\n",
      "Epoch 81/100, Loss: 0.0539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 0.0437\n",
      "Epoch 83/100, Loss: 0.0473\n",
      "Epoch 84/100, Loss: 0.0887\n",
      "Epoch 85/100, Loss: 0.1344\n",
      "Epoch 86/100, Loss: 0.0686\n",
      "Epoch 87/100, Loss: 0.1645\n",
      "Epoch 88/100, Loss: 0.0542\n",
      "Epoch 89/100, Loss: 0.0578\n",
      "Epoch 90/100, Loss: 0.0476\n",
      "Epoch 91/100, Loss: 0.0836\n",
      "Epoch 92/100, Loss: 0.0513\n",
      "Epoch 93/100, Loss: 0.0777\n",
      "Epoch 94/100, Loss: 0.0715\n",
      "Epoch 95/100, Loss: 0.1738\n",
      "Epoch 96/100, Loss: 0.1370\n",
      "Epoch 97/100, Loss: 0.0350\n",
      "Epoch 98/100, Loss: 0.0967\n",
      "Epoch 99/100, Loss: 0.0655\n",
      "Epoch 100/100, Loss: 0.0729\n",
      "Training fold 4/10...\n",
      "Epoch 1/100, Loss: 0.1878\n",
      "Epoch 2/100, Loss: 0.1250\n",
      "Epoch 3/100, Loss: 0.0784\n",
      "Epoch 4/100, Loss: 0.1205\n",
      "Epoch 5/100, Loss: 0.1029\n",
      "Epoch 6/100, Loss: 0.1309\n",
      "Epoch 7/100, Loss: 0.1164\n",
      "Epoch 8/100, Loss: 0.0904\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 9/100, Loss: 0.0936\n",
      "Epoch 10/100, Loss: 0.0948\n",
      "Epoch 11/100, Loss: 0.1042\n",
      "Epoch 12/100, Loss: 0.1433\n",
      "Epoch 13/100, Loss: 0.1111\n",
      "Epoch 14/100, Loss: 0.0938\n",
      "Epoch 15/100, Loss: 0.0742\n",
      "Epoch 16/100, Loss: 0.0964\n",
      "Epoch 17/100, Loss: 0.1579\n",
      "Epoch 18/100, Loss: 0.1022\n",
      "Epoch 19/100, Loss: 0.1074\n",
      "Epoch 20/100, Loss: 0.0564\n",
      "Epoch 21/100, Loss: 0.1804\n",
      "Epoch 22/100, Loss: 0.1363\n",
      "Epoch 23/100, Loss: 0.0922\n",
      "Epoch 24/100, Loss: 0.0770\n",
      "Epoch 25/100, Loss: 0.0777\n",
      "Epoch 26/100, Loss: 0.0557\n",
      "Epoch 27/100, Loss: 0.1315\n",
      "Epoch 28/100, Loss: 0.0938\n",
      "Epoch 29/100, Loss: 0.0712\n",
      "Epoch 30/100, Loss: 0.0997\n",
      "Epoch 31/100, Loss: 0.1084\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 32/100, Loss: 0.0853\n",
      "Epoch 33/100, Loss: 0.0847\n",
      "Epoch 34/100, Loss: 0.0608\n",
      "Epoch 35/100, Loss: 0.1064\n",
      "Epoch 36/100, Loss: 0.0733\n",
      "Epoch 37/100, Loss: 0.0508\n",
      "Epoch 38/100, Loss: 0.0751\n",
      "Epoch 39/100, Loss: 0.0724\n",
      "Epoch 40/100, Loss: 0.1404\n",
      "Epoch 41/100, Loss: 0.0692\n",
      "Epoch 42/100, Loss: 0.2231\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 43/100, Loss: 0.0785\n",
      "Epoch 44/100, Loss: 0.1377\n",
      "Epoch 45/100, Loss: 0.0946\n",
      "Epoch 46/100, Loss: 0.0614\n",
      "Epoch 47/100, Loss: 0.0861\n",
      "Epoch 48/100, Loss: 0.0709\n",
      "Epoch 49/100, Loss: 0.0598\n",
      "Epoch 50/100, Loss: 0.0655\n",
      "Epoch 51/100, Loss: 0.1219\n",
      "Epoch 52/100, Loss: 0.1677\n",
      "Epoch 53/100, Loss: 0.0520\n",
      "Epoch 54/100, Loss: 0.1787\n",
      "Epoch 55/100, Loss: 0.0779\n",
      "Epoch 56/100, Loss: 0.0698\n",
      "Epoch 57/100, Loss: 0.0868\n",
      "Epoch 58/100, Loss: 0.0748\n",
      "Epoch 59/100, Loss: 0.1451\n",
      "Epoch 60/100, Loss: 0.1158\n",
      "Epoch 61/100, Loss: 0.0832\n",
      "Epoch 62/100, Loss: 0.0726\n",
      "Epoch 63/100, Loss: 0.0991\n",
      "Epoch 64/100, Loss: 0.0963\n",
      "Epoch 65/100, Loss: 0.0607\n",
      "Epoch 66/100, Loss: 0.1057\n",
      "Epoch 67/100, Loss: 0.1343\n",
      "Epoch 68/100, Loss: 0.1093\n",
      "Epoch 69/100, Loss: 0.0816\n",
      "Epoch 70/100, Loss: 0.0669\n",
      "Epoch 71/100, Loss: 0.0626\n",
      "Epoch 72/100, Loss: 0.0805\n",
      "Epoch 73/100, Loss: 0.1804\n",
      "Epoch 74/100, Loss: 0.1115\n",
      "Epoch 75/100, Loss: 0.0697\n",
      "Epoch 76/100, Loss: 0.1106\n",
      "Epoch 77/100, Loss: 0.0720\n",
      "Epoch 78/100, Loss: 0.1028\n",
      "Epoch 79/100, Loss: 0.0710\n",
      "Epoch 80/100, Loss: 0.1137\n",
      "Epoch 81/100, Loss: 0.0421\n",
      "Epoch 82/100, Loss: 0.0786\n",
      "Epoch 83/100, Loss: 0.2400\n",
      "Epoch 84/100, Loss: 0.0553\n",
      "Epoch 85/100, Loss: 0.0483\n",
      "Epoch 86/100, Loss: 0.0849\n",
      "Epoch 87/100, Loss: 0.0771\n",
      "Epoch 88/100, Loss: 0.0907\n",
      "Epoch 89/100, Loss: 0.1331\n",
      "Epoch 90/100, Loss: 0.0639\n",
      "Epoch 91/100, Loss: 0.0680\n",
      "Epoch 92/100, Loss: 0.0905\n",
      "Epoch 93/100, Loss: 0.0950\n",
      "Epoch 94/100, Loss: 0.0546\n",
      "Epoch 95/100, Loss: 0.0839\n",
      "Epoch 96/100, Loss: 0.0691\n",
      "Epoch 97/100, Loss: 0.0874\n",
      "Epoch 98/100, Loss: 0.1863\n",
      "Epoch 99/100, Loss: 0.1010\n",
      "Epoch 100/100, Loss: 0.1088\n",
      "Training fold 5/10...\n",
      "Epoch 1/100, Loss: 0.1999\n",
      "Epoch 2/100, Loss: 0.1442\n",
      "Epoch 3/100, Loss: 0.1353\n",
      "Epoch 4/100, Loss: 0.1748\n",
      "Epoch 5/100, Loss: 0.1342\n",
      "Epoch 6/100, Loss: 0.1387\n",
      "Epoch 7/100, Loss: 0.1944\n",
      "Epoch 8/100, Loss: 0.0906\n",
      "Epoch 9/100, Loss: 0.0752\n",
      "Epoch 10/100, Loss: 0.0826\n",
      "Epoch 11/100, Loss: 0.0951\n",
      "Epoch 12/100, Loss: 0.1921\n",
      "Epoch 13/100, Loss: 0.1230\n",
      "Epoch 14/100, Loss: 0.0981\n",
      "Epoch 15/100, Loss: 0.0527\n",
      "Epoch 16/100, Loss: 0.0728\n",
      "Epoch 17/100, Loss: 0.0596\n",
      "Epoch 18/100, Loss: 0.0699\n",
      "Epoch 19/100, Loss: 0.0760\n",
      "Epoch 20/100, Loss: 0.1905\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 21/100, Loss: 0.1848\n",
      "Epoch 22/100, Loss: 0.0696\n",
      "Epoch 23/100, Loss: 0.0492\n",
      "Epoch 24/100, Loss: 0.0725\n",
      "Epoch 25/100, Loss: 0.1376\n",
      "Epoch 26/100, Loss: 0.0669\n",
      "Epoch 27/100, Loss: 0.0485\n",
      "Epoch 28/100, Loss: 0.2187\n",
      "Epoch 29/100, Loss: 0.0518\n",
      "Epoch 30/100, Loss: 0.0661\n",
      "Epoch 31/100, Loss: 0.0800\n",
      "Epoch 32/100, Loss: 0.0682\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 33/100, Loss: 0.0512\n",
      "Epoch 34/100, Loss: 0.0563\n",
      "Epoch 35/100, Loss: 0.0693\n",
      "Epoch 36/100, Loss: 0.0507\n",
      "Epoch 37/100, Loss: 0.2353\n",
      "Epoch 38/100, Loss: 0.0366\n",
      "Epoch 39/100, Loss: 0.0638\n",
      "Epoch 40/100, Loss: 0.0824\n",
      "Epoch 41/100, Loss: 0.1552\n",
      "Epoch 42/100, Loss: 0.0900\n",
      "Epoch 43/100, Loss: 0.0625\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 44/100, Loss: 0.0994\n",
      "Epoch 45/100, Loss: 0.1050\n",
      "Epoch 46/100, Loss: 0.0488\n",
      "Epoch 47/100, Loss: 0.0603\n",
      "Epoch 48/100, Loss: 0.0798\n",
      "Epoch 49/100, Loss: 0.0602\n",
      "Epoch 50/100, Loss: 0.0490\n",
      "Epoch 51/100, Loss: 0.0510\n",
      "Epoch 52/100, Loss: 0.1395\n",
      "Epoch 53/100, Loss: 0.1481\n",
      "Epoch 54/100, Loss: 0.0698\n",
      "Epoch 55/100, Loss: 0.0640\n",
      "Epoch 56/100, Loss: 0.1445\n",
      "Epoch 57/100, Loss: 0.0498\n",
      "Epoch 58/100, Loss: 0.0961\n",
      "Epoch 59/100, Loss: 0.0646\n",
      "Epoch 60/100, Loss: 0.0748\n",
      "Epoch 61/100, Loss: 0.1119\n",
      "Epoch 62/100, Loss: 0.0566\n",
      "Epoch 63/100, Loss: 0.0611\n",
      "Epoch 64/100, Loss: 0.0983\n",
      "Epoch 65/100, Loss: 0.0832\n",
      "Epoch 66/100, Loss: 0.0940\n",
      "Epoch 67/100, Loss: 0.0736\n",
      "Epoch 68/100, Loss: 0.1224\n",
      "Epoch 69/100, Loss: 0.1518\n",
      "Epoch 70/100, Loss: 0.0795\n",
      "Epoch 71/100, Loss: 0.0796\n",
      "Epoch 72/100, Loss: 0.0536\n",
      "Epoch 73/100, Loss: 0.0681\n",
      "Epoch 74/100, Loss: 0.1585\n",
      "Epoch 75/100, Loss: 0.0480\n",
      "Epoch 76/100, Loss: 0.0600\n",
      "Epoch 77/100, Loss: 0.1744\n",
      "Epoch 78/100, Loss: 0.0695\n",
      "Epoch 79/100, Loss: 0.0630\n",
      "Epoch 80/100, Loss: 0.0839\n",
      "Epoch 81/100, Loss: 0.1300\n",
      "Epoch 82/100, Loss: 0.0638\n",
      "Epoch 83/100, Loss: 0.0541\n",
      "Epoch 84/100, Loss: 0.0562\n",
      "Epoch 85/100, Loss: 0.0553\n",
      "Epoch 86/100, Loss: 0.0613\n",
      "Epoch 87/100, Loss: 0.0560\n",
      "Epoch 88/100, Loss: 0.0505\n",
      "Epoch 89/100, Loss: 0.0609\n",
      "Epoch 90/100, Loss: 0.0730\n",
      "Epoch 91/100, Loss: 0.0410\n",
      "Epoch 92/100, Loss: 0.1331\n",
      "Epoch 93/100, Loss: 0.0687\n",
      "Epoch 94/100, Loss: 0.0665\n",
      "Epoch 95/100, Loss: 0.0813\n",
      "Epoch 96/100, Loss: 0.0514\n",
      "Epoch 97/100, Loss: 0.0600\n",
      "Epoch 98/100, Loss: 0.0923\n",
      "Epoch 99/100, Loss: 0.0684\n",
      "Epoch 100/100, Loss: 0.0764\n",
      "Training fold 6/10...\n",
      "Epoch 1/100, Loss: 0.1463\n",
      "Epoch 2/100, Loss: 0.2339\n",
      "Epoch 3/100, Loss: 0.1428\n",
      "Epoch 4/100, Loss: 0.0998\n",
      "Epoch 5/100, Loss: 0.0962\n",
      "Epoch 6/100, Loss: 0.1387\n",
      "Epoch 7/100, Loss: 0.0939\n",
      "Epoch 8/100, Loss: 0.0927\n",
      "Epoch 9/100, Loss: 0.0530\n",
      "Epoch 10/100, Loss: 0.0698\n",
      "Epoch 11/100, Loss: 0.0567\n",
      "Epoch 12/100, Loss: 0.1575\n",
      "Epoch 13/100, Loss: 0.0743\n",
      "Epoch 14/100, Loss: 0.1681\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15/100, Loss: 0.0679\n",
      "Epoch 16/100, Loss: 0.0822\n",
      "Epoch 17/100, Loss: 0.0646\n",
      "Epoch 18/100, Loss: 0.0568\n",
      "Epoch 19/100, Loss: 0.0687\n",
      "Epoch 20/100, Loss: 0.0478\n",
      "Epoch 21/100, Loss: 0.0529\n",
      "Epoch 22/100, Loss: 0.1261\n",
      "Epoch 23/100, Loss: 0.1241\n",
      "Epoch 24/100, Loss: 0.0652\n",
      "Epoch 25/100, Loss: 0.0327\n",
      "Epoch 26/100, Loss: 0.0753\n",
      "Epoch 27/100, Loss: 0.1006\n",
      "Epoch 28/100, Loss: 0.0777\n",
      "Epoch 29/100, Loss: 0.1507\n",
      "Epoch 30/100, Loss: 0.0661\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 31/100, Loss: 0.0870\n",
      "Epoch 32/100, Loss: 0.1412\n",
      "Epoch 33/100, Loss: 0.0632\n",
      "Epoch 34/100, Loss: 0.0543\n",
      "Epoch 35/100, Loss: 0.1651\n",
      "Epoch 36/100, Loss: 0.1039\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 37/100, Loss: 0.0452\n",
      "Epoch 38/100, Loss: 0.0529\n",
      "Epoch 39/100, Loss: 0.0467\n",
      "Epoch 40/100, Loss: 0.0762\n",
      "Epoch 41/100, Loss: 0.0549\n",
      "Epoch 42/100, Loss: 0.0581\n",
      "Epoch 43/100, Loss: 0.0720\n",
      "Epoch 44/100, Loss: 0.1464\n",
      "Epoch 45/100, Loss: 0.0404\n",
      "Epoch 46/100, Loss: 0.0667\n",
      "Epoch 47/100, Loss: 0.1786\n",
      "Epoch 48/100, Loss: 0.0570\n",
      "Epoch 49/100, Loss: 0.1598\n",
      "Epoch 50/100, Loss: 0.0668\n",
      "Epoch 51/100, Loss: 0.0417\n",
      "Epoch 52/100, Loss: 0.0499\n",
      "Epoch 53/100, Loss: 0.1545\n",
      "Epoch 54/100, Loss: 0.1276\n",
      "Epoch 55/100, Loss: 0.0517\n",
      "Epoch 56/100, Loss: 0.0465\n",
      "Epoch 57/100, Loss: 0.0438\n",
      "Epoch 58/100, Loss: 0.0582\n",
      "Epoch 59/100, Loss: 0.0749\n",
      "Epoch 60/100, Loss: 0.0564\n",
      "Epoch 61/100, Loss: 0.0554\n",
      "Epoch 62/100, Loss: 0.0505\n",
      "Epoch 63/100, Loss: 0.0516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100, Loss: 0.0677\n",
      "Epoch 65/100, Loss: 0.0691\n",
      "Epoch 66/100, Loss: 0.1262\n",
      "Epoch 67/100, Loss: 0.1566\n",
      "Epoch 68/100, Loss: 0.0633\n",
      "Epoch 69/100, Loss: 0.0554\n",
      "Epoch 70/100, Loss: 0.0701\n",
      "Epoch 71/100, Loss: 0.0910\n",
      "Epoch 72/100, Loss: 0.0828\n",
      "Epoch 73/100, Loss: 0.0484\n",
      "Epoch 74/100, Loss: 0.0711\n",
      "Epoch 75/100, Loss: 0.0456\n",
      "Epoch 76/100, Loss: 0.1472\n",
      "Epoch 77/100, Loss: 0.0520\n",
      "Epoch 78/100, Loss: 0.0556\n",
      "Epoch 79/100, Loss: 0.1672\n",
      "Epoch 80/100, Loss: 0.0561\n",
      "Epoch 81/100, Loss: 0.0662\n",
      "Epoch 82/100, Loss: 0.0693\n",
      "Epoch 83/100, Loss: 0.0440\n",
      "Epoch 84/100, Loss: 0.0480\n",
      "Epoch 85/100, Loss: 0.0772\n",
      "Epoch 86/100, Loss: 0.0573\n",
      "Epoch 87/100, Loss: 0.0604\n",
      "Epoch 88/100, Loss: 0.0639\n",
      "Epoch 89/100, Loss: 0.1035\n",
      "Epoch 90/100, Loss: 0.1061\n",
      "Epoch 91/100, Loss: 0.1042\n",
      "Epoch 92/100, Loss: 0.1010\n",
      "Epoch 93/100, Loss: 0.0620\n",
      "Epoch 94/100, Loss: 0.0524\n",
      "Epoch 95/100, Loss: 0.2200\n",
      "Epoch 96/100, Loss: 0.1435\n",
      "Epoch 97/100, Loss: 0.1017\n",
      "Epoch 98/100, Loss: 0.0908\n",
      "Epoch 99/100, Loss: 0.0557\n",
      "Epoch 100/100, Loss: 0.0542\n",
      "Training fold 7/10...\n",
      "Epoch 1/100, Loss: 0.1632\n",
      "Epoch 2/100, Loss: 0.1512\n",
      "Epoch 3/100, Loss: 0.1662\n",
      "Epoch 4/100, Loss: 0.1032\n",
      "Epoch 5/100, Loss: 0.1186\n",
      "Epoch 6/100, Loss: 0.1033\n",
      "Epoch 7/100, Loss: 0.1007\n",
      "Epoch 8/100, Loss: 0.1062\n",
      "Epoch 9/100, Loss: 0.3324\n",
      "Epoch 10/100, Loss: 0.0804\n",
      "Epoch 11/100, Loss: 0.0569\n",
      "Epoch 12/100, Loss: 0.0766\n",
      "Epoch 13/100, Loss: 0.2104\n",
      "Epoch 14/100, Loss: 0.0705\n",
      "Epoch 15/100, Loss: 0.0974\n",
      "Epoch 16/100, Loss: 0.3007\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 17/100, Loss: 0.1850\n",
      "Epoch 18/100, Loss: 0.0635\n",
      "Epoch 19/100, Loss: 0.1341\n",
      "Epoch 20/100, Loss: 0.0742\n",
      "Epoch 21/100, Loss: 0.0652\n",
      "Epoch 22/100, Loss: 0.1329\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 23/100, Loss: 0.1500\n",
      "Epoch 24/100, Loss: 0.1028\n",
      "Epoch 25/100, Loss: 0.1048\n",
      "Epoch 26/100, Loss: 0.0521\n",
      "Epoch 27/100, Loss: 0.0649\n",
      "Epoch 28/100, Loss: 0.0568\n",
      "Epoch 29/100, Loss: 0.0692\n",
      "Epoch 30/100, Loss: 0.0596\n",
      "Epoch 31/100, Loss: 0.0553\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 32/100, Loss: 0.0692\n",
      "Epoch 33/100, Loss: 0.1593\n",
      "Epoch 34/100, Loss: 0.0541\n",
      "Epoch 35/100, Loss: 0.0480\n",
      "Epoch 36/100, Loss: 0.0526\n",
      "Epoch 37/100, Loss: 0.1304\n",
      "Epoch 38/100, Loss: 0.0889\n",
      "Epoch 39/100, Loss: 0.0581\n",
      "Epoch 40/100, Loss: 0.0563\n",
      "Epoch 41/100, Loss: 0.0515\n",
      "Epoch 42/100, Loss: 0.0567\n",
      "Epoch 43/100, Loss: 0.0585\n",
      "Epoch 44/100, Loss: 0.0757\n",
      "Epoch 45/100, Loss: 0.0500\n",
      "Epoch 46/100, Loss: 0.0603\n",
      "Epoch 47/100, Loss: 0.0562\n",
      "Epoch 48/100, Loss: 0.0980\n",
      "Epoch 49/100, Loss: 0.0589\n",
      "Epoch 50/100, Loss: 0.0571\n",
      "Epoch 51/100, Loss: 0.0492\n",
      "Epoch 52/100, Loss: 0.0713\n",
      "Epoch 53/100, Loss: 0.0578\n",
      "Epoch 54/100, Loss: 0.0661\n",
      "Epoch 55/100, Loss: 0.0663\n",
      "Epoch 56/100, Loss: 0.0556\n",
      "Epoch 57/100, Loss: 0.0412\n",
      "Epoch 58/100, Loss: 0.1115\n",
      "Epoch 59/100, Loss: 0.0749\n",
      "Epoch 60/100, Loss: 0.1941\n",
      "Epoch 61/100, Loss: 0.0701\n",
      "Epoch 62/100, Loss: 0.0803\n",
      "Epoch 63/100, Loss: 0.0573\n",
      "Epoch 64/100, Loss: 0.0904\n",
      "Epoch 65/100, Loss: 0.0395\n",
      "Epoch 66/100, Loss: 0.0650\n",
      "Epoch 67/100, Loss: 0.0644\n",
      "Epoch 68/100, Loss: 0.0679\n",
      "Epoch 69/100, Loss: 0.0614\n",
      "Epoch 70/100, Loss: 0.0662\n",
      "Epoch 71/100, Loss: 0.0451\n",
      "Epoch 72/100, Loss: 0.1337\n",
      "Epoch 73/100, Loss: 0.0614\n",
      "Epoch 74/100, Loss: 0.0850\n",
      "Epoch 75/100, Loss: 0.1504\n",
      "Epoch 76/100, Loss: 0.0845\n",
      "Epoch 77/100, Loss: 0.0982\n",
      "Epoch 78/100, Loss: 0.0663\n",
      "Epoch 79/100, Loss: 0.0524\n",
      "Epoch 80/100, Loss: 0.1008\n",
      "Epoch 81/100, Loss: 0.0699\n",
      "Epoch 82/100, Loss: 0.0453\n",
      "Epoch 83/100, Loss: 0.1391\n",
      "Epoch 84/100, Loss: 0.0525\n",
      "Epoch 85/100, Loss: 0.0884\n",
      "Epoch 86/100, Loss: 0.1063\n",
      "Epoch 87/100, Loss: 0.0994\n",
      "Epoch 88/100, Loss: 0.1370\n",
      "Epoch 89/100, Loss: 0.0926\n",
      "Epoch 90/100, Loss: 0.0544\n",
      "Epoch 91/100, Loss: 0.1048\n",
      "Epoch 92/100, Loss: 0.0512\n",
      "Epoch 93/100, Loss: 0.0909\n",
      "Epoch 94/100, Loss: 0.0546\n",
      "Epoch 95/100, Loss: 0.0623\n",
      "Epoch 96/100, Loss: 0.0821\n",
      "Epoch 97/100, Loss: 0.0560\n",
      "Epoch 98/100, Loss: 0.0484\n",
      "Epoch 99/100, Loss: 0.0330\n",
      "Epoch 100/100, Loss: 0.0574\n",
      "Training fold 8/10...\n",
      "Epoch 1/100, Loss: 0.1670\n",
      "Epoch 2/100, Loss: 0.2044\n",
      "Epoch 3/100, Loss: 0.1316\n",
      "Epoch 4/100, Loss: 0.1275\n",
      "Epoch 5/100, Loss: 0.1590\n",
      "Epoch 6/100, Loss: 0.1049\n",
      "Epoch 7/100, Loss: 0.0742\n",
      "Epoch 8/100, Loss: 0.2372\n",
      "Epoch 9/100, Loss: 0.1673\n",
      "Epoch 10/100, Loss: 0.0684\n",
      "Epoch 11/100, Loss: 0.1100\n",
      "Epoch 12/100, Loss: 0.0604\n",
      "Epoch 13/100, Loss: 0.2196\n",
      "Epoch 14/100, Loss: 0.1051\n",
      "Epoch 15/100, Loss: 0.0846\n",
      "Epoch 16/100, Loss: 0.0772\n",
      "Epoch 17/100, Loss: 0.0767\n",
      "Epoch 18/100, Loss: 0.0589\n",
      "Epoch 19/100, Loss: 0.0920\n",
      "Epoch 20/100, Loss: 0.0495\n",
      "Epoch 21/100, Loss: 0.0704\n",
      "Epoch 22/100, Loss: 0.0549\n",
      "Epoch 23/100, Loss: 0.0663\n",
      "Epoch 24/100, Loss: 0.0481\n",
      "Epoch 25/100, Loss: 0.0620\n",
      "Epoch 26/100, Loss: 0.0617\n",
      "Epoch 27/100, Loss: 0.0836\n",
      "Epoch 28/100, Loss: 0.0472\n",
      "Epoch 29/100, Loss: 0.0421\n",
      "Epoch 30/100, Loss: 0.0492\n",
      "Epoch 31/100, Loss: 0.0445\n",
      "Epoch 32/100, Loss: 0.0522\n",
      "Epoch 33/100, Loss: 0.0314\n",
      "Epoch 34/100, Loss: 0.0726\n",
      "Epoch 35/100, Loss: 0.0413\n",
      "Epoch 36/100, Loss: 0.0354\n",
      "Epoch 37/100, Loss: 0.0675\n",
      "Epoch 38/100, Loss: 0.0615\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 39/100, Loss: 0.0338\n",
      "Epoch 40/100, Loss: 0.0908\n",
      "Epoch 41/100, Loss: 0.0441\n",
      "Epoch 42/100, Loss: 0.0529\n",
      "Epoch 43/100, Loss: 0.0284\n",
      "Epoch 44/100, Loss: 0.0284\n",
      "Epoch 45/100, Loss: 0.0544\n",
      "Epoch 46/100, Loss: 0.0419\n",
      "Epoch 47/100, Loss: 0.0320\n",
      "Epoch 48/100, Loss: 0.0283\n",
      "Epoch 49/100, Loss: 0.0308\n",
      "Epoch 50/100, Loss: 0.0860\n",
      "Epoch 51/100, Loss: 0.0321\n",
      "Epoch 52/100, Loss: 0.0336\n",
      "Epoch 53/100, Loss: 0.0283\n",
      "Epoch 54/100, Loss: 0.0289\n",
      "Epoch 55/100, Loss: 0.0447\n",
      "Epoch 56/100, Loss: 0.0726\n",
      "Epoch 57/100, Loss: 0.0280\n",
      "Epoch 58/100, Loss: 0.0440\n",
      "Epoch 59/100, Loss: 0.0265\n",
      "Epoch 60/100, Loss: 0.0441\n",
      "Epoch 61/100, Loss: 0.0639\n",
      "Epoch 62/100, Loss: 0.0785\n",
      "Epoch 63/100, Loss: 0.0278\n",
      "Epoch 64/100, Loss: 0.0242\n",
      "Epoch 65/100, Loss: 0.0343\n",
      "Epoch 66/100, Loss: 0.0393\n",
      "Epoch 67/100, Loss: 0.0379\n",
      "Epoch 68/100, Loss: 0.0351\n",
      "Epoch 69/100, Loss: 0.0369\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 70/100, Loss: 0.0329\n",
      "Epoch 71/100, Loss: 0.0286\n",
      "Epoch 72/100, Loss: 0.0447\n",
      "Epoch 73/100, Loss: 0.0367\n",
      "Epoch 74/100, Loss: 0.0341\n",
      "Epoch 75/100, Loss: 0.0366\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 76/100, Loss: 0.0696\n",
      "Epoch 77/100, Loss: 0.0405\n",
      "Epoch 78/100, Loss: 0.0334\n",
      "Epoch 79/100, Loss: 0.0397\n",
      "Epoch 80/100, Loss: 0.0347\n",
      "Epoch 81/100, Loss: 0.0210\n",
      "Epoch 82/100, Loss: 0.0239\n",
      "Epoch 83/100, Loss: 0.0248\n",
      "Epoch 84/100, Loss: 0.0254\n",
      "Epoch 85/100, Loss: 0.0289\n",
      "Epoch 86/100, Loss: 0.0213\n",
      "Epoch 87/100, Loss: 0.0480\n",
      "Epoch 88/100, Loss: 0.0299\n",
      "Epoch 89/100, Loss: 0.0265\n",
      "Epoch 90/100, Loss: 0.0309\n",
      "Epoch 91/100, Loss: 0.0353\n",
      "Epoch 92/100, Loss: 0.0308\n",
      "Epoch 93/100, Loss: 0.0285\n",
      "Epoch 94/100, Loss: 0.0508\n",
      "Epoch 95/100, Loss: 0.0308\n",
      "Epoch 96/100, Loss: 0.0294\n",
      "Epoch 97/100, Loss: 0.0845\n",
      "Epoch 98/100, Loss: 0.0381\n",
      "Epoch 99/100, Loss: 0.0332\n",
      "Epoch 100/100, Loss: 0.0342\n",
      "Training fold 9/10...\n",
      "Epoch 1/100, Loss: 0.3245\n",
      "Epoch 2/100, Loss: 0.1180\n",
      "Epoch 3/100, Loss: 0.2022\n",
      "Epoch 4/100, Loss: 0.2251\n",
      "Epoch 5/100, Loss: 0.1148\n",
      "Epoch 6/100, Loss: 0.1928\n",
      "Epoch 7/100, Loss: 0.1312\n",
      "Epoch 8/100, Loss: 0.0828\n",
      "Epoch 9/100, Loss: 0.1091\n",
      "Epoch 10/100, Loss: 0.1747\n",
      "Epoch 11/100, Loss: 0.0680\n",
      "Epoch 12/100, Loss: 0.2032\n",
      "Epoch 13/100, Loss: 0.1429\n",
      "Epoch 14/100, Loss: 0.0506\n",
      "Epoch 15/100, Loss: 0.0970\n",
      "Epoch 16/100, Loss: 0.0593\n",
      "Epoch 17/100, Loss: 0.0666\n",
      "Epoch 18/100, Loss: 0.1908\n",
      "Epoch 19/100, Loss: 0.0638\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 20/100, Loss: 0.1294\n",
      "Epoch 21/100, Loss: 0.0834\n",
      "Epoch 22/100, Loss: 0.1552\n",
      "Epoch 23/100, Loss: 0.0425\n",
      "Epoch 24/100, Loss: 0.0689\n",
      "Epoch 25/100, Loss: 0.0507\n",
      "Epoch 26/100, Loss: 0.0382\n",
      "Epoch 27/100, Loss: 0.0706\n",
      "Epoch 28/100, Loss: 0.0558\n",
      "Epoch 29/100, Loss: 0.1207\n",
      "Epoch 30/100, Loss: 0.0635\n",
      "Epoch 31/100, Loss: 0.1218\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 32/100, Loss: 0.0426\n",
      "Epoch 33/100, Loss: 0.1713\n",
      "Epoch 34/100, Loss: 0.0593\n",
      "Epoch 35/100, Loss: 0.0656\n",
      "Epoch 36/100, Loss: 0.0681\n",
      "Epoch 37/100, Loss: 0.0824\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 38/100, Loss: 0.0480\n",
      "Epoch 39/100, Loss: 0.0618\n",
      "Epoch 40/100, Loss: 0.1641\n",
      "Epoch 41/100, Loss: 0.0762\n",
      "Epoch 42/100, Loss: 0.0624\n",
      "Epoch 43/100, Loss: 0.1421\n",
      "Epoch 44/100, Loss: 0.0556\n",
      "Epoch 45/100, Loss: 0.0427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, Loss: 0.0433\n",
      "Epoch 47/100, Loss: 0.0364\n",
      "Epoch 48/100, Loss: 0.0920\n",
      "Epoch 49/100, Loss: 0.0825\n",
      "Epoch 50/100, Loss: 0.0678\n",
      "Epoch 51/100, Loss: 0.0631\n",
      "Epoch 52/100, Loss: 0.1308\n",
      "Epoch 53/100, Loss: 0.0721\n",
      "Epoch 54/100, Loss: 0.1303\n",
      "Epoch 55/100, Loss: 0.1653\n",
      "Epoch 56/100, Loss: 0.1309\n",
      "Epoch 57/100, Loss: 0.0501\n",
      "Epoch 58/100, Loss: 0.0412\n",
      "Epoch 59/100, Loss: 0.0562\n",
      "Epoch 60/100, Loss: 0.1481\n",
      "Epoch 61/100, Loss: 0.0493\n",
      "Epoch 62/100, Loss: 0.0483\n",
      "Epoch 63/100, Loss: 0.0565\n",
      "Epoch 64/100, Loss: 0.0663\n",
      "Epoch 65/100, Loss: 0.0807\n",
      "Epoch 66/100, Loss: 0.0737\n",
      "Epoch 67/100, Loss: 0.0821\n",
      "Epoch 68/100, Loss: 0.0645\n",
      "Epoch 69/100, Loss: 0.0472\n",
      "Epoch 70/100, Loss: 0.1049\n",
      "Epoch 71/100, Loss: 0.0455\n",
      "Epoch 72/100, Loss: 0.0698\n",
      "Epoch 73/100, Loss: 0.0489\n",
      "Epoch 74/100, Loss: 0.1043\n",
      "Epoch 75/100, Loss: 0.0585\n",
      "Epoch 76/100, Loss: 0.0996\n",
      "Epoch 77/100, Loss: 0.0719\n",
      "Epoch 78/100, Loss: 0.0703\n",
      "Epoch 79/100, Loss: 0.0471\n",
      "Epoch 80/100, Loss: 0.1009\n",
      "Epoch 81/100, Loss: 0.0652\n",
      "Epoch 82/100, Loss: 0.0779\n",
      "Epoch 83/100, Loss: 0.0736\n",
      "Epoch 84/100, Loss: 0.0611\n",
      "Epoch 85/100, Loss: 0.0496\n",
      "Epoch 86/100, Loss: 0.0502\n",
      "Epoch 87/100, Loss: 0.1292\n",
      "Epoch 88/100, Loss: 0.0468\n",
      "Epoch 89/100, Loss: 0.0478\n",
      "Epoch 90/100, Loss: 0.0574\n",
      "Epoch 91/100, Loss: 0.0393\n",
      "Epoch 92/100, Loss: 0.0617\n",
      "Epoch 93/100, Loss: 0.0454\n",
      "Epoch 94/100, Loss: 0.0691\n",
      "Epoch 95/100, Loss: 0.0581\n",
      "Epoch 96/100, Loss: 0.0741\n",
      "Epoch 97/100, Loss: 0.0412\n",
      "Epoch 98/100, Loss: 0.0504\n",
      "Epoch 99/100, Loss: 0.0643\n",
      "Epoch 100/100, Loss: 0.0464\n",
      "Training fold 10/10...\n",
      "Epoch 1/100, Loss: 0.1451\n",
      "Epoch 2/100, Loss: 0.2452\n",
      "Epoch 3/100, Loss: 0.1874\n",
      "Epoch 4/100, Loss: 0.1654\n",
      "Epoch 5/100, Loss: 0.0948\n",
      "Epoch 6/100, Loss: 0.1002\n",
      "Epoch 7/100, Loss: 0.0809\n",
      "Epoch 8/100, Loss: 0.0930\n",
      "Epoch 9/100, Loss: 0.1300\n",
      "Epoch 10/100, Loss: 0.1050\n",
      "Epoch 11/100, Loss: 0.0915\n",
      "Epoch 12/100, Loss: 0.1021\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13/100, Loss: 0.1362\n",
      "Epoch 14/100, Loss: 0.0764\n",
      "Epoch 15/100, Loss: 0.0850\n",
      "Epoch 16/100, Loss: 0.0810\n",
      "Epoch 17/100, Loss: 0.0651\n",
      "Epoch 18/100, Loss: 0.0653\n",
      "Epoch 19/100, Loss: 0.0856\n",
      "Epoch 20/100, Loss: 0.0839\n",
      "Epoch 21/100, Loss: 0.1226\n",
      "Epoch 22/100, Loss: 0.0503\n",
      "Epoch 23/100, Loss: 0.0523\n",
      "Epoch 24/100, Loss: 0.1274\n",
      "Epoch 25/100, Loss: 0.0745\n",
      "Epoch 26/100, Loss: 0.1417\n",
      "Epoch 27/100, Loss: 0.0919\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 28/100, Loss: 0.1504\n",
      "Epoch 29/100, Loss: 0.0895\n",
      "Epoch 30/100, Loss: 0.1372\n",
      "Epoch 31/100, Loss: 0.0511\n",
      "Epoch 32/100, Loss: 0.1011\n",
      "Epoch 33/100, Loss: 0.0612\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 34/100, Loss: 0.0541\n",
      "Epoch 35/100, Loss: 0.0826\n",
      "Epoch 36/100, Loss: 0.0713\n",
      "Epoch 37/100, Loss: 0.1350\n",
      "Epoch 38/100, Loss: 0.1509\n",
      "Epoch 39/100, Loss: 0.0792\n",
      "Epoch 40/100, Loss: 0.1240\n",
      "Epoch 41/100, Loss: 0.0969\n",
      "Epoch 42/100, Loss: 0.1235\n",
      "Epoch 43/100, Loss: 0.0903\n",
      "Epoch 44/100, Loss: 0.0651\n",
      "Epoch 45/100, Loss: 0.0586\n",
      "Epoch 46/100, Loss: 0.0609\n",
      "Epoch 47/100, Loss: 0.1309\n",
      "Epoch 48/100, Loss: 0.0988\n",
      "Epoch 49/100, Loss: 0.0759\n",
      "Epoch 50/100, Loss: 0.1308\n",
      "Epoch 51/100, Loss: 0.0834\n",
      "Epoch 52/100, Loss: 0.0675\n",
      "Epoch 53/100, Loss: 0.1410\n",
      "Epoch 54/100, Loss: 0.0934\n",
      "Epoch 55/100, Loss: 0.0600\n",
      "Epoch 56/100, Loss: 0.0777\n",
      "Epoch 57/100, Loss: 0.0846\n",
      "Epoch 58/100, Loss: 0.1153\n",
      "Epoch 59/100, Loss: 0.1093\n",
      "Epoch 60/100, Loss: 0.0772\n",
      "Epoch 61/100, Loss: 0.0676\n",
      "Epoch 62/100, Loss: 0.0940\n",
      "Epoch 63/100, Loss: 0.0661\n",
      "Epoch 64/100, Loss: 0.0540\n",
      "Epoch 65/100, Loss: 0.0456\n",
      "Epoch 66/100, Loss: 0.0704\n",
      "Epoch 67/100, Loss: 0.0685\n",
      "Epoch 68/100, Loss: 0.1179\n",
      "Epoch 69/100, Loss: 0.0848\n",
      "Epoch 70/100, Loss: 0.0527\n",
      "Epoch 71/100, Loss: 0.0916\n",
      "Epoch 72/100, Loss: 0.1042\n",
      "Epoch 73/100, Loss: 0.0740\n",
      "Epoch 74/100, Loss: 0.0775\n",
      "Epoch 75/100, Loss: 0.0528\n",
      "Epoch 76/100, Loss: 0.0593\n",
      "Epoch 77/100, Loss: 0.0527\n",
      "Epoch 78/100, Loss: 0.0816\n",
      "Epoch 79/100, Loss: 0.0823\n",
      "Epoch 80/100, Loss: 0.0969\n",
      "Epoch 81/100, Loss: 0.1246\n",
      "Epoch 82/100, Loss: 0.1625\n",
      "Epoch 83/100, Loss: 0.0591\n",
      "Epoch 84/100, Loss: 0.1464\n",
      "Epoch 85/100, Loss: 0.0603\n",
      "Epoch 86/100, Loss: 0.1490\n",
      "Epoch 87/100, Loss: 0.0380\n",
      "Epoch 88/100, Loss: 0.2074\n",
      "Epoch 89/100, Loss: 0.0869\n",
      "Epoch 90/100, Loss: 0.1273\n",
      "Epoch 91/100, Loss: 0.0470\n",
      "Epoch 92/100, Loss: 0.1283\n",
      "Epoch 93/100, Loss: 0.0691\n",
      "Epoch 94/100, Loss: 0.0514\n",
      "Epoch 95/100, Loss: 0.0717\n",
      "Epoch 96/100, Loss: 0.1224\n",
      "Epoch 97/100, Loss: 0.1024\n",
      "Epoch 98/100, Loss: 0.1702\n",
      "Epoch 99/100, Loss: 0.1255\n",
      "Epoch 100/100, Loss: 0.0633\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "class CustomLabelEncoder(LabelEncoder):\n",
    "    def fit(self, y):\n",
    "        super().fit(y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        known_classes = set(self.classes_)\n",
    "        y_transformed = []\n",
    "        new_label = len(self.classes_)\n",
    "        class_mapping = {label: idx for idx, label in enumerate(self.classes_)}\n",
    "\n",
    "        for label in y:\n",
    "            if label in known_classes:\n",
    "                y_transformed.append(class_mapping[label])\n",
    "            else:\n",
    "                y_transformed.append(new_label)\n",
    "                new_label += 1\n",
    "\n",
    "        return np.array(y_transformed)\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        self.fit(y)\n",
    "        return self.transform(y)\n",
    "    \n",
    "train = df_train_clof.drop(columns = ['sj_1','sj_5','sj_6','sj_7','sj_8','sj_9'])\n",
    "test = df_test_clof.drop(columns = ['sj_1','sj_5','sj_6','sj_7','sj_8','sj_9'])\n",
    "\n",
    "columns_to_keep = train.columns[train.columns != 'target']\n",
    "train = train[columns_to_keep.tolist() + ['target']]  # Keep 'target' in train\n",
    "test = test[columns_to_keep.tolist()]  # Exclude 'target' in test\n",
    "\n",
    "train['target'] = train['target'].map({'AbNormal': 1, 'Normal': 0}).astype(int)\n",
    "\n",
    "label_encoders = {}\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        le = CustomLabelEncoder()\n",
    "        train[col] = le.fit_transform(train[col].astype(str))\n",
    "        test[col] = le.transform(test[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "# Initialize an empty array to store predictions\n",
    "ensemble_predictions = np.zeros((test.shape[0],))\n",
    "\n",
    "# Set up Stratified K-Fold\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Improved Contrastive Loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss\n",
    "\n",
    "# Enhanced encoder network with residual connections for CLOF\n",
    "class EnhancedCLOFEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(EnhancedCLOFEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc_residual = nn.Linear(input_dim, output_dim)  # Residual connection\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.fc_residual(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x + residual  # Add the residual connection\n",
    "\n",
    "# Create a custom dataset for contrastive learning\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if label == 0:  # Inlier\n",
    "            idx2 = idx  # Choose the same index\n",
    "        else:  # Outlier\n",
    "            idx2 = torch.randint(0, len(self.data), (1,)).item()  # Choose a random different index\n",
    "\n",
    "        x2 = self.data[idx2]\n",
    "        label = torch.tensor(int(label != self.labels[idx2]), dtype=torch.float32)  # 1 if different, 0 if same\n",
    "\n",
    "        return x1, x2, label\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = train.drop(columns=['target']).shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 64\n",
    "batch_size = 128\n",
    "test_batch_size = 256  # Larger batch size for test data to process faster\n",
    "inlier_batch_size = 512  # Batch size for processing inliers\n",
    "learning_rate = 0.00001\n",
    "n_epochs = 100\n",
    "margin = 1.0\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train, train['target'])):\n",
    "    print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "\n",
    "    # Split the data\n",
    "    train_fold = train.iloc[train_idx]\n",
    "    val_fold = train.iloc[val_idx]\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(train_fold.drop(columns=['target']))\n",
    "    X_val = scaler.transform(val_fold.drop(columns=['target']))\n",
    "    X_test = scaler.transform(test)\n",
    "\n",
    "    y_train = train_fold['target'].values\n",
    "\n",
    "    # Convert to tensors\n",
    "    train_data = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    test_data = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = ContrastiveDataset(train_data, y_train)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = EnhancedCLOFEncoder(input_dim, hidden_dim, output_dim).to(device)\n",
    "    criterion = ContrastiveLoss(margin=margin).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x1, x2, label in train_loader:\n",
    "            x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output1 = model(x1)\n",
    "            output2 = model(x2)\n",
    "            loss = criterion(output1, output2, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step(total_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Generate predictions on the test set for the current fold\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False)\n",
    "        encoded_test_data_list = []\n",
    "\n",
    "        for x in test_loader:\n",
    "            x = x.to(device)\n",
    "            encoded_x = model(x)\n",
    "            encoded_test_data_list.append(encoded_x)\n",
    "\n",
    "        encoded_test_data = torch.cat(encoded_test_data_list)\n",
    "\n",
    "        # Process inliers in batches to manage memory usage\n",
    "        inlier_data = train_data[y_train == 0]  # Using only known inliers\n",
    "        inlier_loader = DataLoader(inlier_data, batch_size=inlier_batch_size, shuffle=False)\n",
    "        encoded_inliers_list = []\n",
    "\n",
    "        for inlier_batch in inlier_loader:\n",
    "            inlier_batch = inlier_batch.to(device)\n",
    "            encoded_inlier_batch = model(inlier_batch)\n",
    "            encoded_inliers_list.append(encoded_inlier_batch)\n",
    "\n",
    "        encoded_inliers = torch.cat(encoded_inliers_list)\n",
    "\n",
    "        # Compute distances in batches\n",
    "        min_distances = []\n",
    "\n",
    "        for test_vector in encoded_test_data:\n",
    "            test_vector = test_vector.unsqueeze(0)  # Add batch dimension\n",
    "            distances = F.pairwise_distance(test_vector, encoded_inliers)\n",
    "            min_distance = torch.min(distances).item()\n",
    "            min_distances.append(min_distance)\n",
    "\n",
    "        min_distances = torch.tensor(min_distances, device=device)\n",
    "\n",
    "        # Ensemble: Add predictions from the current fold to the ensemble\n",
    "        ensemble_predictions += min_distances.cpu().numpy()\n",
    "\n",
    "# Average the predictions from all folds\n",
    "ensemble_predictions /= n_splits\n",
    "\n",
    "# Improved thresholding for outlier detection using mean and standard deviation\n",
    "threshold = np.mean(ensemble_predictions) + np.std(ensemble_predictions)  # Threshold for detecting outliers\n",
    "outliers = ensemble_predictions > threshold\n",
    "\n",
    "submission_clof = pd.read_csv(\"submission.csv\")\n",
    "submission_clof['target'] = ensemble_predictions\n",
    "submission_clof.to_csv('submission_clof.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52beb1b6",
   "metadata": {},
   "source": [
    "# dbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0311e00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/30, Training Loss: 0.2521, Validation Loss: 0.1922\n",
      "Epoch 2/30, Training Loss: 0.2036, Validation Loss: 0.1869\n",
      "Epoch 3/30, Training Loss: 0.2013, Validation Loss: 0.1841\n",
      "Epoch 4/30, Training Loss: 0.1980, Validation Loss: 0.1843\n",
      "Epoch 5/30, Training Loss: 0.1971, Validation Loss: 0.1857\n",
      "Epoch 6/30, Training Loss: 0.1962, Validation Loss: 0.1839\n",
      "Epoch 7/30, Training Loss: 0.1946, Validation Loss: 0.1799\n",
      "Epoch 8/30, Training Loss: 0.1935, Validation Loss: 0.1804\n",
      "Epoch 9/30, Training Loss: 0.1918, Validation Loss: 0.1805\n",
      "Epoch 10/30, Training Loss: 0.1924, Validation Loss: 0.1790\n",
      "Epoch 11/30, Training Loss: 0.1908, Validation Loss: 0.1798\n",
      "Epoch 12/30, Training Loss: 0.1904, Validation Loss: 0.1804\n",
      "Epoch 13/30, Training Loss: 0.1890, Validation Loss: 0.1789\n",
      "Epoch 14/30, Training Loss: 0.1897, Validation Loss: 0.1802\n",
      "Epoch 15/30, Training Loss: 0.1895, Validation Loss: 0.1808\n",
      "Epoch 16/30, Training Loss: 0.1868, Validation Loss: 0.1784\n",
      "Epoch 17/30, Training Loss: 0.1877, Validation Loss: 0.1792\n",
      "Epoch 18/30, Training Loss: 0.1875, Validation Loss: 0.1793\n",
      "Epoch 19/30, Training Loss: 0.1873, Validation Loss: 0.1791\n",
      "Epoch 20/30, Training Loss: 0.1865, Validation Loss: 0.1777\n",
      "Epoch 21/30, Training Loss: 0.1874, Validation Loss: 0.1797\n",
      "Epoch 22/30, Training Loss: 0.1866, Validation Loss: 0.1766\n",
      "Epoch 23/30, Training Loss: 0.1855, Validation Loss: 0.1777\n",
      "Epoch 24/30, Training Loss: 0.1860, Validation Loss: 0.1784\n",
      "Epoch 25/30, Training Loss: 0.1855, Validation Loss: 0.1784\n",
      "Epoch 00026: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 26/30, Training Loss: 0.1855, Validation Loss: 0.1773\n",
      "Epoch 27/30, Training Loss: 0.1845, Validation Loss: 0.1767\n",
      "Early stopping triggered.\n",
      "Fold 2\n",
      "Epoch 1/30, Training Loss: 0.2358, Validation Loss: 0.2032\n",
      "Epoch 2/30, Training Loss: 0.2038, Validation Loss: 0.1962\n",
      "Epoch 3/30, Training Loss: 0.2001, Validation Loss: 0.1951\n",
      "Epoch 4/30, Training Loss: 0.1994, Validation Loss: 0.1970\n",
      "Epoch 5/30, Training Loss: 0.1955, Validation Loss: 0.1959\n",
      "Epoch 6/30, Training Loss: 0.1940, Validation Loss: 0.1908\n",
      "Epoch 7/30, Training Loss: 0.1926, Validation Loss: 0.1910\n",
      "Epoch 8/30, Training Loss: 0.1918, Validation Loss: 0.1907\n",
      "Epoch 9/30, Training Loss: 0.1914, Validation Loss: 0.1918\n",
      "Epoch 10/30, Training Loss: 0.1909, Validation Loss: 0.1917\n",
      "Epoch 11/30, Training Loss: 0.1903, Validation Loss: 0.1901\n",
      "Epoch 12/30, Training Loss: 0.1888, Validation Loss: 0.1883\n",
      "Epoch 13/30, Training Loss: 0.1893, Validation Loss: 0.1888\n",
      "Epoch 14/30, Training Loss: 0.1876, Validation Loss: 0.1869\n",
      "Epoch 15/30, Training Loss: 0.1872, Validation Loss: 0.1893\n",
      "Epoch 16/30, Training Loss: 0.1875, Validation Loss: 0.1884\n",
      "Epoch 17/30, Training Loss: 0.1867, Validation Loss: 0.1882\n",
      "Epoch 00018: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 18/30, Training Loss: 0.1864, Validation Loss: 0.1887\n",
      "Epoch 19/30, Training Loss: 0.1847, Validation Loss: 0.1875\n",
      "Early stopping triggered.\n",
      "Fold 3\n",
      "Epoch 1/30, Training Loss: 0.2397, Validation Loss: 0.1940\n",
      "Epoch 2/30, Training Loss: 0.2056, Validation Loss: 0.1894\n",
      "Epoch 3/30, Training Loss: 0.2008, Validation Loss: 0.1859\n",
      "Epoch 4/30, Training Loss: 0.1985, Validation Loss: 0.1845\n",
      "Epoch 5/30, Training Loss: 0.1966, Validation Loss: 0.1811\n",
      "Epoch 6/30, Training Loss: 0.1953, Validation Loss: 0.1825\n",
      "Epoch 7/30, Training Loss: 0.1935, Validation Loss: 0.1819\n",
      "Epoch 8/30, Training Loss: 0.1941, Validation Loss: 0.1812\n",
      "Epoch 9/30, Training Loss: 0.1927, Validation Loss: 0.1794\n",
      "Epoch 10/30, Training Loss: 0.1910, Validation Loss: 0.1813\n",
      "Epoch 11/30, Training Loss: 0.1910, Validation Loss: 0.1793\n",
      "Epoch 12/30, Training Loss: 0.1905, Validation Loss: 0.1784\n",
      "Epoch 13/30, Training Loss: 0.1889, Validation Loss: 0.1791\n",
      "Epoch 14/30, Training Loss: 0.1892, Validation Loss: 0.1791\n",
      "Epoch 15/30, Training Loss: 0.1893, Validation Loss: 0.1810\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 16/30, Training Loss: 0.1872, Validation Loss: 0.1789\n",
      "Epoch 17/30, Training Loss: 0.1877, Validation Loss: 0.1798\n",
      "Early stopping triggered.\n",
      "Fold 4\n",
      "Epoch 1/30, Training Loss: 0.2378, Validation Loss: 0.1952\n",
      "Epoch 2/30, Training Loss: 0.2060, Validation Loss: 0.1917\n",
      "Epoch 3/30, Training Loss: 0.2011, Validation Loss: 0.1895\n",
      "Epoch 4/30, Training Loss: 0.1983, Validation Loss: 0.1879\n",
      "Epoch 5/30, Training Loss: 0.1963, Validation Loss: 0.1893\n",
      "Epoch 6/30, Training Loss: 0.1948, Validation Loss: 0.1879\n",
      "Epoch 7/30, Training Loss: 0.1929, Validation Loss: 0.1849\n",
      "Epoch 8/30, Training Loss: 0.1929, Validation Loss: 0.1880\n",
      "Epoch 9/30, Training Loss: 0.1928, Validation Loss: 0.1859\n",
      "Epoch 10/30, Training Loss: 0.1906, Validation Loss: 0.1842\n",
      "Epoch 11/30, Training Loss: 0.1911, Validation Loss: 0.1843\n",
      "Epoch 12/30, Training Loss: 0.1906, Validation Loss: 0.1856\n",
      "Epoch 13/30, Training Loss: 0.1902, Validation Loss: 0.1851\n",
      "Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 14/30, Training Loss: 0.1893, Validation Loss: 0.1846\n",
      "Epoch 15/30, Training Loss: 0.1872, Validation Loss: 0.1820\n",
      "Epoch 16/30, Training Loss: 0.1864, Validation Loss: 0.1841\n",
      "Epoch 17/30, Training Loss: 0.1860, Validation Loss: 0.1830\n",
      "Epoch 18/30, Training Loss: 0.1865, Validation Loss: 0.1822\n",
      "Epoch 00019: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 19/30, Training Loss: 0.1856, Validation Loss: 0.1827\n",
      "Epoch 20/30, Training Loss: 0.1838, Validation Loss: 0.1828\n",
      "Early stopping triggered.\n",
      "Fold 5\n",
      "Epoch 1/30, Training Loss: 0.2272, Validation Loss: 0.2010\n",
      "Epoch 2/30, Training Loss: 0.2040, Validation Loss: 0.1992\n",
      "Epoch 3/30, Training Loss: 0.2002, Validation Loss: 0.1942\n",
      "Epoch 4/30, Training Loss: 0.1965, Validation Loss: 0.1931\n",
      "Epoch 5/30, Training Loss: 0.1951, Validation Loss: 0.1923\n",
      "Epoch 6/30, Training Loss: 0.1940, Validation Loss: 0.1921\n",
      "Epoch 7/30, Training Loss: 0.1933, Validation Loss: 0.1905\n",
      "Epoch 8/30, Training Loss: 0.1913, Validation Loss: 0.1904\n",
      "Epoch 9/30, Training Loss: 0.1905, Validation Loss: 0.1915\n",
      "Epoch 10/30, Training Loss: 0.1903, Validation Loss: 0.1891\n",
      "Epoch 11/30, Training Loss: 0.1893, Validation Loss: 0.1906\n",
      "Epoch 12/30, Training Loss: 0.1894, Validation Loss: 0.1885\n",
      "Epoch 13/30, Training Loss: 0.1879, Validation Loss: 0.1899\n",
      "Epoch 14/30, Training Loss: 0.1880, Validation Loss: 0.1869\n",
      "Epoch 15/30, Training Loss: 0.1876, Validation Loss: 0.1887\n",
      "Epoch 16/30, Training Loss: 0.1875, Validation Loss: 0.1893\n",
      "Epoch 17/30, Training Loss: 0.1871, Validation Loss: 0.1892\n",
      "Epoch 00018: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 18/30, Training Loss: 0.1869, Validation Loss: 0.1873\n",
      "Epoch 19/30, Training Loss: 0.1849, Validation Loss: 0.1882\n",
      "Early stopping triggered.\n",
      "Fold 6\n",
      "Epoch 1/30, Training Loss: 0.2308, Validation Loss: 0.2058\n",
      "Epoch 2/30, Training Loss: 0.2044, Validation Loss: 0.2023\n",
      "Epoch 3/30, Training Loss: 0.1999, Validation Loss: 0.1967\n",
      "Epoch 4/30, Training Loss: 0.1968, Validation Loss: 0.1979\n",
      "Epoch 5/30, Training Loss: 0.1958, Validation Loss: 0.1972\n",
      "Epoch 6/30, Training Loss: 0.1939, Validation Loss: 0.1958\n",
      "Epoch 7/30, Training Loss: 0.1922, Validation Loss: 0.1953\n",
      "Epoch 8/30, Training Loss: 0.1920, Validation Loss: 0.1965\n",
      "Epoch 9/30, Training Loss: 0.1901, Validation Loss: 0.1980\n",
      "Epoch 10/30, Training Loss: 0.1894, Validation Loss: 0.1944\n",
      "Epoch 11/30, Training Loss: 0.1898, Validation Loss: 0.1941\n",
      "Epoch 12/30, Training Loss: 0.1884, Validation Loss: 0.1947\n",
      "Epoch 13/30, Training Loss: 0.1885, Validation Loss: 0.1944\n",
      "Epoch 14/30, Training Loss: 0.1878, Validation Loss: 0.1969\n",
      "Epoch 00015: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 15/30, Training Loss: 0.1887, Validation Loss: 0.1963\n",
      "Epoch 16/30, Training Loss: 0.1854, Validation Loss: 0.1934\n",
      "Epoch 17/30, Training Loss: 0.1843, Validation Loss: 0.1954\n",
      "Epoch 18/30, Training Loss: 0.1836, Validation Loss: 0.1932\n",
      "Epoch 19/30, Training Loss: 0.1850, Validation Loss: 0.1965\n",
      "Epoch 20/30, Training Loss: 0.1831, Validation Loss: 0.1924\n",
      "Epoch 21/30, Training Loss: 0.1835, Validation Loss: 0.1948\n",
      "Epoch 22/30, Training Loss: 0.1831, Validation Loss: 0.1929\n",
      "Epoch 23/30, Training Loss: 0.1817, Validation Loss: 0.1943\n",
      "Epoch 00024: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 24/30, Training Loss: 0.1819, Validation Loss: 0.1955\n",
      "Epoch 25/30, Training Loss: 0.1815, Validation Loss: 0.1930\n",
      "Early stopping triggered.\n",
      "Fold 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 0.2398, Validation Loss: 0.1939\n",
      "Epoch 2/30, Training Loss: 0.2050, Validation Loss: 0.1925\n",
      "Epoch 3/30, Training Loss: 0.2006, Validation Loss: 0.1880\n",
      "Epoch 4/30, Training Loss: 0.1978, Validation Loss: 0.1900\n",
      "Epoch 5/30, Training Loss: 0.1970, Validation Loss: 0.1880\n",
      "Epoch 6/30, Training Loss: 0.1940, Validation Loss: 0.1881\n",
      "Epoch 7/30, Training Loss: 0.1930, Validation Loss: 0.1845\n",
      "Epoch 8/30, Training Loss: 0.1926, Validation Loss: 0.1843\n",
      "Epoch 9/30, Training Loss: 0.1914, Validation Loss: 0.1867\n",
      "Epoch 10/30, Training Loss: 0.1913, Validation Loss: 0.1846\n",
      "Epoch 11/30, Training Loss: 0.1897, Validation Loss: 0.1827\n",
      "Epoch 12/30, Training Loss: 0.1902, Validation Loss: 0.1861\n",
      "Epoch 13/30, Training Loss: 0.1891, Validation Loss: 0.1835\n",
      "Epoch 14/30, Training Loss: 0.1888, Validation Loss: 0.1819\n",
      "Epoch 15/30, Training Loss: 0.1888, Validation Loss: 0.1827\n",
      "Epoch 16/30, Training Loss: 0.1872, Validation Loss: 0.1841\n",
      "Epoch 17/30, Training Loss: 0.1875, Validation Loss: 0.1830\n",
      "Epoch 00018: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 18/30, Training Loss: 0.1867, Validation Loss: 0.1833\n",
      "Epoch 19/30, Training Loss: 0.1850, Validation Loss: 0.1810\n",
      "Epoch 20/30, Training Loss: 0.1841, Validation Loss: 0.1812\n",
      "Epoch 21/30, Training Loss: 0.1850, Validation Loss: 0.1814\n",
      "Epoch 22/30, Training Loss: 0.1842, Validation Loss: 0.1815\n",
      "Epoch 23/30, Training Loss: 0.1836, Validation Loss: 0.1804\n",
      "Epoch 24/30, Training Loss: 0.1836, Validation Loss: 0.1802\n",
      "Epoch 25/30, Training Loss: 0.1829, Validation Loss: 0.1804\n",
      "Epoch 26/30, Training Loss: 0.1828, Validation Loss: 0.1807\n",
      "Epoch 27/30, Training Loss: 0.1817, Validation Loss: 0.1799\n",
      "Epoch 28/30, Training Loss: 0.1822, Validation Loss: 0.1808\n",
      "Epoch 29/30, Training Loss: 0.1828, Validation Loss: 0.1795\n",
      "Epoch 30/30, Training Loss: 0.1813, Validation Loss: 0.1806\n",
      "Fold 8\n",
      "Epoch 1/30, Training Loss: 0.2331, Validation Loss: 0.2126\n",
      "Epoch 2/30, Training Loss: 0.2035, Validation Loss: 0.2108\n",
      "Epoch 3/30, Training Loss: 0.1991, Validation Loss: 0.2076\n",
      "Epoch 4/30, Training Loss: 0.1971, Validation Loss: 0.2073\n",
      "Epoch 5/30, Training Loss: 0.1945, Validation Loss: 0.2063\n",
      "Epoch 6/30, Training Loss: 0.1933, Validation Loss: 0.2035\n",
      "Epoch 7/30, Training Loss: 0.1911, Validation Loss: 0.2009\n",
      "Epoch 8/30, Training Loss: 0.1902, Validation Loss: 0.2020\n",
      "Epoch 9/30, Training Loss: 0.1897, Validation Loss: 0.2055\n",
      "Epoch 10/30, Training Loss: 0.1896, Validation Loss: 0.2009\n",
      "Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 11/30, Training Loss: 0.1900, Validation Loss: 0.2047\n",
      "Epoch 12/30, Training Loss: 0.1871, Validation Loss: 0.1975\n",
      "Epoch 13/30, Training Loss: 0.1862, Validation Loss: 0.1969\n",
      "Epoch 14/30, Training Loss: 0.1861, Validation Loss: 0.2005\n",
      "Epoch 15/30, Training Loss: 0.1847, Validation Loss: 0.1983\n",
      "Epoch 16/30, Training Loss: 0.1847, Validation Loss: 0.1969\n",
      "Epoch 17/30, Training Loss: 0.1851, Validation Loss: 0.1980\n",
      "Epoch 18/30, Training Loss: 0.1846, Validation Loss: 0.1969\n",
      "Epoch 19/30, Training Loss: 0.1841, Validation Loss: 0.1967\n",
      "Epoch 20/30, Training Loss: 0.1840, Validation Loss: 0.1993\n",
      "Epoch 21/30, Training Loss: 0.1834, Validation Loss: 0.1968\n",
      "Epoch 22/30, Training Loss: 0.1839, Validation Loss: 0.1992\n",
      "Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 23/30, Training Loss: 0.1833, Validation Loss: 0.1977\n",
      "Epoch 24/30, Training Loss: 0.1815, Validation Loss: 0.1959\n",
      "Epoch 25/30, Training Loss: 0.1807, Validation Loss: 0.1978\n",
      "Epoch 26/30, Training Loss: 0.1809, Validation Loss: 0.1956\n",
      "Epoch 27/30, Training Loss: 0.1803, Validation Loss: 0.1964\n",
      "Epoch 28/30, Training Loss: 0.1804, Validation Loss: 0.1957\n",
      "Epoch 29/30, Training Loss: 0.1812, Validation Loss: 0.1957\n",
      "Epoch 30/30, Training Loss: 0.1798, Validation Loss: 0.1955\n",
      "Fold 9\n",
      "Epoch 1/30, Training Loss: 0.2406, Validation Loss: 0.2033\n",
      "Epoch 2/30, Training Loss: 0.2038, Validation Loss: 0.1970\n",
      "Epoch 3/30, Training Loss: 0.2004, Validation Loss: 0.1940\n",
      "Epoch 4/30, Training Loss: 0.1979, Validation Loss: 0.1904\n",
      "Epoch 5/30, Training Loss: 0.1944, Validation Loss: 0.1915\n",
      "Epoch 6/30, Training Loss: 0.1939, Validation Loss: 0.1914\n",
      "Epoch 7/30, Training Loss: 0.1926, Validation Loss: 0.1927\n",
      "Epoch 8/30, Training Loss: 0.1912, Validation Loss: 0.1887\n",
      "Epoch 9/30, Training Loss: 0.1908, Validation Loss: 0.1925\n",
      "Epoch 10/30, Training Loss: 0.1904, Validation Loss: 0.1900\n",
      "Epoch 11/30, Training Loss: 0.1896, Validation Loss: 0.1933\n",
      "Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 12/30, Training Loss: 0.1893, Validation Loss: 0.1899\n",
      "Epoch 13/30, Training Loss: 0.1869, Validation Loss: 0.1894\n",
      "Early stopping triggered.\n",
      "Fold 10\n",
      "Epoch 1/30, Training Loss: 0.2392, Validation Loss: 0.1904\n",
      "Epoch 2/30, Training Loss: 0.2048, Validation Loss: 0.1900\n",
      "Epoch 3/30, Training Loss: 0.2007, Validation Loss: 0.1840\n",
      "Epoch 4/30, Training Loss: 0.1988, Validation Loss: 0.1820\n",
      "Epoch 5/30, Training Loss: 0.1965, Validation Loss: 0.1824\n",
      "Epoch 6/30, Training Loss: 0.1952, Validation Loss: 0.1814\n",
      "Epoch 7/30, Training Loss: 0.1933, Validation Loss: 0.1821\n",
      "Epoch 8/30, Training Loss: 0.1927, Validation Loss: 0.1808\n",
      "Epoch 9/30, Training Loss: 0.1923, Validation Loss: 0.1815\n",
      "Epoch 10/30, Training Loss: 0.1916, Validation Loss: 0.1823\n",
      "Epoch 11/30, Training Loss: 0.1907, Validation Loss: 0.1818\n",
      "Epoch 12/30, Training Loss: 0.1896, Validation Loss: 0.1803\n",
      "Epoch 13/30, Training Loss: 0.1894, Validation Loss: 0.1805\n",
      "Epoch 14/30, Training Loss: 0.1890, Validation Loss: 0.1794\n",
      "Epoch 15/30, Training Loss: 0.1891, Validation Loss: 0.1791\n",
      "Epoch 16/30, Training Loss: 0.1890, Validation Loss: 0.1812\n",
      "Epoch 17/30, Training Loss: 0.1881, Validation Loss: 0.1797\n",
      "Epoch 18/30, Training Loss: 0.1865, Validation Loss: 0.1798\n",
      "Epoch 19/30, Training Loss: 0.1873, Validation Loss: 0.1790\n",
      "Epoch 20/30, Training Loss: 0.1865, Validation Loss: 0.1787\n",
      "Epoch 21/30, Training Loss: 0.1861, Validation Loss: 0.1794\n",
      "Epoch 22/30, Training Loss: 0.1870, Validation Loss: 0.1788\n",
      "Epoch 23/30, Training Loss: 0.1865, Validation Loss: 0.1789\n",
      "Epoch 24/30, Training Loss: 0.1857, Validation Loss: 0.1779\n",
      "Epoch 25/30, Training Loss: 0.1861, Validation Loss: 0.1771\n",
      "Epoch 26/30, Training Loss: 0.1855, Validation Loss: 0.1791\n",
      "Epoch 27/30, Training Loss: 0.1851, Validation Loss: 0.1766\n",
      "Epoch 28/30, Training Loss: 0.1857, Validation Loss: 0.1794\n",
      "Epoch 29/30, Training Loss: 0.1844, Validation Loss: 0.1778\n",
      "Epoch 30/30, Training Loss: 0.1846, Validation Loss: 0.1780\n",
      "Fold 1\n",
      "Epoch 1/30, Training Loss: 0.2395, Validation Loss: 0.1924\n",
      "Epoch 2/30, Training Loss: 0.2055, Validation Loss: 0.1888\n",
      "Epoch 3/30, Training Loss: 0.2026, Validation Loss: 0.1906\n",
      "Epoch 4/30, Training Loss: 0.2003, Validation Loss: 0.1839\n",
      "Epoch 5/30, Training Loss: 0.1978, Validation Loss: 0.1817\n",
      "Epoch 6/30, Training Loss: 0.1950, Validation Loss: 0.1812\n",
      "Epoch 7/30, Training Loss: 0.1952, Validation Loss: 0.1817\n",
      "Epoch 8/30, Training Loss: 0.1938, Validation Loss: 0.1825\n",
      "Epoch 9/30, Training Loss: 0.1924, Validation Loss: 0.1792\n",
      "Epoch 10/30, Training Loss: 0.1921, Validation Loss: 0.1817\n",
      "Epoch 11/30, Training Loss: 0.1899, Validation Loss: 0.1811\n",
      "Epoch 12/30, Training Loss: 0.1902, Validation Loss: 0.1787\n",
      "Epoch 13/30, Training Loss: 0.1901, Validation Loss: 0.1790\n",
      "Epoch 14/30, Training Loss: 0.1888, Validation Loss: 0.1767\n",
      "Epoch 15/30, Training Loss: 0.1893, Validation Loss: 0.1781\n",
      "Epoch 16/30, Training Loss: 0.1890, Validation Loss: 0.1787\n",
      "Epoch 17/30, Training Loss: 0.1876, Validation Loss: 0.1788\n",
      "Epoch 00018: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 18/30, Training Loss: 0.1880, Validation Loss: 0.1806\n",
      "Epoch 19/30, Training Loss: 0.1863, Validation Loss: 0.1793\n",
      "Early stopping triggered.\n",
      "Fold 2\n",
      "Epoch 1/30, Training Loss: 0.2369, Validation Loss: 0.2028\n",
      "Epoch 2/30, Training Loss: 0.2034, Validation Loss: 0.1980\n",
      "Epoch 3/30, Training Loss: 0.1994, Validation Loss: 0.1940\n",
      "Epoch 4/30, Training Loss: 0.1980, Validation Loss: 0.1963\n",
      "Epoch 5/30, Training Loss: 0.1962, Validation Loss: 0.1935\n",
      "Epoch 6/30, Training Loss: 0.1933, Validation Loss: 0.1902\n",
      "Epoch 7/30, Training Loss: 0.1926, Validation Loss: 0.1896\n",
      "Epoch 8/30, Training Loss: 0.1914, Validation Loss: 0.1946\n",
      "Epoch 9/30, Training Loss: 0.1907, Validation Loss: 0.1899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Training Loss: 0.1901, Validation Loss: 0.1895\n",
      "Epoch 11/30, Training Loss: 0.1898, Validation Loss: 0.1878\n",
      "Epoch 12/30, Training Loss: 0.1899, Validation Loss: 0.1887\n",
      "Epoch 13/30, Training Loss: 0.1891, Validation Loss: 0.1919\n",
      "Epoch 14/30, Training Loss: 0.1884, Validation Loss: 0.1886\n",
      "Epoch 15/30, Training Loss: 0.1875, Validation Loss: 0.1871\n",
      "Epoch 16/30, Training Loss: 0.1877, Validation Loss: 0.1872\n",
      "Epoch 17/30, Training Loss: 0.1865, Validation Loss: 0.1890\n",
      "Epoch 18/30, Training Loss: 0.1867, Validation Loss: 0.1878\n",
      "Epoch 19/30, Training Loss: 0.1873, Validation Loss: 0.1866\n",
      "Epoch 20/30, Training Loss: 0.1864, Validation Loss: 0.1883\n",
      "Epoch 21/30, Training Loss: 0.1854, Validation Loss: 0.1884\n",
      "Epoch 22/30, Training Loss: 0.1851, Validation Loss: 0.1886\n",
      "Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 23/30, Training Loss: 0.1853, Validation Loss: 0.1906\n",
      "Epoch 24/30, Training Loss: 0.1824, Validation Loss: 0.1889\n",
      "Early stopping triggered.\n",
      "Fold 3\n",
      "Epoch 1/30, Training Loss: 0.2301, Validation Loss: 0.1986\n",
      "Epoch 2/30, Training Loss: 0.2053, Validation Loss: 0.1859\n",
      "Epoch 3/30, Training Loss: 0.2005, Validation Loss: 0.1879\n",
      "Epoch 4/30, Training Loss: 0.1976, Validation Loss: 0.1852\n",
      "Epoch 5/30, Training Loss: 0.1966, Validation Loss: 0.1820\n",
      "Epoch 6/30, Training Loss: 0.1955, Validation Loss: 0.1806\n",
      "Epoch 7/30, Training Loss: 0.1936, Validation Loss: 0.1833\n",
      "Epoch 8/30, Training Loss: 0.1936, Validation Loss: 0.1800\n",
      "Epoch 9/30, Training Loss: 0.1927, Validation Loss: 0.1822\n",
      "Epoch 10/30, Training Loss: 0.1915, Validation Loss: 0.1799\n",
      "Epoch 11/30, Training Loss: 0.1907, Validation Loss: 0.1823\n",
      "Epoch 12/30, Training Loss: 0.1896, Validation Loss: 0.1798\n",
      "Epoch 13/30, Training Loss: 0.1900, Validation Loss: 0.1791\n",
      "Epoch 14/30, Training Loss: 0.1885, Validation Loss: 0.1858\n",
      "Epoch 15/30, Training Loss: 0.1881, Validation Loss: 0.1800\n",
      "Epoch 16/30, Training Loss: 0.1888, Validation Loss: 0.1791\n",
      "Epoch 17/30, Training Loss: 0.1881, Validation Loss: 0.1779\n",
      "Epoch 18/30, Training Loss: 0.1876, Validation Loss: 0.1777\n",
      "Epoch 19/30, Training Loss: 0.1872, Validation Loss: 0.1777\n",
      "Epoch 20/30, Training Loss: 0.1873, Validation Loss: 0.1796\n",
      "Epoch 21/30, Training Loss: 0.1864, Validation Loss: 0.1779\n",
      "Epoch 22/30, Training Loss: 0.1867, Validation Loss: 0.1783\n",
      "Epoch 23/30, Training Loss: 0.1862, Validation Loss: 0.1772\n",
      "Epoch 24/30, Training Loss: 0.1860, Validation Loss: 0.1789\n",
      "Epoch 25/30, Training Loss: 0.1851, Validation Loss: 0.1781\n",
      "Epoch 26/30, Training Loss: 0.1860, Validation Loss: 0.1776\n",
      "Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 27/30, Training Loss: 0.1860, Validation Loss: 0.1777\n",
      "Epoch 28/30, Training Loss: 0.1836, Validation Loss: 0.1774\n",
      "Early stopping triggered.\n",
      "Fold 4\n",
      "Epoch 1/30, Training Loss: 0.2339, Validation Loss: 0.1951\n",
      "Epoch 2/30, Training Loss: 0.2059, Validation Loss: 0.1941\n",
      "Epoch 3/30, Training Loss: 0.2011, Validation Loss: 0.1886\n",
      "Epoch 4/30, Training Loss: 0.1986, Validation Loss: 0.1870\n",
      "Epoch 5/30, Training Loss: 0.1969, Validation Loss: 0.1884\n",
      "Epoch 6/30, Training Loss: 0.1949, Validation Loss: 0.1873\n",
      "Epoch 7/30, Training Loss: 0.1935, Validation Loss: 0.1866\n",
      "Epoch 8/30, Training Loss: 0.1933, Validation Loss: 0.1839\n",
      "Epoch 9/30, Training Loss: 0.1913, Validation Loss: 0.1843\n",
      "Epoch 10/30, Training Loss: 0.1915, Validation Loss: 0.1827\n",
      "Epoch 11/30, Training Loss: 0.1902, Validation Loss: 0.1843\n",
      "Epoch 12/30, Training Loss: 0.1908, Validation Loss: 0.1823\n",
      "Epoch 13/30, Training Loss: 0.1891, Validation Loss: 0.1832\n",
      "Epoch 14/30, Training Loss: 0.1886, Validation Loss: 0.1857\n",
      "Epoch 15/30, Training Loss: 0.1880, Validation Loss: 0.1841\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 16/30, Training Loss: 0.1876, Validation Loss: 0.1870\n",
      "Epoch 17/30, Training Loss: 0.1851, Validation Loss: 0.1828\n",
      "Early stopping triggered.\n",
      "Fold 5\n",
      "Epoch 1/30, Training Loss: 0.2304, Validation Loss: 0.2076\n",
      "Epoch 2/30, Training Loss: 0.2029, Validation Loss: 0.2016\n",
      "Epoch 3/30, Training Loss: 0.1996, Validation Loss: 0.1918\n",
      "Epoch 4/30, Training Loss: 0.1968, Validation Loss: 0.1969\n",
      "Epoch 5/30, Training Loss: 0.1955, Validation Loss: 0.1929\n",
      "Epoch 6/30, Training Loss: 0.1938, Validation Loss: 0.1896\n",
      "Epoch 7/30, Training Loss: 0.1930, Validation Loss: 0.1919\n",
      "Epoch 8/30, Training Loss: 0.1914, Validation Loss: 0.1877\n",
      "Epoch 9/30, Training Loss: 0.1904, Validation Loss: 0.1890\n",
      "Epoch 10/30, Training Loss: 0.1904, Validation Loss: 0.1890\n",
      "Epoch 11/30, Training Loss: 0.1903, Validation Loss: 0.1902\n",
      "Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 12/30, Training Loss: 0.1895, Validation Loss: 0.1906\n",
      "Epoch 13/30, Training Loss: 0.1866, Validation Loss: 0.1889\n",
      "Early stopping triggered.\n",
      "Fold 6\n",
      "Epoch 1/30, Training Loss: 0.2354, Validation Loss: 0.2039\n",
      "Epoch 2/30, Training Loss: 0.2035, Validation Loss: 0.2016\n",
      "Epoch 3/30, Training Loss: 0.2015, Validation Loss: 0.1988\n",
      "Epoch 4/30, Training Loss: 0.1966, Validation Loss: 0.1994\n",
      "Epoch 5/30, Training Loss: 0.1942, Validation Loss: 0.1993\n",
      "Epoch 6/30, Training Loss: 0.1941, Validation Loss: 0.1968\n",
      "Epoch 7/30, Training Loss: 0.1917, Validation Loss: 0.1967\n",
      "Epoch 8/30, Training Loss: 0.1918, Validation Loss: 0.1954\n",
      "Epoch 9/30, Training Loss: 0.1899, Validation Loss: 0.1983\n",
      "Epoch 10/30, Training Loss: 0.1890, Validation Loss: 0.1966\n",
      "Epoch 11/30, Training Loss: 0.1899, Validation Loss: 0.1929\n",
      "Epoch 12/30, Training Loss: 0.1886, Validation Loss: 0.1947\n",
      "Epoch 13/30, Training Loss: 0.1875, Validation Loss: 0.1926\n",
      "Epoch 14/30, Training Loss: 0.1883, Validation Loss: 0.1981\n",
      "Epoch 15/30, Training Loss: 0.1877, Validation Loss: 0.1954\n",
      "Epoch 16/30, Training Loss: 0.1868, Validation Loss: 0.1946\n",
      "Epoch 00017: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 17/30, Training Loss: 0.1855, Validation Loss: 0.1949\n",
      "Epoch 18/30, Training Loss: 0.1851, Validation Loss: 0.1943\n",
      "Early stopping triggered.\n",
      "Fold 7\n",
      "Epoch 1/30, Training Loss: 0.2318, Validation Loss: 0.1986\n",
      "Epoch 2/30, Training Loss: 0.2058, Validation Loss: 0.1903\n",
      "Epoch 3/30, Training Loss: 0.2012, Validation Loss: 0.1930\n",
      "Epoch 4/30, Training Loss: 0.1978, Validation Loss: 0.1854\n",
      "Epoch 5/30, Training Loss: 0.1958, Validation Loss: 0.1914\n",
      "Epoch 6/30, Training Loss: 0.1954, Validation Loss: 0.1843\n",
      "Epoch 7/30, Training Loss: 0.1936, Validation Loss: 0.1866\n",
      "Epoch 8/30, Training Loss: 0.1929, Validation Loss: 0.1846\n",
      "Epoch 9/30, Training Loss: 0.1927, Validation Loss: 0.1850\n",
      "Epoch 10/30, Training Loss: 0.1904, Validation Loss: 0.1835\n",
      "Epoch 11/30, Training Loss: 0.1900, Validation Loss: 0.1836\n",
      "Epoch 12/30, Training Loss: 0.1904, Validation Loss: 0.1839\n",
      "Epoch 13/30, Training Loss: 0.1896, Validation Loss: 0.1833\n",
      "Epoch 14/30, Training Loss: 0.1887, Validation Loss: 0.1835\n",
      "Epoch 15/30, Training Loss: 0.1891, Validation Loss: 0.1831\n",
      "Epoch 16/30, Training Loss: 0.1879, Validation Loss: 0.1832\n",
      "Epoch 17/30, Training Loss: 0.1876, Validation Loss: 0.1851\n",
      "Epoch 18/30, Training Loss: 0.1880, Validation Loss: 0.1832\n",
      "Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 19/30, Training Loss: 0.1870, Validation Loss: 0.1852\n",
      "Epoch 20/30, Training Loss: 0.1849, Validation Loss: 0.1811\n",
      "Epoch 21/30, Training Loss: 0.1854, Validation Loss: 0.1822\n",
      "Epoch 22/30, Training Loss: 0.1836, Validation Loss: 0.1812\n",
      "Epoch 23/30, Training Loss: 0.1841, Validation Loss: 0.1811\n",
      "Epoch 00024: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 24/30, Training Loss: 0.1832, Validation Loss: 0.1819\n",
      "Epoch 25/30, Training Loss: 0.1826, Validation Loss: 0.1804\n",
      "Epoch 26/30, Training Loss: 0.1816, Validation Loss: 0.1816\n",
      "Epoch 27/30, Training Loss: 0.1813, Validation Loss: 0.1805\n",
      "Epoch 28/30, Training Loss: 0.1812, Validation Loss: 0.1801\n",
      "Epoch 29/30, Training Loss: 0.1812, Validation Loss: 0.1807\n",
      "Epoch 30/30, Training Loss: 0.1812, Validation Loss: 0.1808\n",
      "Fold 8\n",
      "Epoch 1/30, Training Loss: 0.2500, Validation Loss: 0.2159\n",
      "Epoch 2/30, Training Loss: 0.2029, Validation Loss: 0.2074\n",
      "Epoch 3/30, Training Loss: 0.2001, Validation Loss: 0.2077\n",
      "Epoch 4/30, Training Loss: 0.1984, Validation Loss: 0.2051\n",
      "Epoch 5/30, Training Loss: 0.1942, Validation Loss: 0.2041\n",
      "Epoch 6/30, Training Loss: 0.1933, Validation Loss: 0.2058\n",
      "Epoch 7/30, Training Loss: 0.1910, Validation Loss: 0.2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Training Loss: 0.1916, Validation Loss: 0.2025\n",
      "Epoch 9/30, Training Loss: 0.1896, Validation Loss: 0.2018\n",
      "Epoch 10/30, Training Loss: 0.1896, Validation Loss: 0.2018\n",
      "Epoch 11/30, Training Loss: 0.1886, Validation Loss: 0.1997\n",
      "Epoch 12/30, Training Loss: 0.1893, Validation Loss: 0.1988\n",
      "Epoch 13/30, Training Loss: 0.1877, Validation Loss: 0.2027\n",
      "Epoch 14/30, Training Loss: 0.1865, Validation Loss: 0.2011\n",
      "Epoch 15/30, Training Loss: 0.1867, Validation Loss: 0.2000\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 16/30, Training Loss: 0.1860, Validation Loss: 0.1994\n",
      "Epoch 17/30, Training Loss: 0.1849, Validation Loss: 0.1970\n",
      "Epoch 18/30, Training Loss: 0.1839, Validation Loss: 0.1959\n",
      "Epoch 19/30, Training Loss: 0.1828, Validation Loss: 0.1961\n",
      "Epoch 20/30, Training Loss: 0.1832, Validation Loss: 0.1981\n",
      "Epoch 21/30, Training Loss: 0.1823, Validation Loss: 0.1967\n",
      "Epoch 00022: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 22/30, Training Loss: 0.1825, Validation Loss: 0.1981\n",
      "Epoch 23/30, Training Loss: 0.1813, Validation Loss: 0.1957\n",
      "Epoch 24/30, Training Loss: 0.1808, Validation Loss: 0.1965\n",
      "Epoch 25/30, Training Loss: 0.1799, Validation Loss: 0.1974\n",
      "Epoch 26/30, Training Loss: 0.1799, Validation Loss: 0.1964\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 27/30, Training Loss: 0.1812, Validation Loss: 0.1982\n",
      "Epoch 28/30, Training Loss: 0.1792, Validation Loss: 0.1983\n",
      "Early stopping triggered.\n",
      "Fold 9\n",
      "Epoch 1/30, Training Loss: 0.2429, Validation Loss: 0.1978\n",
      "Epoch 2/30, Training Loss: 0.2043, Validation Loss: 0.1940\n",
      "Epoch 3/30, Training Loss: 0.2009, Validation Loss: 0.1958\n",
      "Epoch 4/30, Training Loss: 0.1973, Validation Loss: 0.1948\n",
      "Epoch 5/30, Training Loss: 0.1955, Validation Loss: 0.1961\n",
      "Epoch 6/30, Training Loss: 0.1937, Validation Loss: 0.1927\n",
      "Epoch 7/30, Training Loss: 0.1936, Validation Loss: 0.1911\n",
      "Epoch 8/30, Training Loss: 0.1919, Validation Loss: 0.1916\n",
      "Epoch 9/30, Training Loss: 0.1912, Validation Loss: 0.1897\n",
      "Epoch 10/30, Training Loss: 0.1913, Validation Loss: 0.1902\n",
      "Epoch 11/30, Training Loss: 0.1901, Validation Loss: 0.1918\n",
      "Epoch 12/30, Training Loss: 0.1901, Validation Loss: 0.1919\n",
      "Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 13/30, Training Loss: 0.1891, Validation Loss: 0.1915\n",
      "Epoch 14/30, Training Loss: 0.1858, Validation Loss: 0.1911\n",
      "Early stopping triggered.\n",
      "Fold 10\n",
      "Epoch 1/30, Training Loss: 0.2358, Validation Loss: 0.1939\n",
      "Epoch 2/30, Training Loss: 0.2045, Validation Loss: 0.1876\n",
      "Epoch 3/30, Training Loss: 0.2023, Validation Loss: 0.1845\n",
      "Epoch 4/30, Training Loss: 0.1971, Validation Loss: 0.1861\n",
      "Epoch 5/30, Training Loss: 0.1956, Validation Loss: 0.1822\n",
      "Epoch 6/30, Training Loss: 0.1938, Validation Loss: 0.1808\n",
      "Epoch 7/30, Training Loss: 0.1929, Validation Loss: 0.1809\n",
      "Epoch 8/30, Training Loss: 0.1921, Validation Loss: 0.1808\n",
      "Epoch 9/30, Training Loss: 0.1912, Validation Loss: 0.1812\n",
      "Epoch 10/30, Training Loss: 0.1914, Validation Loss: 0.1801\n",
      "Epoch 11/30, Training Loss: 0.1903, Validation Loss: 0.1808\n",
      "Epoch 12/30, Training Loss: 0.1901, Validation Loss: 0.1790\n",
      "Epoch 13/30, Training Loss: 0.1905, Validation Loss: 0.1803\n",
      "Epoch 14/30, Training Loss: 0.1882, Validation Loss: 0.1793\n",
      "Epoch 15/30, Training Loss: 0.1875, Validation Loss: 0.1812\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 16/30, Training Loss: 0.1876, Validation Loss: 0.1800\n",
      "Epoch 17/30, Training Loss: 0.1862, Validation Loss: 0.1795\n",
      "Early stopping triggered.\n",
      "Fold 1\n",
      "Epoch 1/30, Training Loss: 0.2450, Validation Loss: 0.1885\n",
      "Epoch 2/30, Training Loss: 0.2044, Validation Loss: 0.1842\n",
      "Epoch 3/30, Training Loss: 0.2018, Validation Loss: 0.1873\n",
      "Epoch 4/30, Training Loss: 0.1985, Validation Loss: 0.1821\n",
      "Epoch 5/30, Training Loss: 0.1971, Validation Loss: 0.1813\n",
      "Epoch 6/30, Training Loss: 0.1937, Validation Loss: 0.1813\n",
      "Epoch 7/30, Training Loss: 0.1937, Validation Loss: 0.1840\n",
      "Epoch 8/30, Training Loss: 0.1935, Validation Loss: 0.1824\n",
      "Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 9/30, Training Loss: 0.1919, Validation Loss: 0.1836\n",
      "Epoch 10/30, Training Loss: 0.1889, Validation Loss: 0.1776\n",
      "Epoch 11/30, Training Loss: 0.1880, Validation Loss: 0.1798\n",
      "Epoch 12/30, Training Loss: 0.1880, Validation Loss: 0.1795\n",
      "Epoch 13/30, Training Loss: 0.1897, Validation Loss: 0.1775\n",
      "Epoch 14/30, Training Loss: 0.1873, Validation Loss: 0.1789\n",
      "Epoch 15/30, Training Loss: 0.1867, Validation Loss: 0.1798\n",
      "Epoch 16/30, Training Loss: 0.1866, Validation Loss: 0.1782\n",
      "Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 17/30, Training Loss: 0.1859, Validation Loss: 0.1787\n",
      "Epoch 18/30, Training Loss: 0.1842, Validation Loss: 0.1782\n",
      "Early stopping triggered.\n",
      "Fold 2\n",
      "Epoch 1/30, Training Loss: 0.2359, Validation Loss: 0.2003\n",
      "Epoch 2/30, Training Loss: 0.2038, Validation Loss: 0.1970\n",
      "Epoch 3/30, Training Loss: 0.2007, Validation Loss: 0.1926\n",
      "Epoch 4/30, Training Loss: 0.1968, Validation Loss: 0.1927\n",
      "Epoch 5/30, Training Loss: 0.1964, Validation Loss: 0.1973\n",
      "Epoch 6/30, Training Loss: 0.1947, Validation Loss: 0.1945\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 7/30, Training Loss: 0.1921, Validation Loss: 0.1953\n",
      "Epoch 8/30, Training Loss: 0.1911, Validation Loss: 0.1911\n",
      "Epoch 9/30, Training Loss: 0.1898, Validation Loss: 0.1897\n",
      "Epoch 10/30, Training Loss: 0.1899, Validation Loss: 0.1896\n",
      "Epoch 11/30, Training Loss: 0.1889, Validation Loss: 0.1883\n",
      "Epoch 12/30, Training Loss: 0.1892, Validation Loss: 0.1876\n",
      "Epoch 13/30, Training Loss: 0.1878, Validation Loss: 0.1888\n",
      "Epoch 14/30, Training Loss: 0.1873, Validation Loss: 0.1908\n",
      "Epoch 15/30, Training Loss: 0.1869, Validation Loss: 0.1897\n",
      "Epoch 16/30, Training Loss: 0.1868, Validation Loss: 0.1874\n",
      "Epoch 17/30, Training Loss: 0.1868, Validation Loss: 0.1894\n",
      "Epoch 18/30, Training Loss: 0.1860, Validation Loss: 0.1868\n",
      "Epoch 19/30, Training Loss: 0.1853, Validation Loss: 0.1869\n",
      "Epoch 20/30, Training Loss: 0.1856, Validation Loss: 0.1891\n",
      "Epoch 21/30, Training Loss: 0.1839, Validation Loss: 0.1883\n",
      "Epoch 00022: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 22/30, Training Loss: 0.1847, Validation Loss: 0.1869\n",
      "Epoch 23/30, Training Loss: 0.1826, Validation Loss: 0.1872\n",
      "Early stopping triggered.\n",
      "Fold 3\n",
      "Epoch 1/30, Training Loss: 0.2448, Validation Loss: 0.1888\n",
      "Epoch 2/30, Training Loss: 0.2033, Validation Loss: 0.1848\n",
      "Epoch 3/30, Training Loss: 0.2010, Validation Loss: 0.1851\n",
      "Epoch 4/30, Training Loss: 0.1982, Validation Loss: 0.1840\n",
      "Epoch 5/30, Training Loss: 0.1960, Validation Loss: 0.1834\n",
      "Epoch 6/30, Training Loss: 0.1945, Validation Loss: 0.1815\n",
      "Epoch 7/30, Training Loss: 0.1938, Validation Loss: 0.1801\n",
      "Epoch 8/30, Training Loss: 0.1931, Validation Loss: 0.1793\n",
      "Epoch 9/30, Training Loss: 0.1919, Validation Loss: 0.1807\n",
      "Epoch 10/30, Training Loss: 0.1924, Validation Loss: 0.1787\n",
      "Epoch 11/30, Training Loss: 0.1912, Validation Loss: 0.1793\n",
      "Epoch 12/30, Training Loss: 0.1900, Validation Loss: 0.1802\n",
      "Epoch 13/30, Training Loss: 0.1893, Validation Loss: 0.1796\n",
      "Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 14/30, Training Loss: 0.1895, Validation Loss: 0.1806\n",
      "Epoch 15/30, Training Loss: 0.1873, Validation Loss: 0.1781\n",
      "Epoch 16/30, Training Loss: 0.1859, Validation Loss: 0.1795\n",
      "Epoch 17/30, Training Loss: 0.1858, Validation Loss: 0.1776\n",
      "Epoch 18/30, Training Loss: 0.1859, Validation Loss: 0.1790\n",
      "Epoch 19/30, Training Loss: 0.1846, Validation Loss: 0.1788\n",
      "Epoch 20/30, Training Loss: 0.1854, Validation Loss: 0.1802\n",
      "Epoch 00021: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 21/30, Training Loss: 0.1849, Validation Loss: 0.1784\n",
      "Epoch 22/30, Training Loss: 0.1832, Validation Loss: 0.1775\n",
      "Epoch 23/30, Training Loss: 0.1835, Validation Loss: 0.1778\n",
      "Epoch 24/30, Training Loss: 0.1836, Validation Loss: 0.1780\n",
      "Epoch 25/30, Training Loss: 0.1832, Validation Loss: 0.1780\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 26/30, Training Loss: 0.1834, Validation Loss: 0.1782\n",
      "Epoch 27/30, Training Loss: 0.1808, Validation Loss: 0.1781\n",
      "Early stopping triggered.\n",
      "Fold 4\n",
      "Epoch 1/30, Training Loss: 0.2448, Validation Loss: 0.1957\n",
      "Epoch 2/30, Training Loss: 0.2046, Validation Loss: 0.1920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Training Loss: 0.2003, Validation Loss: 0.1884\n",
      "Epoch 4/30, Training Loss: 0.1977, Validation Loss: 0.1863\n",
      "Epoch 5/30, Training Loss: 0.1968, Validation Loss: 0.1878\n",
      "Epoch 6/30, Training Loss: 0.1933, Validation Loss: 0.1891\n",
      "Epoch 7/30, Training Loss: 0.1936, Validation Loss: 0.1857\n",
      "Epoch 8/30, Training Loss: 0.1913, Validation Loss: 0.1824\n",
      "Epoch 9/30, Training Loss: 0.1916, Validation Loss: 0.1874\n",
      "Epoch 10/30, Training Loss: 0.1910, Validation Loss: 0.1838\n",
      "Epoch 11/30, Training Loss: 0.1900, Validation Loss: 0.1819\n",
      "Epoch 12/30, Training Loss: 0.1893, Validation Loss: 0.1821\n",
      "Epoch 13/30, Training Loss: 0.1899, Validation Loss: 0.1823\n",
      "Epoch 14/30, Training Loss: 0.1888, Validation Loss: 0.1825\n",
      "Epoch 00015: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 15/30, Training Loss: 0.1875, Validation Loss: 0.1833\n",
      "Epoch 16/30, Training Loss: 0.1867, Validation Loss: 0.1816\n",
      "Epoch 17/30, Training Loss: 0.1866, Validation Loss: 0.1833\n",
      "Epoch 18/30, Training Loss: 0.1849, Validation Loss: 0.1819\n",
      "Epoch 19/30, Training Loss: 0.1851, Validation Loss: 0.1821\n",
      "Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 20/30, Training Loss: 0.1841, Validation Loss: 0.1818\n",
      "Epoch 21/30, Training Loss: 0.1829, Validation Loss: 0.1809\n",
      "Epoch 22/30, Training Loss: 0.1833, Validation Loss: 0.1821\n",
      "Epoch 23/30, Training Loss: 0.1818, Validation Loss: 0.1817\n",
      "Epoch 24/30, Training Loss: 0.1824, Validation Loss: 0.1832\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 25/30, Training Loss: 0.1814, Validation Loss: 0.1820\n",
      "Epoch 26/30, Training Loss: 0.1800, Validation Loss: 0.1816\n",
      "Early stopping triggered.\n",
      "Fold 5\n",
      "Epoch 1/30, Training Loss: 0.2475, Validation Loss: 0.1980\n",
      "Epoch 2/30, Training Loss: 0.2042, Validation Loss: 0.1980\n",
      "Epoch 3/30, Training Loss: 0.1997, Validation Loss: 0.1924\n",
      "Epoch 4/30, Training Loss: 0.1975, Validation Loss: 0.1918\n",
      "Epoch 5/30, Training Loss: 0.1965, Validation Loss: 0.1936\n",
      "Epoch 6/30, Training Loss: 0.1935, Validation Loss: 0.1875\n",
      "Epoch 7/30, Training Loss: 0.1922, Validation Loss: 0.1901\n",
      "Epoch 8/30, Training Loss: 0.1926, Validation Loss: 0.1896\n",
      "Epoch 9/30, Training Loss: 0.1914, Validation Loss: 0.1884\n",
      "Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 10/30, Training Loss: 0.1913, Validation Loss: 0.1890\n",
      "Epoch 11/30, Training Loss: 0.1893, Validation Loss: 0.1886\n",
      "Early stopping triggered.\n",
      "Fold 6\n",
      "Epoch 1/30, Training Loss: 0.2663, Validation Loss: 0.2037\n",
      "Epoch 2/30, Training Loss: 0.2038, Validation Loss: 0.2012\n",
      "Epoch 3/30, Training Loss: 0.2014, Validation Loss: 0.2007\n",
      "Epoch 4/30, Training Loss: 0.1976, Validation Loss: 0.1989\n",
      "Epoch 5/30, Training Loss: 0.1962, Validation Loss: 0.1960\n",
      "Epoch 6/30, Training Loss: 0.1947, Validation Loss: 0.1980\n",
      "Epoch 7/30, Training Loss: 0.1925, Validation Loss: 0.2021\n",
      "Epoch 8/30, Training Loss: 0.1928, Validation Loss: 0.1968\n",
      "Epoch 9/30, Training Loss: 0.1913, Validation Loss: 0.1947\n",
      "Epoch 10/30, Training Loss: 0.1896, Validation Loss: 0.1967\n",
      "Epoch 11/30, Training Loss: 0.1888, Validation Loss: 0.1961\n",
      "Epoch 12/30, Training Loss: 0.1889, Validation Loss: 0.1991\n",
      "Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 13/30, Training Loss: 0.1884, Validation Loss: 0.1950\n",
      "Epoch 14/30, Training Loss: 0.1867, Validation Loss: 0.1935\n",
      "Epoch 15/30, Training Loss: 0.1854, Validation Loss: 0.1930\n",
      "Epoch 16/30, Training Loss: 0.1863, Validation Loss: 0.1931\n",
      "Epoch 17/30, Training Loss: 0.1847, Validation Loss: 0.1925\n",
      "Epoch 18/30, Training Loss: 0.1841, Validation Loss: 0.1924\n",
      "Epoch 19/30, Training Loss: 0.1834, Validation Loss: 0.1972\n",
      "Epoch 20/30, Training Loss: 0.1839, Validation Loss: 0.1947\n",
      "Epoch 21/30, Training Loss: 0.1834, Validation Loss: 0.1916\n",
      "Epoch 22/30, Training Loss: 0.1821, Validation Loss: 0.1966\n",
      "Epoch 23/30, Training Loss: 0.1825, Validation Loss: 0.1953\n",
      "Epoch 24/30, Training Loss: 0.1819, Validation Loss: 0.1942\n",
      "Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 25/30, Training Loss: 0.1817, Validation Loss: 0.1926\n",
      "Epoch 26/30, Training Loss: 0.1809, Validation Loss: 0.1944\n",
      "Early stopping triggered.\n",
      "Fold 7\n",
      "Epoch 1/30, Training Loss: 0.2393, Validation Loss: 0.1931\n",
      "Epoch 2/30, Training Loss: 0.2055, Validation Loss: 0.1911\n",
      "Epoch 3/30, Training Loss: 0.2008, Validation Loss: 0.1896\n",
      "Epoch 4/30, Training Loss: 0.1993, Validation Loss: 0.1910\n",
      "Epoch 5/30, Training Loss: 0.1973, Validation Loss: 0.1854\n",
      "Epoch 6/30, Training Loss: 0.1953, Validation Loss: 0.1880\n",
      "Epoch 7/30, Training Loss: 0.1943, Validation Loss: 0.1874\n",
      "Epoch 8/30, Training Loss: 0.1936, Validation Loss: 0.1838\n",
      "Epoch 9/30, Training Loss: 0.1902, Validation Loss: 0.1826\n",
      "Epoch 10/30, Training Loss: 0.1921, Validation Loss: 0.1842\n",
      "Epoch 11/30, Training Loss: 0.1912, Validation Loss: 0.1860\n",
      "Epoch 12/30, Training Loss: 0.1899, Validation Loss: 0.1825\n",
      "Epoch 13/30, Training Loss: 0.1895, Validation Loss: 0.1837\n",
      "Epoch 14/30, Training Loss: 0.1890, Validation Loss: 0.1831\n",
      "Epoch 15/30, Training Loss: 0.1886, Validation Loss: 0.1827\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 16/30, Training Loss: 0.1885, Validation Loss: 0.1833\n",
      "Epoch 17/30, Training Loss: 0.1855, Validation Loss: 0.1823\n",
      "Epoch 18/30, Training Loss: 0.1859, Validation Loss: 0.1833\n",
      "Epoch 19/30, Training Loss: 0.1843, Validation Loss: 0.1817\n",
      "Epoch 20/30, Training Loss: 0.1851, Validation Loss: 0.1816\n",
      "Epoch 21/30, Training Loss: 0.1853, Validation Loss: 0.1818\n",
      "Epoch 22/30, Training Loss: 0.1847, Validation Loss: 0.1810\n",
      "Epoch 23/30, Training Loss: 0.1837, Validation Loss: 0.1814\n",
      "Epoch 24/30, Training Loss: 0.1836, Validation Loss: 0.1827\n",
      "Epoch 25/30, Training Loss: 0.1830, Validation Loss: 0.1805\n",
      "Epoch 26/30, Training Loss: 0.1834, Validation Loss: 0.1823\n",
      "Epoch 27/30, Training Loss: 0.1830, Validation Loss: 0.1815\n",
      "Epoch 28/30, Training Loss: 0.1827, Validation Loss: 0.1831\n",
      "Epoch 00029: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 29/30, Training Loss: 0.1829, Validation Loss: 0.1817\n",
      "Epoch 30/30, Training Loss: 0.1817, Validation Loss: 0.1802\n",
      "Fold 8\n",
      "Epoch 1/30, Training Loss: 0.2344, Validation Loss: 0.2102\n",
      "Epoch 2/30, Training Loss: 0.2015, Validation Loss: 0.2105\n",
      "Epoch 3/30, Training Loss: 0.1995, Validation Loss: 0.2057\n",
      "Epoch 4/30, Training Loss: 0.1965, Validation Loss: 0.2038\n",
      "Epoch 5/30, Training Loss: 0.1941, Validation Loss: 0.2061\n",
      "Epoch 6/30, Training Loss: 0.1933, Validation Loss: 0.2017\n",
      "Epoch 7/30, Training Loss: 0.1923, Validation Loss: 0.2021\n",
      "Epoch 8/30, Training Loss: 0.1916, Validation Loss: 0.2028\n",
      "Epoch 9/30, Training Loss: 0.1908, Validation Loss: 0.2013\n",
      "Epoch 10/30, Training Loss: 0.1897, Validation Loss: 0.2024\n",
      "Epoch 11/30, Training Loss: 0.1899, Validation Loss: 0.1994\n",
      "Epoch 12/30, Training Loss: 0.1875, Validation Loss: 0.2006\n",
      "Epoch 13/30, Training Loss: 0.1868, Validation Loss: 0.2001\n",
      "Epoch 14/30, Training Loss: 0.1876, Validation Loss: 0.1991\n",
      "Epoch 15/30, Training Loss: 0.1861, Validation Loss: 0.1967\n",
      "Epoch 16/30, Training Loss: 0.1867, Validation Loss: 0.2005\n",
      "Epoch 17/30, Training Loss: 0.1860, Validation Loss: 0.2011\n",
      "Epoch 18/30, Training Loss: 0.1855, Validation Loss: 0.1988\n",
      "Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 19/30, Training Loss: 0.1858, Validation Loss: 0.1999\n",
      "Epoch 20/30, Training Loss: 0.1834, Validation Loss: 0.1998\n",
      "Early stopping triggered.\n",
      "Fold 9\n",
      "Epoch 1/30, Training Loss: 0.2515, Validation Loss: 0.1971\n",
      "Epoch 2/30, Training Loss: 0.2033, Validation Loss: 0.1987\n",
      "Epoch 3/30, Training Loss: 0.2005, Validation Loss: 0.1945\n",
      "Epoch 4/30, Training Loss: 0.1977, Validation Loss: 0.1915\n",
      "Epoch 5/30, Training Loss: 0.1955, Validation Loss: 0.1932\n",
      "Epoch 6/30, Training Loss: 0.1947, Validation Loss: 0.1898\n",
      "Epoch 7/30, Training Loss: 0.1927, Validation Loss: 0.1899\n",
      "Epoch 8/30, Training Loss: 0.1928, Validation Loss: 0.1900\n",
      "Epoch 9/30, Training Loss: 0.1910, Validation Loss: 0.1902\n",
      "Epoch 10/30, Training Loss: 0.1905, Validation Loss: 0.1897\n",
      "Epoch 11/30, Training Loss: 0.1901, Validation Loss: 0.1915\n",
      "Epoch 12/30, Training Loss: 0.1891, Validation Loss: 0.1927\n",
      "Epoch 13/30, Training Loss: 0.1896, Validation Loss: 0.1921\n",
      "Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 14/30, Training Loss: 0.1883, Validation Loss: 0.1922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Training Loss: 0.1868, Validation Loss: 0.1907\n",
      "Early stopping triggered.\n",
      "Fold 10\n",
      "Epoch 1/30, Training Loss: 0.2634, Validation Loss: 0.1952\n",
      "Epoch 2/30, Training Loss: 0.2060, Validation Loss: 0.1881\n",
      "Epoch 3/30, Training Loss: 0.2012, Validation Loss: 0.1826\n",
      "Epoch 4/30, Training Loss: 0.1985, Validation Loss: 0.1863\n",
      "Epoch 5/30, Training Loss: 0.1970, Validation Loss: 0.1827\n",
      "Epoch 6/30, Training Loss: 0.1948, Validation Loss: 0.1825\n",
      "Epoch 7/30, Training Loss: 0.1936, Validation Loss: 0.1817\n",
      "Epoch 8/30, Training Loss: 0.1930, Validation Loss: 0.1822\n",
      "Epoch 9/30, Training Loss: 0.1922, Validation Loss: 0.1854\n",
      "Epoch 10/30, Training Loss: 0.1917, Validation Loss: 0.1805\n",
      "Epoch 11/30, Training Loss: 0.1904, Validation Loss: 0.1817\n",
      "Epoch 12/30, Training Loss: 0.1900, Validation Loss: 0.1816\n",
      "Epoch 13/30, Training Loss: 0.1901, Validation Loss: 0.1795\n",
      "Epoch 14/30, Training Loss: 0.1891, Validation Loss: 0.1797\n",
      "Epoch 15/30, Training Loss: 0.1890, Validation Loss: 0.1797\n",
      "Epoch 16/30, Training Loss: 0.1891, Validation Loss: 0.1789\n",
      "Epoch 17/30, Training Loss: 0.1889, Validation Loss: 0.1809\n",
      "Epoch 18/30, Training Loss: 0.1879, Validation Loss: 0.1813\n",
      "Epoch 19/30, Training Loss: 0.1871, Validation Loss: 0.1796\n",
      "Epoch 20/30, Training Loss: 0.1862, Validation Loss: 0.1786\n",
      "Epoch 21/30, Training Loss: 0.1863, Validation Loss: 0.1795\n",
      "Epoch 22/30, Training Loss: 0.1864, Validation Loss: 0.1801\n",
      "Epoch 23/30, Training Loss: 0.1859, Validation Loss: 0.1792\n",
      "Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 24/30, Training Loss: 0.1860, Validation Loss: 0.1797\n",
      "Epoch 25/30, Training Loss: 0.1839, Validation Loss: 0.1803\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "seed_sum = 0\n",
    "\n",
    "for i in range(42,45):\n",
    "    \n",
    "    seed_everything(i)  # 원하는 시드 값으로 고정\n",
    "\n",
    "    # Custom Label Encoder to handle unseen labels\n",
    "#     class CustomLabelEncoder(LabelEncoder):\n",
    "#         def fit(self, y):\n",
    "#             super().fit(y)\n",
    "#             return self\n",
    "\n",
    "#         def transform(self, y):\n",
    "#             known_classes = set(self.classes_)\n",
    "#             y_transformed = []\n",
    "#             new_label = len(self.classes_)\n",
    "#             class_mapping = {label: idx for idx, label in enumerate(self.classes_)}\n",
    "\n",
    "#             for label in y:\n",
    "#                 if label in known_classes:\n",
    "#                     y_transformed.append(class_mapping[label])\n",
    "#                 else:\n",
    "#                     y_transformed.append(new_label)\n",
    "#                     new_label += 1\n",
    "\n",
    "#             return np.array(y_transformed)\n",
    "\n",
    "#         def fit_transform(self, y):\n",
    "#             self.fit(y)\n",
    "#             return self.transform(y)\n",
    "\n",
    "    train = df_train.drop(columns = ['sj_1','sj_5','sj_6','sj_7','sj_8','sj_9'])\n",
    "    test = df_test.drop(columns = ['sj_1','sj_5','sj_6','sj_7','sj_8','sj_9'])\n",
    "\n",
    "    columns_to_keep = train.columns[train.columns != 'target']\n",
    "    train = train[columns_to_keep.tolist() + ['target']]  # Keep 'target' in train\n",
    "    columns_to_keep = train.columns[(train.columns != 'target') & (train.columns != 'easy_hard')]\n",
    "    test = test[columns_to_keep.tolist()]  # Exclude 'target' in test\n",
    "\n",
    "#     train['target'] = train['target'].map({'AbNormal': 1, 'Normal': 0}).astype(int)\n",
    "\n",
    "#     label_encoders = {}\n",
    "#     for col in train.columns:\n",
    "#         if train[col].dtype == 'object':\n",
    "#             le = CustomLabelEncoder()\n",
    "#             train[col] = le.fit_transform(train[col].astype(str))\n",
    "#             test[col] = le.transform(test[col].astype(str))\n",
    "#             label_encoders[col] = le\n",
    "\n",
    "    # Convert to PyTorch Datasets\n",
    "    class TabularDataset(Dataset):\n",
    "        def __init__(self, data, target=None):\n",
    "            self.data = torch.tensor(data, dtype=torch.float32)\n",
    "            self.target = torch.tensor(target, dtype=torch.float32) if target is not None else None\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            if self.target is not None:\n",
    "                return self.data[index], self.target[index]\n",
    "            else:\n",
    "                return self.data[index]\n",
    "\n",
    "    test_dataset = TabularDataset(X_test)\n",
    "\n",
    "    # Define the enhanced DBN with a feedforward architecture\n",
    "    class EnhancedDBN(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dims):\n",
    "            super(EnhancedDBN, self).__init__()\n",
    "            layers = []\n",
    "            for i in range(len(hidden_dims)):\n",
    "                if i == 0:\n",
    "                    layers.append(nn.Linear(input_dim, hidden_dims[i]))\n",
    "                else:\n",
    "                    layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "                layers.append(nn.BatchNorm1d(hidden_dims[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(0.3))\n",
    "            self.hidden_layers = nn.Sequential(*layers)\n",
    "            self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.hidden_layers(x)\n",
    "            x = self.output_layer(x)\n",
    "            return self.sigmoid(x)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    input_dim = train.shape[1] - 2\n",
    "    hidden_dims = [256, 128, 64]  # Enhanced hidden layers\n",
    "\n",
    "    # Stratified K-Fold Cross-Validation with ensembling\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    best_val_losses = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(train, train['easy_hard'])):\n",
    "        print(f\"Fold {fold+1}\")\n",
    "        X_train, X_val = train.drop(columns=['target','easy_hard']).values[train_index], train.drop(columns=['target','easy_hard']).values[val_index]\n",
    "        y_train, y_val = train['target'].values[train_index], train['target'].values[val_index]\n",
    "        \n",
    "        # Normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(test)\n",
    "        \n",
    "        train_dataset = TabularDataset(X_train, y_train)\n",
    "        val_dataset = TabularDataset(X_val, y_val)\n",
    "        test_dataset = TabularDataset(X_test)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        model = EnhancedDBN(input_dim, hidden_dims)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "        \n",
    "        # Training loop\n",
    "        epochs = 30\n",
    "        best_val_loss = np.inf\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for data, target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data).squeeze()\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_preds = []\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    output = model(data).squeeze()\n",
    "                    loss = criterion(output, target)\n",
    "                    val_loss += loss.item() * data.size(0)\n",
    "                    val_preds.extend(output.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        # Out-of-Fold Predictions and Test Set Predictions\n",
    "        oof_predictions[val_index] = val_preds\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "        test_preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                output = model(data).squeeze()\n",
    "                test_preds.extend(output.cpu().numpy())\n",
    "        test_predictions += np.array(test_preds) / skf.n_splits\n",
    "\n",
    "        best_val_losses.append(best_val_loss)\n",
    "    \n",
    "    seed_sum += 1\n",
    "    \n",
    "# Save the submission file\n",
    "submission_dbn = pd.read_csv(\"submission.csv\")\n",
    "submission_dbn['target'] = test_predictions / seed_sum\n",
    "submission_dbn.to_csv('submission_dbn_원본.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1118f6",
   "metadata": {},
   "source": [
    "# transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b45ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/30, Training Loss: 0.2337, Validation Loss: 0.1929\n",
      "Epoch 2/30, Training Loss: 0.1982, Validation Loss: 0.1892\n",
      "Epoch 3/30, Training Loss: 0.1947, Validation Loss: 0.1865\n",
      "Epoch 4/30, Training Loss: 0.1927, Validation Loss: 0.1856\n",
      "Epoch 5/30, Training Loss: 0.1914, Validation Loss: 0.1851\n",
      "Epoch 6/30, Training Loss: 0.1911, Validation Loss: 0.1847\n",
      "Epoch 7/30, Training Loss: 0.1901, Validation Loss: 0.1836\n",
      "Epoch 8/30, Training Loss: 0.1894, Validation Loss: 0.1837\n",
      "Epoch 9/30, Training Loss: 0.1892, Validation Loss: 0.1836\n",
      "Epoch 10/30, Training Loss: 0.1890, Validation Loss: 0.1837\n",
      "Epoch 11/30, Training Loss: 0.1888, Validation Loss: 0.1837\n",
      "Epoch 12/30, Training Loss: 0.1888, Validation Loss: 0.1837\n",
      "Epoch 13/30, Training Loss: 0.1896, Validation Loss: 0.1837\n",
      "Epoch 14/30, Training Loss: 0.1897, Validation Loss: 0.1839\n",
      "Epoch 15/30, Training Loss: 0.1890, Validation Loss: 0.1834\n",
      "Epoch 16/30, Training Loss: 0.1888, Validation Loss: 0.1828\n",
      "Epoch 17/30, Training Loss: 0.1885, Validation Loss: 0.1829\n",
      "Epoch 18/30, Training Loss: 0.1883, Validation Loss: 0.1825\n",
      "Epoch 19/30, Training Loss: 0.1884, Validation Loss: 0.1833\n",
      "Epoch 20/30, Training Loss: 0.1878, Validation Loss: 0.1824\n",
      "Epoch 21/30, Training Loss: 0.1869, Validation Loss: 0.1824\n",
      "Epoch 22/30, Training Loss: 0.1860, Validation Loss: 0.1815\n",
      "Epoch 23/30, Training Loss: 0.1858, Validation Loss: 0.1799\n",
      "Epoch 24/30, Training Loss: 0.1849, Validation Loss: 0.1799\n",
      "Epoch 25/30, Training Loss: 0.1845, Validation Loss: 0.1799\n",
      "Epoch 26/30, Training Loss: 0.1839, Validation Loss: 0.1801\n",
      "Epoch 27/30, Training Loss: 0.1838, Validation Loss: 0.1796\n",
      "Epoch 28/30, Training Loss: 0.1828, Validation Loss: 0.1797\n",
      "Epoch 29/30, Training Loss: 0.1833, Validation Loss: 0.1795\n",
      "Epoch 30/30, Training Loss: 0.1823, Validation Loss: 0.1795\n",
      "Fold 2\n",
      "Epoch 1/30, Training Loss: 0.1820, Validation Loss: 0.1809\n",
      "Epoch 2/30, Training Loss: 0.1826, Validation Loss: 0.1810\n",
      "Epoch 3/30, Training Loss: 0.1822, Validation Loss: 0.1813\n",
      "Epoch 4/30, Training Loss: 0.1826, Validation Loss: 0.1817\n",
      "Epoch 5/30, Training Loss: 0.1825, Validation Loss: 0.1819\n",
      "Epoch 6/30, Training Loss: 0.1828, Validation Loss: 0.1827\n",
      "Epoch 7/30, Training Loss: 0.1823, Validation Loss: 0.1835\n",
      "Epoch 8/30, Training Loss: 0.1825, Validation Loss: 0.1841\n",
      "Epoch 9/30, Training Loss: 0.1825, Validation Loss: 0.1847\n",
      "Epoch 10/30, Training Loss: 0.1817, Validation Loss: 0.1846\n",
      "Epoch 11/30, Training Loss: 0.1814, Validation Loss: 0.1849\n",
      "Epoch 12/30, Training Loss: 0.1813, Validation Loss: 0.1852\n",
      "Epoch 13/30, Training Loss: 0.1808, Validation Loss: 0.1836\n",
      "Epoch 14/30, Training Loss: 0.1804, Validation Loss: 0.1849\n",
      "Epoch 15/30, Training Loss: 0.1794, Validation Loss: 0.1845\n",
      "Epoch 16/30, Training Loss: 0.1792, Validation Loss: 0.1840\n",
      "Epoch 17/30, Training Loss: 0.1789, Validation Loss: 0.1833\n",
      "Epoch 18/30, Training Loss: 0.1784, Validation Loss: 0.1841\n",
      "Epoch 19/30, Training Loss: 0.1780, Validation Loss: 0.1838\n",
      "Epoch 20/30, Training Loss: 0.1780, Validation Loss: 0.1838\n",
      "Epoch 21/30, Training Loss: 0.1779, Validation Loss: 0.1838\n",
      "Epoch 22/30, Training Loss: 0.1773, Validation Loss: 0.1838\n",
      "Epoch 23/30, Training Loss: 0.1783, Validation Loss: 0.1838\n",
      "Epoch 24/30, Training Loss: 0.1784, Validation Loss: 0.1837\n",
      "Epoch 25/30, Training Loss: 0.1785, Validation Loss: 0.1832\n",
      "Epoch 26/30, Training Loss: 0.1787, Validation Loss: 0.1845\n",
      "Epoch 27/30, Training Loss: 0.1785, Validation Loss: 0.1852\n",
      "Epoch 28/30, Training Loss: 0.1791, Validation Loss: 0.1854\n",
      "Epoch 29/30, Training Loss: 0.1789, Validation Loss: 0.1843\n",
      "Epoch 30/30, Training Loss: 0.1782, Validation Loss: 0.1862\n",
      "Fold 3\n",
      "Epoch 1/30, Training Loss: 0.1801, Validation Loss: 0.1701\n",
      "Epoch 2/30, Training Loss: 0.1796, Validation Loss: 0.1712\n",
      "Epoch 3/30, Training Loss: 0.1786, Validation Loss: 0.1731\n",
      "Epoch 4/30, Training Loss: 0.1789, Validation Loss: 0.1715\n",
      "Epoch 5/30, Training Loss: 0.1779, Validation Loss: 0.1714\n",
      "Epoch 6/30, Training Loss: 0.1776, Validation Loss: 0.1715\n",
      "Epoch 7/30, Training Loss: 0.1766, Validation Loss: 0.1718\n",
      "Epoch 8/30, Training Loss: 0.1766, Validation Loss: 0.1716\n",
      "Epoch 9/30, Training Loss: 0.1762, Validation Loss: 0.1714\n",
      "Epoch 10/30, Training Loss: 0.1765, Validation Loss: 0.1713\n",
      "Epoch 11/30, Training Loss: 0.1761, Validation Loss: 0.1713\n",
      "Epoch 12/30, Training Loss: 0.1759, Validation Loss: 0.1713\n",
      "Epoch 13/30, Training Loss: 0.1766, Validation Loss: 0.1713\n",
      "Epoch 14/30, Training Loss: 0.1763, Validation Loss: 0.1716\n",
      "Epoch 15/30, Training Loss: 0.1769, Validation Loss: 0.1719\n",
      "Epoch 16/30, Training Loss: 0.1774, Validation Loss: 0.1720\n",
      "Epoch 17/30, Training Loss: 0.1772, Validation Loss: 0.1728\n",
      "Epoch 18/30, Training Loss: 0.1768, Validation Loss: 0.1722\n",
      "Epoch 19/30, Training Loss: 0.1774, Validation Loss: 0.1721\n",
      "Epoch 20/30, Training Loss: 0.1771, Validation Loss: 0.1735\n",
      "Epoch 21/30, Training Loss: 0.1770, Validation Loss: 0.1734\n",
      "Epoch 22/30, Training Loss: 0.1766, Validation Loss: 0.1727\n",
      "Epoch 23/30, Training Loss: 0.1764, Validation Loss: 0.1741\n",
      "Epoch 24/30, Training Loss: 0.1756, Validation Loss: 0.1744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 115\u001b[0m\n\u001b[1;32m    113\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m    114\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 115\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    118\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:466\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 466\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "seed_sum = 0\n",
    "\n",
    "for i in range(42,43):\n",
    "    \n",
    "    seed_everything(i)  # 원하는 시드 값으로 고정\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Identify columns to keep\n",
    "    train = df_train.drop(columns = ['sj_1','sj_5','sj_6','sj_7','sj_8','sj_9'])\n",
    "    test = df_test.drop(columns = ['sj_1','sj_5','sj_6','sj_7','sj_8','sj_9'])\n",
    "\n",
    "    columns_to_keep = train.columns[train.columns != 'target']\n",
    "    train = train[columns_to_keep.tolist() + ['target']]  # Keep 'target' in train\n",
    "    columns_to_keep = train.columns[(train.columns != 'target') & (train.columns != 'easy_hard')]\n",
    "    test = test[columns_to_keep.tolist()]  # Exclude 'target' in test\n",
    "\n",
    "#     train['target'] = train['target'].map({'AbNormal': 1, 'Normal': 0}).astype(int)\n",
    "\n",
    "#     str_col = []\n",
    "#     for col in train.columns:\n",
    "#         if train[col].dtype == \"object\":\n",
    "#             str_col.append(col)\n",
    "\n",
    "#     for col in str_col:\n",
    "#         te = ce.TargetEncoder()\n",
    "#         train[col] = te.fit_transform(train[col], train['target'])\n",
    "#         test[col] = te.transform(test[col])\n",
    "\n",
    "    # Convert to PyTorch Datasets\n",
    "    class TabularDataset(Dataset):\n",
    "        def __init__(self, data, target=None):\n",
    "            self.data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "            self.target = torch.tensor(target, dtype=torch.float32).to(device) if target is not None else None\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            if self.target is not None:\n",
    "                return self.data[index], self.target[index]\n",
    "            else:\n",
    "                return self.data[index]\n",
    "\n",
    "    # Define the Transformer-based model with enhancements\n",
    "    class EnhancedTransformer(nn.Module):\n",
    "        def __init__(self, input_dim, n_heads=8, n_layers=4, dim_feedforward=256, dropout=0.1):\n",
    "            super(EnhancedTransformer, self).__init__()\n",
    "            self.embedding = nn.Linear(input_dim, dim_feedforward)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=dim_feedforward, nhead=n_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "            self.fc_out = nn.Linear(dim_feedforward, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Embed the input\n",
    "            x = self.embedding(x)\n",
    "            x = self.transformer_encoder(x.unsqueeze(0))  # Adding sequence dimension\n",
    "            # Apply the final fully connected layer and sigmoid activation\n",
    "            x = self.fc_out(x.squeeze(0))\n",
    "            return self.sigmoid(x)\n",
    "\n",
    "    input_dim = train.shape[1] - 2\n",
    "\n",
    "    # Stratified K-Fold Cross-Validation with ensembling\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    test_predictions = np.zeros(len(test))\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = EnhancedTransformer(input_dim=input_dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(train, train['easy_hard'])):\n",
    "        print(f\"Fold {fold+1}\")\n",
    "        X_train, X_val = train.drop(columns=['target','easy_hard']).values[train_index], train.drop(columns=['target','easy_hard']).values[val_index]\n",
    "        y_train, y_val = train['target'].values[train_index], train['target'].values[val_index]\n",
    "        \n",
    "        # Normalize data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(test)\n",
    "        \n",
    "        train_dataset = TabularDataset(X_train, y_train)\n",
    "        val_dataset = TabularDataset(X_val, y_val)\n",
    "        test_dataset = TabularDataset(X_test)\n",
    "                                                                                                   \n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Training loop\n",
    "        epochs = 30\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for data, target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data).squeeze()\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_preds = []\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    output = model(data).squeeze()\n",
    "                    loss = criterion(output, target)\n",
    "                    val_loss += loss.item() * data.size(0)\n",
    "                    val_preds.extend(output.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Out-of-Fold Predictions and Test Set Predictions\n",
    "        oof_predictions[val_index] = val_preds\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "        test_preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                output = model(data).squeeze()\n",
    "                test_preds.extend(output.cpu().numpy())\n",
    "        test_predictions += np.array(test_preds) / skf.n_splits\n",
    "    \n",
    "    seed_sum += 1\n",
    "    \n",
    "# Save the submission file\n",
    "submission_tf = pd.read_csv(\"submission.csv\")\n",
    "submission_tf['target'] = test_predictions / seed_sum\n",
    "submission_tf.to_csv('submission_tf_원본.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beae1e1",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c8eb0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d9a18",
   "metadata": {},
   "source": [
    "# LGBM CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4ad2c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y and cat_features definition\n",
    "X = df_train.drop(columns = ['target'])\n",
    "columns_to_keep = X.columns[X.columns != 'easy_hard']\n",
    "X_test = df_test[columns_to_keep]\n",
    "y = df_train['target'].values\n",
    "#cat_features = str_col.copy() #Target Encoding 사용 시 주석처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cd849269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'max_depth': 16, 'num_leaves': 351, 'learning_rate': 0.03893878626108622, \n",
    "#  'lambda_l1': 1.4673071932573515e-06, 'lambda_l2': 0.1264772890278767, \n",
    "#  'bagging_fraction': 0.6993706440668855, 'feature_fraction': 0.5626216918840518, 'max_bin': 74}. Best is trial 23 with value: 0.27672955974842767.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "45621378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32404, 306) (8102, 306)\n",
      "1\n",
      "0.7418278527397626\n",
      "0.26410564225690275\n",
      "(32405, 306) (8101, 306)\n",
      "1\n",
      "0.7212892960462874\n",
      "0.23515715948777646\n",
      "(32405, 306) (8101, 306)\n",
      "1\n",
      "0.7507326265365786\n",
      "0.2780373831775701\n",
      "(32405, 306) (8101, 306)\n",
      "1\n",
      "0.7358890018284858\n",
      "0.28868360277136257\n",
      "(32405, 306) (8101, 306)\n",
      "1\n",
      "0.7223128530100565\n",
      "0.23570595099183195\n"
     ]
    }
   ],
   "source": [
    "# {'max_depth': 27, 'num_leaves': 208, 'learning_rate': 0.019101568674409413, \n",
    "#  'lambda_l1': 0.3326367603177364, 'lambda_l2': 1.486177480283442e-05, \n",
    "#  'bagging_fraction': 0.5242887175981938, 'feature_fraction': 0.5644130879580334, 'max_bin': 63}\n",
    "\n",
    "# {'max_depth': 21, 'num_leaves': 200, 'learning_rate': 0.010341075793340348, \n",
    "#  'lambda_l1': 0.9571241496969837, 'lambda_l2': 0.0008310433648726436, \n",
    "#  'bagging_fraction': 0.5900656895949932, 'feature_fraction': 0.513070737805499, 'max_bin': 73}. Best is trial 12 with value: 0.2762345679012346.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.where(y_hat < 0.5, 0, 1)  # scikit-learn's f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "train = X\n",
    "y = y\n",
    "models_1 = []\n",
    "evals_result = {}\n",
    "test_preds = []\n",
    "\n",
    "# for state in [1,3,5,42,27]:\n",
    "for state in [1]:\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=state)\n",
    "    for train_index, test_index in skf.split(train,train['easy_hard']):\n",
    "        X_train, X_val = train.drop(columns=['easy_hard']).iloc[train_index], train.drop(columns=['easy_hard']).iloc[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        print(X_train.shape, X_val.shape)\n",
    "\n",
    "        y_pred_list = []\n",
    "        \n",
    "        dtrain = lgbm.Dataset(X_train, y_train)\n",
    "        dvalid = lgbm.Dataset(X_val, y_val)\n",
    "        print(state)\n",
    "        params = {\n",
    "            \"objective\": \"binary\",\n",
    "            \"metric\": \"binary_error\",\n",
    "            \"verbosity\": -1,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "          'max_depth': 25, 'num_leaves': 362, 'learning_rate': 0.046370921779368225, \n",
    " 'lambda_l1': 0.024642915353579757, 'lambda_l2': 0.06870119528218616, \n",
    " 'bagging_fraction': 0.6854130371192677, 'feature_fraction': 0.5004143807034069, 'max_bin': 96, \"early_stopping_round\": 100,\n",
    "        }\n",
    "\n",
    "        params[\"seed\"] = state\n",
    "        model = lgbm.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            valid_sets=[dvalid],\n",
    "            feval=lgb_f1_score,\n",
    "            num_boost_round=1000\n",
    "        )\n",
    "\n",
    "        y_pred_list.append(model.predict(X_val))\n",
    "        print(roc_auc_score(y_val, np.mean(y_pred_list, axis=0)))\n",
    "        y_val_f1 = np.where(y_val >= 0.15, 1, 0)\n",
    "        y_pred_f1 = np.where(np.mean(y_pred_list, axis=0) >= 0.15, 1, 0)\n",
    "        print(f1_score(y_val_f1, y_pred_f1, average='binary'))\n",
    "\n",
    "        models_1.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3cf90334",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_1 = []\n",
    "test_preds_lst = []\n",
    "for i in range(len(models_1)):\n",
    "    pred = models_1[i].predict(X_test)\n",
    "    test_preds_1.append(pred)\n",
    "    \n",
    "test_preds_1 = np.mean(test_preds_1,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f520d7c7",
   "metadata": {},
   "source": [
    "# LGBM REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    " 0.27892234548335976 \n",
    "{'max_depth': 25, 'num_leaves': 362, 'learning_rate': 0.046370921779368225, \n",
    " 'lambda_l1': 0.024642915353579757, 'lambda_l2': 0.06870119528218616, \n",
    " 'bagging_fraction': 0.6854130371192677, 'feature_fraction': 0.5004143807034069, 'max_bin': 96}. Best is trial 59 with value: 0.27892234548335976.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea84520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df_train.drop(columns = ['target','sj_1','sj_5','sj_6','sj_7','sj_8','sj_9'])\n",
    "X = df_train.drop(columns = ['target'])\n",
    "columns_to_keep = X.columns[X.columns != 'easy_hard']\n",
    "X_test = df_test[columns_to_keep]\n",
    "y = df_train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d97ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.where(y_hat < 0.5, 0, 1)  # scikit-learn's f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "train = X\n",
    "y = y\n",
    "models_2 = []\n",
    "evals_result = {}\n",
    "test_preds = []\n",
    "\n",
    "# for state in [1,3,5,42,27]:\n",
    "for state in [1,3,5,42,27]:\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=state)\n",
    "\n",
    "    for train_index, test_index in skf.split(train,train['easy_hard']):\n",
    "        X_train, X_val = train.drop(columns=['easy_hard']).iloc[train_index], train.drop(columns=['easy_hard']).iloc[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        print(X_train.shape, X_val.shape)\n",
    "\n",
    "        y_pred_list = []\n",
    "        \n",
    "        dtrain = lgbm.Dataset(X_train, y_train)\n",
    "        dvalid = lgbm.Dataset(X_val, y_val)\n",
    "        print(state)\n",
    "        params = {\n",
    "            \"objective\": \"regression\",\n",
    "            \"metric\": \"rmse\",\n",
    "            \"verbosity\": -1,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "'max_depth': 16, 'num_leaves': 333, 'learning_rate': 0.021471027173361846,\n",
    " 'lambda_l1': 0.005384441312585211, 'lambda_l2': 0.000648197586104832, \n",
    " 'bagging_fraction': 0.48644143461088324, 'feature_fraction': 0.5120050389226708, 'max_bin': 66, \"early_stopping_round\": 100,\n",
    "        }\n",
    "        params[\"seed\"] = state\n",
    "        \n",
    "        model = lgbm.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            valid_sets=[dvalid],\n",
    "            feval=lgb_f1_score,\n",
    "            num_boost_round=500\n",
    "        )\n",
    "\n",
    "        y_pred_list.append(model.predict(X_val))\n",
    "        print(roc_auc_score(y_val, np.mean(y_pred_list, axis=0)))\n",
    "        y_val_f1 = np.where(y_val >= 0.15, 1, 0)\n",
    "        y_pred_f1 = np.where(np.mean(y_pred_list, axis=0) >= 0.15, 1, 0)\n",
    "        print(f1_score(y_val_f1, y_pred_f1, average='binary'))\n",
    "\n",
    "        models_2.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_2 = []\n",
    "\n",
    "for i in range(len(models_2)):\n",
    "    pred = models_2[i].predict(X_test)\n",
    "    test_preds_2.append(pred)\n",
    "\n",
    "sorted_probs = np.sort(test_preds_2, axis=0)\n",
    "test_preds_2 = np.mean(sorted_probs[:],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605c453",
   "metadata": {},
   "source": [
    "# CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df_train.drop(columns = ['target','sj_1','sj_5','sj_6','sj_7','sj_8','sj_9'])\n",
    "X = df_train.drop(columns = ['target'])\n",
    "columns_to_keep = X.columns[X.columns != 'easy_hard']\n",
    "X_test = df_test[columns_to_keep]\n",
    "y = df_train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "model_cat_1 = []\n",
    "is_holdout = False\n",
    "f1_scores = []\n",
    "mcc_scores = []  # MCC 점수를 저장할 리스트 추가\n",
    "n_split_list = [10]\n",
    "\n",
    "# For reproducibility\n",
    "for state in [1, 5, 42, 77, 777]:\n",
    "    for split in n_split_list:\n",
    "        fold_idx = 1\n",
    "        cv = StratifiedKFold(n_splits=split, shuffle=True, random_state=state)\n",
    "        for train_index, valid_index in cv.split(X, X['easy_hard']):\n",
    "            X_train, X_valid = X.drop(columns=['easy_hard']).iloc[train_index], X.drop(columns=['easy_hard']).iloc[valid_index]\n",
    "            Y_train, Y_valid = y[train_index], y[valid_index]\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            model = CatBoostClassifier(\n",
    "                iterations=3000,\n",
    "                random_state=state,\n",
    "                task_type=\"CPU\",\n",
    "                depth=3,\n",
    "                eval_metric=\"F1\",\n",
    "                bootstrap_type='Bayesian',  # Bayesian Bootstrap 사용\n",
    "                random_strength=4,\n",
    "                l2_leaf_reg=4,\n",
    "                bagging_temperature=0.5,\n",
    "                grow_policy='Depthwise',\n",
    "                learning_rate=0.7,\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train, Y_train,\n",
    "                eval_set=(X_valid, Y_valid),   # 검증 세트 설정\n",
    "                early_stopping_rounds=300, \n",
    "                verbose=100\n",
    "            )\n",
    "            \n",
    "            pred = model.predict_proba(X_valid)[:, 1]\n",
    "            threshold = 0.5\n",
    "\n",
    "            pred = np.where(pred >= threshold, True, False)\n",
    "            \n",
    "            # F1 score 계산\n",
    "            f1 = f1_score(Y_valid, pred, labels=[True, False], average='binary')\n",
    "            # MCC 계산\n",
    "            mcc = matthews_corrcoef(Y_valid, pred)\n",
    "            \n",
    "            print(fold_idx, \"Fold Validation F1 score:\", f1)\n",
    "            print(fold_idx, \"Fold Validation MCC score:\", mcc)\n",
    "            \n",
    "            f1_scores.append(f1)\n",
    "            mcc_scores.append(mcc)  # MCC 점수를 리스트에 추가\n",
    "            model_cat_1.append(model)\n",
    "            fold_idx += 1\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "            if is_holdout:\n",
    "                break \n",
    "    print(state, '학습 완료')\n",
    "    \n",
    "print(\"Validation : F1:\", np.mean(f1_scores))\n",
    "print(\"Validation : MCC:\", np.mean(mcc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67509c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_3 = []\n",
    "\n",
    "for i in range(len(model_cat_1)):\n",
    "    pred = model_cat_1[i].predict(X_test)\n",
    "    test_preds_3.append(pred)\n",
    "\n",
    "sorted_probs = np.sort(test_preds_3, axis=0)\n",
    "test_preds_3 = np.mean(sorted_probs[:],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdcfeb8",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df_train.drop(columns = ['target','sj_1','sj_5','sj_6','sj_7','sj_8','sj_9'])\n",
    "X = df_train.drop(columns = ['target'])\n",
    "columns_to_keep = X.columns[X.columns != 'easy_hard']\n",
    "X_test = df_test[columns_to_keep]\n",
    "y = df_train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "model_xgb_1 = []\n",
    "is_holdout = False\n",
    "f1_scores = []\n",
    "mcc_scores = []  # MCC 점수를 저장할 리스트 추가\n",
    "n_split_list = [10]\n",
    "\n",
    "# For reproducibility\n",
    "for state in [1, 5, 42, 77, 777]:\n",
    "    for split in n_split_list:\n",
    "        fold_idx = 1\n",
    "        cv = StratifiedKFold(n_splits=split, shuffle=True, random_state=state)\n",
    "        for train_index, valid_index in cv.split(X, X['easy_hard']):\n",
    "            X_train, X_valid = X.drop(columns=['easy_hard']).iloc[train_index], X.drop(columns=['easy_hard']).iloc[valid_index]\n",
    "            Y_train, Y_valid = y[train_index], y[valid_index]\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            model = XGBClassifier(\n",
    "                n_estimators=3000,\n",
    "                random_state=state,\n",
    "                max_depth=12,\n",
    "                learning_rate=0.05,\n",
    "                scale_pos_weight=1,  # 클래스 불균형 처리\n",
    "                colsample_bytree=0.9,  # 각 트리에 사용할 피처 비율\n",
    "                subsample=1.0,  # 각 트리에 사용할 샘플 비율\n",
    "                tree_method='gpu_hist', \n",
    "                gpu_id=0,\n",
    "                grow_policy='lossguide',  # Depth-wise 트리 성장 방식\n",
    "                use_label_encoder=False,  # 경고 메시지 방지\n",
    "                early_stopping_rounds=30,\n",
    "                verbosity=1\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train, Y_train,\n",
    "                eval_set=[(X_valid, Y_valid)],\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            pred = model.predict_proba(X_valid)[:, 1]\n",
    "            threshold = 0.15\n",
    "\n",
    "            pred = np.where(pred >= threshold, True, False)\n",
    "            \n",
    "            # F1 score 계산\n",
    "            f1 = f1_score(Y_valid, pred, labels=[True, False], average='binary')\n",
    "            # MCC 계산\n",
    "            mcc = matthews_corrcoef(Y_valid, pred)\n",
    "            \n",
    "            print(fold_idx, \"Fold Validation F1 score:\", f1)\n",
    "            print(fold_idx, \"Fold Validation MCC score:\", mcc)\n",
    "            \n",
    "            f1_scores.append(f1)\n",
    "            mcc_scores.append(mcc)  # MCC 점수를 리스트에 추가\n",
    "            model_xgb_1.append(model)\n",
    "            fold_idx += 1\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "            if is_holdout:\n",
    "                break \n",
    "    print(state, '학습 완료')\n",
    "    \n",
    "print(\"Validation : F1:\", np.mean(f1_scores))\n",
    "print(\"Validation : MCC:\", np.mean(mcc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fd5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_4 = []\n",
    "\n",
    "for i in range(len(model_xgb_1)):\n",
    "    pred = model_xgb_1[i].predict(X_test)\n",
    "    test_preds_4.append(pred)\n",
    "\n",
    "sorted_probs = np.sort(test_preds_4, axis=0)\n",
    "test_preds_4 = np.mean(sorted_probs[:],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675b48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23f86a65",
   "metadata": {},
   "source": [
    "# ENSEMBLE WITH LGBM, XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4ac3f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw_cat = pd.read_csv(\"sw_cat.csv\")\n",
    "# sw_xgb = pd.read_csv(\"sw_xgb.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cfd7faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw_cat.to_csv('lgbm_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0d849cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_preds_lgbm = (test_preds_lgbm_clf *0.3+ test_preds_lgbm_reg*0.1+ sw_cat['target']*0.3 + sw_xgb['target']*0.3)\n",
    "# test_preds_final = (test_preds_1+ test_preds_3 + test_preds_4)/3  ## lgbm regressor 는 빠짐\n",
    "# test_preds_final = (test_preds_1 *0.3+ test_preds_3*0.4 + test_preds_4*0.3)  ## lgbm regressor 는 빠짐\n",
    "test_preds_final = test_preds_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f8fbbdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_final = np.where(test_preds_final >= 0.195, \"AbNormal\",\"Normal\") # 19best    1:1:1 에 19 가 best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5e127305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal      16924\n",
       "AbNormal      437\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_preds_final).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e4876",
   "metadata": {},
   "source": [
    "# ENSEMBLE WITH DEEP & 후처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "05d5987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_clof = pd.read_csv(\"submission_clof_ts.csv\")\n",
    "submission_dbn = pd.read_csv('submission_dbn_ts.csv')\n",
    "submission_transformer = pd.read_csv('submission_tf_ts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bc03e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_clof_index = set(submission_clof[submission_clof['target']>np.percentile(submission_clof['target'], 94)].index)\n",
    "submission_dbn_index = set(submission_dbn[submission_dbn['target']>np.percentile(submission_dbn['target'], 94)].index)\n",
    "submission_transformer_index = set(submission_transformer[submission_transformer['target']>np.percentile(submission_transformer['target'], 94)].index)\n",
    "\n",
    "intersection =  submission_clof_index & submission_transformer_index & submission_dbn_index #& submission_siam_index\n",
    "\n",
    "intersection_list_1 = list(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f1bab92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intersection_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1b72c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_clof_index = set(submission_clof[submission_clof['target']<np.percentile(submission_clof['target'], 80)].index)\n",
    "submission_dbn_index = set(submission_dbn[submission_dbn['target']<np.percentile(submission_dbn['target'], 80)].index)\n",
    "submission_transformer_index = set(submission_transformer[submission_transformer['target']<np.percentile(submission_transformer['target'], 80)].index)\n",
    "\n",
    "intersection = submission_clof_index  & submission_transformer_index & submission_dbn_index #& submission_siam_index\n",
    "\n",
    "intersection_list_2 = list(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c1c3a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "Normal      16889\n",
       "AbNormal      472\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1 = df_test_후처리용[df_test_후처리용['Receip No Collect Result_Dam']!=df_test_후처리용['Receip No Collect Result_Fill1']].index\n",
    "k2 = df_test_후처리용[df_test_후처리용['Receip No Collect Result_Fill2']!=df_test_후처리용['Receip No Collect Result_Fill1']].index\n",
    "k3 = df_test_후처리용[df_test_후처리용['Receip No Collect Result_Dam']!=df_test_후처리용['Receip No Collect Result_Fill2']].index\n",
    "\n",
    "union_1 = list(set(k1) | set(k2) | set(k3))\n",
    "\n",
    "k1 = df_test_후처리용[df_test_후처리용['Production Qty Collect Result_Dam']!=df_test_후처리용['Production Qty Collect Result_Fill2']].index\n",
    "k2 = df_test_후처리용[df_test_후처리용['Production Qty Collect Result_Fill2']!=df_test_후처리용['Production Qty Collect Result_Fill1']].index\n",
    "k3 = df_test_후처리용[df_test_후처리용['Production Qty Collect Result_Dam']!=df_test_후처리용['Production Qty Collect Result_Fill1']].index\n",
    "union_2 = list(set(k1) | set(k2) | set(k3))\n",
    "union_3 = list(set(union_1) | set(union_2))\n",
    "\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub.loc[X_test.index,\"target\"] = test_preds_final\n",
    "#df_sub.loc[submission_lgbm_index,\"target\"] = 'AbNormal'\n",
    "\n",
    "df_sub.loc[intersection_list_1,\"target\"] ='AbNormal'  # 딥 후처리 여기서\n",
    "# df_sub.loc[intersection_list_3,\"target\"] ='AbNormal'\n",
    "\n",
    "df_sub.loc[intersection_list_2,\"target\"] ='Normal'\n",
    "#df_sub.loc[idx,\"target\"] ='AbNormal'\n",
    "\n",
    "# df_sub.loc[union_3,\"target\"] = 'AbNormal'\n",
    "df_sub['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "32079d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(union_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2b9b74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tt['target'] = np.where(df_train_tt['target'] == 'Normal', 0, 1)\n",
    "\n",
    "# 그룹화하고 mean 값 계산\n",
    "grouped_df = df_train_tt.groupby(['Workorder', 'Collect Date_AutoClave'])['target'].mean()\n",
    "\n",
    "# mean 값이 1인 행만 필터링\n",
    "filtered_df = grouped_df[grouped_df >= 0.6]\n",
    "\n",
    "# index로 'Workorder'와 'Collect Date_AutoClave'를 리스트화\n",
    "filtered_list = list(filtered_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "578ddb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_sub\n",
    "\n",
    "df_test_tt['target'] = submission['target']\n",
    "\n",
    "df_test_tt.loc[df_test_tt.set_index(['Workorder', 'Collect Date_AutoClave']).index.isin(filtered_list), 'target'] = 'AbNormal'\n",
    "\n",
    "submission['target'] = df_test_tt['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ed967ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.loc[union_3,\"target\"] = 'AbNormal'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "43c91a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "Normal      16832\n",
       "AbNormal      529\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2aa9f9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001be084fbc4aaa9d921f39e595961b</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005bbd180064abd99e63f9ed3e1ac80</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000948934c4140d883d670adcb609584</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000a6bfd02874c6296dc7b2e9c5678a7</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0018e78ce91343678716e2ea27a51c95</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17356</th>\n",
       "      <td>ffea508b59934d689b540f95eb3fa730</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17357</th>\n",
       "      <td>ffed8923c8a448a98afc641b770be153</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17358</th>\n",
       "      <td>fff1e73734da40adbe805359b3efb462</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17359</th>\n",
       "      <td>fff8e38bdd09470baf95f71e92075dec</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17360</th>\n",
       "      <td>fffa83e0a13d4c2db4fd8bf905b517d4</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17361 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Set ID  target\n",
       "0      0001be084fbc4aaa9d921f39e595961b  Normal\n",
       "1      0005bbd180064abd99e63f9ed3e1ac80  Normal\n",
       "2      000948934c4140d883d670adcb609584  Normal\n",
       "3      000a6bfd02874c6296dc7b2e9c5678a7  Normal\n",
       "4      0018e78ce91343678716e2ea27a51c95  Normal\n",
       "...                                 ...     ...\n",
       "17356  ffea508b59934d689b540f95eb3fa730  Normal\n",
       "17357  ffed8923c8a448a98afc641b770be153  Normal\n",
       "17358  fff1e73734da40adbe805359b3efb462  Normal\n",
       "17359  fff8e38bdd09470baf95f71e92075dec  Normal\n",
       "17360  fffa83e0a13d4c2db4fd8bf905b517d4  Normal\n",
       "\n",
       "[17361 rows x 2 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a705c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_clof = pd.read_csv(\"submission_clof_ts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbef47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307657e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
